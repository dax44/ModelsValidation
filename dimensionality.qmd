---
code-fold: show
bibliography: references.bib
---

# Redukcja wymiarowości

Redukcja wymiarowości (ang. *dimensionality reduction*) jest przekształceniem zbioru danych z przestrzeni wielowymiarowej w przestrzeń niskowymiarową i może być dobrym wyborem, gdy podejrzewamy, że jest "za dużo" zmiennych.
Nadmiar zmiennych, zwykle predyktorów, może być problemem, ponieważ trudno jest zrozumieć lub wizualizować dane w wyższych wymiarach.

Redukcja wymiarowości może być stosowana zarówno w inżynierii cech, jak i w eksploracyjnej analizie danych.
Na przykład, w wielowymiarowych eksperymentach biologicznych, jednym z pierwszych zadań, przed jakimkolwiek modelowaniem, jest określenie, czy istnieją jakiekolwiek niepożądane trendy w danych (np. efekty niezwiązane z interesującym nas zagadnieniem, takie jak różnice między laboratoriami).
Eksploracja danych jest trudna, gdy istnieją setki tysięcy wymiarów, a redukcja wymiarowości może być pomocą w analizie danych.

Inną potencjalną konsekwencją posiadania wielu predyktorów jest model nadmiarową liczbą predyktorów.
Najprostszym przykładem jest regresja liniowa, gdzie liczba predyktorów powinna być mniejsza niż liczba obserwacji użytych do dopasowania modelu.
Innym problemem jest współliniowość, gdzie korelacje między predyktorami mogą negatywnie wpływać na operacje matematyczne używane do oszacowania modelu.
Jeśli istnieje bardzo duża liczba predyktorów, jest mało prawdopodobne, że istnieje taka sama liczba rzeczywistych efektów leżących u podstaw.
Predyktory mogą mierzyć ten sam ukryty efekt (efekty), a zatem takie predyktory będą wysoko skorelowane.
Wiele technik redukcji wymiarowości sprawdza się w tej sytuacji.
W rzeczywistości większość z nich może być skuteczna tylko wtedy, gdy istnieją takie relacje między predyktorami, które można wykorzystać.

Analiza składowych głównych (PCA) jest jedną z najprostszych metod redukcji liczby zmiennych w zbiorze danych, ponieważ opiera się na metodach liniowych i jest nienadzorowana.
W przypadku wielowymiarowego problemu klasyfikacji, początkowy wykres głównych komponentów PCA może pokazać wyraźny podział między klasami.
Jeśli tak jest, to można bezpiecznie założyć, że klasyfikator liniowy może się sprawdzić w takim zadaniu.
Jednak nie jest prawdą, że brak separacji nie oznacza, że problem jest nie do pokonania.

Metody redukcji wymiarowości omawiane w tym rozdziale nie są na ogół metodami selekcji cech.
Metody takie jak PCA reprezentują oryginalne predyktory za pomocą mniejszego podzbioru nowych cech.
Wszystkie oryginalne predyktory są wymagane do obliczenia tych nowych cech.
Wyjątkiem są metody rzadkie, które mają zdolność do całkowitego usunięcia wpływu predyktorów podczas tworzenia nowych cech.

## Opis danych

Do celów ilustracji, jak używać redukcji wymiarowości z przepisami wykorzystamy przykładowy zestaw danych.
@koklu2020 opublikowali zestaw danych dotyczących wizualnych cech suszonej fasoli i opisali metody określania odmian suszonej fasoli na obrazie.
Chociaż wymiarowość tych danych nie jest bardzo duża w porównaniu z wieloma problemami modelowania w świecie rzeczywistym, zapewnia ładny przykład roboczy, aby zademonstrować, jak zmniejszyć liczbę cech.
W swojej pracy napisali:

> Podstawowym celem niniejszej pracy jest dostarczenie metody uzyskiwania jednolitych odmian nasion z produkcji roślinnej, która ma postać populacji, więc nasiona nie są certyfikowane jako jedyna odmiana.
> W związku z tym opracowano system wizji komputerowej do rozróżniania siedmiu różnych zarejestrowanych odmian suchej fasoli o podobnych cechach w celu uzyskania jednolitej klasyfikacji nasion.
> Dla modelu klasyfikacji wykonano obrazy 13 611 ziaren 7 różnych zarejestrowanych suchych odmian fasoli za pomocą kamery o wysokiej rozdzielczości.

Każdy obraz zawiera wiele ziaren.
Proces określania, które piksele odpowiadają konkretnej fasoli, nazywany jest segmentacją obrazu.
Te piksele mogą być analizowane w celu uzyskania cech dla każdej fasoli, takich jak kolor i morfologia (tj. kształt).
Cechy te są następnie wykorzystywane do modelowania wyniku (odmiany fasoli), ponieważ różne odmiany fasoli wyglądają inaczej.
Dane treningowe pochodzą z zestawu ręcznie oznakowanych obrazów, a ten zestaw danych jest używany do tworzenia modelu predykcyjnego, który może rozróżnić siedem odmian fasoli: Cali, Horoz, Dermason, Seker, Bombay, Barbunya i Sira.
Stworzenie skutecznego modelu może pomóc producentom w ilościowym określeniu jednorodności partii fasoli.

Istnieje wiele metod kwantyfikacji kształtów obiektów [@mingqiang2008].
Wiele z nich dotyczy granic lub regionów interesującego nas obiektu.
Przykładowe cechy obejmują:

-   Obszar (lub rozmiar) może być oszacowany przy użyciu liczby pikseli w obiekcie lub rozmiaru wypukłego kadłuba wokół obiektu.
-   Obwód możemy zmierzyć używając liczby pikseli w granicy, jak również prostokąta ograniczającego (najmniejszy prostokąt zamykający obiekt).
-   Oś główna określa ilościowo najdłuższą linię łączącą najbardziej skrajne części obiektu. Mała oś jest prostopadła do osi głównej.
-   Zwartość obiektu możemy mierzyć za pomocą stosunku pola powierzchni obiektu do pola powierzchni koła o tym samym obwodzie. Na przykład symbole $\bullet$ i $\times$ mają bardzo różną zwartość.
-   Istnieją również różne miary tego, jak bardzo obiekt jest podłużny. Na przykład statystyka ekscentryczności to stosunek osi głównej i małej. Istnieją również powiązane szacunki dla okrągłości i wypukłości.

W danych dotyczących fasoli obliczono 16 cech morfologicznych: powierzchnię, obwód, długość osi głównej, długość osi małej, współczynnik kształtu, ekscentryczność, powierzchnię wypukłą, średnicę równoważną, rozległość, zwartość, krągłość, zwięzłość, współczynnik kształtu 1, współczynnik kształtu 2, współczynnik kształtu 3 i współczynnik kształtu 4.

```{r}
library(tidymodels)
tidymodels_prefer()
library(beans)
```

Dla naszych analiz zaczynamy od podziału zbioru za pomocą `initial_split()`.
Pozostałe dane są dzielone na zbiory treningowe i walidacyjne:

```{r}
set.seed(1601)
bean_split <- initial_split(beans, strata = class, prop = 3/4)

bean_train <- training(bean_split)
bean_test  <- testing(bean_split)

set.seed(1602)
bean_val <- validation_split(bean_train, strata = class, prop = 4/5)
bean_val$splits[[1]]
```

Aby wizualnie ocenić, jak dobrze działają różne metody, możemy oszacować metody na zbiorze treningowym (n = 8163 ziaren) i wyświetlić wyniki przy użyciu zbioru walidacyjnego (n = 2043).

Przed rozpoczęciem jakiejkolwiek redukcji wymiarowości możemy poświęcić trochę czasu na zbadanie naszych danych.
Ponieważ wiemy, że wiele z tych cech kształtu prawdopodobnie mierzy podobne koncepcje, przyjrzyjmy się strukturze korelacyjnej danych na @fig-dim1.

```{r}
#| label: fig-dim1
#| fig-cap: Macierz korelacji
library(corrplot)
tmwr_cols <- colorRampPalette(c("#91CBD765", "#CA225E"))
bean_train %>% 
  select(-class) %>% 
  cor() %>% 
  corrplot(col = tmwr_cols(200), tl.col = "black", method = "ellipse")
```

Wiele z tych predyktorów jest silnie skorelowanych, jak np.
powierzchnia i obwód lub współczynniki kształtu 2 i 3.
Chociaż nie poświęcamy temu czasu tutaj, ważne jest również, aby zobaczyć, czy ta struktura korelacji znacząco zmienia się w różnych kategoriach wyników.
Może to pomóc w stworzeniu lepszych modeli.

Zacznijmy od podstawowego przepisu wstępnego przetwarzania danych, który często stosujemy przed jakimikolwiek krokami redukcji wymiarowości.
Kilka predyktorów to współczynniki, a więc prawdopodobnie będą miały skośne rozkłady.
Takie rozkłady mogą siać spustoszenie w obliczeniach wariancji (takich jak te używane w PCA).
Pakiet `bestNormalize` posiada krok, który może wymusić symetryczny rozkład predyktorów.
Użyjemy tego, aby złagodzić problem skośnych rozkładów:

```{r}
library(bestNormalize)
bean_rec <-
  # Use the training data from the bean_val split object
  recipe(class ~ ., data = analysis(bean_val$splits[[1]])) %>%
  step_zv(all_numeric_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors())
```

::: callout-warning
Pamiętaj, że podczas wywoływania funkcji `recipe()` kroki nie są w żaden sposób szacowane ani wykonywane.
:::

Przepis ten zostanie rozszerzony o dodatkowe kroki dla analiz redukcji wymiarowości.
Zanim to zrobimy, przejdźmy do tego, jak receptura może być używana poza przepływem pracy.

Przepływ pracy zawierający recepturę wykorzystuje `fit()` do estymacji receptury i modelu, a następnie `predict()` do przetwarzania danych i tworzenia przewidywań modelu.
W pakiecie `recipes` znajdują się analogiczne funkcje, które mogą być użyte do tego samego celu:

-   `prep(recipe, training)` dopasowuje przepis do zbioru treningowego.
-   `bake(recipe, new_data)` stosuje operacje receptury do `new_data`.

Rysunek 16.3 podsumowuje to.

![Zasada działania poszczególnych czasowników](images/Zrzut%20ekranu%202023-03-11%20o%2017.14.23.png){#fig-dim2 fig-align="center" width="600"}

```{r}
bean_rec_trained <- prep(bean_rec)
bean_rec_trained
```

Zauważ, że kroki zostały wytrenowane i że selektory nie są już ogólne (tj. `all_numeric_predictors()`); teraz pokazują rzeczywiste kolumny, które zostały wybrane.
Również, `prep(bean_rec)` nie wymaga argumentu `training`.
Możesz przekazać dowolne dane do tego argumentu, ale pominięcie go oznacza, że użyte zostaną oryginalne dane z wywołania `recipe()`.
W naszym przypadku były to dane ze zbioru treningowego.

Jednym z ważnych argumentów funkcji `prep()` jest `retain`.
Kiedy `retain = TRUE` (domyślnie), szacunkowa wersja zbioru treningowego jest przechowywana wewnątrz receptury.
Ten zestaw danych został wstępnie przetworzony przy użyciu wszystkich kroków wymienionych w recepturze.
Ponieważ funkcja `prep()` musi wykonywać recepturę w trakcie jej wykonywania, korzystne może być zachowanie tej wersji zbioru treningowego, tak że jeśli ten zbiór danych ma być użyty później, można uniknąć zbędnych obliczeń.
Jednakże, jeśli zestaw treningowy jest duży, może być problematyczne przechowywanie tak dużej ilości danych w pamięci.
Użyj wówczas `retain = FALSE`, aby tego uniknąć.

Po dodaniu nowych kroków do oszacowanej receptury, ponowne zastosowanie `prep()` oszacuje tylko niewykształcone kroki.
Przyda się to, gdy będziemy próbować różnych metod ekstrakcji cech.
Inną opcją, która może pomóc zrozumieć, co dzieje się w analizie, jest `log_changes`:

```{r}
show_variables <- 
  bean_rec %>% 
  prep(log_changes = TRUE)
```
