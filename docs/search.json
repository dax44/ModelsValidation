[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Metody walidacji modeli statystycznych",
    "section": "",
    "text": "Wprowadzenie\nNiniejsza książka została przygotowana z myślą o studentach kierunków Inżynierii i Analizy Danych oraz Matematyka. Ma ona posłużyć w nauce szeroko rozumianej walidacji modeli statystycznych. Pojęcie zostanie dosyć szeroko potraktowane, dlatego w książce nawiązuję do metod próbkowania (ang. resampling), stosowanych do oceny jakości modelu oraz w procesie tuningowaniu modelu, jak również do miar stosowanych do oceny dopasowania modeli. Ponieważ książka ta ma stanowić przegląd metod używanych w ocenie jak dobrze model przewiduje nieznane wartości, to zostanie również uzupełniona o analizę wrażliwości pozwalającą na określenie jakie poszczególne czynniki (predyktory) i w jaki sposób wpływają na predykcję.\nW statystyce walidacja modelu jest zadaniem polegającym na ocenie, czy wybrany model statystyczny jest odpowiedni, czy nie. Często we wnioskowaniu statystycznym wnioski z modeli, które wydają się pasować do danych, mogą być błędne, co powoduje, że badacze nie poznają rzeczywistej wartości swojego modelu. Aby temu zapobiec, stosuje się walidację modelu, aby sprawdzić, czy model statystyczny posiada tzw. zdolność generalizacji. Owa zdolność polega na możliwości poprawnego przewidywania wartości wyjściowej nie tylko dla obserwacji ze zbioru uczącego, ale również z testowego, bez znaczącej straty jakości wspomnianej predykcji. Tego tematu nie należy mylić z blisko związanym zadaniem wyboru modelu, czyli procesem rozróżniania wielu modeli kandydujących: walidacja modelu nie dotyczy tak bardzo konceptualnej konstrukcji modeli, lecz testuje jedynie spójność pomiędzy wybranym modelem a jego deklarowanymi wynikami. Nadrzędnym celem walidacji modelu jest chęć poprawienia jego właściwości.\n\n\n\n\n\nWalidacja modelu występuje w wielu odmianach, a konkretna metoda walidacji modelu stosowana przez badacza jest często uwarunkowana jego projektem badawczym. Oznacza to, że nie ma jednej uniwersalnej metody walidacji modelu. Na przykład, jeśli badacz pracuje z bardzo ograniczonym zestawem danych, ale ma silne założenia wstępne dotyczące danych, może rozważyć walidację dopasowania swojego modelu poprzez zastosowanie metod bayesowskich i testowanie dopasowania modelu przy użyciu różnych rozkładów wstępnych. Jeśli jednak badacz ma dużo danych i testuje wiele zagnieżdżonych modeli, warunki te mogą sprzyjać walidacji krzyżowej i ewentualnie testowi “leave one out”. Są to dwa abstrakcyjne przykłady i każda rzeczywista walidacja modelu będzie musiała rozważyć znacznie więcej zawiłości niż tu opisano, ale przykład ten ilustruje, że metody walidacji modelu zawsze będą miały charakter poszlakowy. Ogólnie rzecz biorąc, modele mogą być walidowane przy użyciu istniejących danych lub przy użyciu nowych danych, a obie metody są omówione w kolejnych rozdziałach.\nNiezbędnym narzędziem w ocenie jakości dopasowania modeli będą metryki, czy inne narzędzia stosowane w ocenie poprawności modelu jak wykresy diagnostyczne, czy macierze klasyfikacji (ang. confusion matrix). W zależności od realizowanego zadania inne metryki mogą być wykorzystywane. I tak w zadaniach klasyfikacyjnych, w zależności od liczby klas zmiennej wyjściowej, będziemy stosowali inne miary do oceny modelu. Również w modelach regresyjnych istnieje cała gama miar pomagających ocenić dopasowanie. Jeszcze inne miary służą do ewaluacji metod nienadzorowanego uczenia maszynowego. Istnieje bowiem wiele technik określenia stopnia jednorodności powstałych skupień, a jednocześnie odpowiedzi napytanie czy wybrana została właściwa liczba grup."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Ekosystem",
    "section": "",
    "text": "Książka zawiera szereg przykładów ilustrujących zasadę działania poszczególnych technik walidacji modeli. Ponieważ od wielu lat używam języka R do budowania modeli uczenia maszynowego, to również w tej książce znajdą się implementacje metod ewaluacji modeli wykonane w R. Spośród trzech dominujących ekosystemów w języku R do budowania modeli uczenia maszynowego, czyli caret1,mlr32 i tidymodels3 postanowiłem postawić na ten ostatn i. Stanowi on przeniesienie filozofii tidy data realizowanej w pakiecie tidyverse4 na modele ML. tidymodels jest meta-pakietem zawierającym w sob ie:1 biblioteka napisana przez Maxa Kuhna, która niestety nie dostaje już wsparcia - https://topepo.github.io/caret/2 jedna biblioteka z całego ekosystemu przygotowanego głównie przez niemieckich programistów Lars Kotthoff, Raphael Sonabend, Michel Lang, Bernd Bischl. Jej największą zaletą jest fakt, iż wszelkie operacje na danych są dokonywane w formacie data.table uznawanym za najszybszy - https://mlr3book.mlr-org.com3 https://www.tidymodels.org4 pakiet został stworzony przez Julie Silge i Maxa Kuhna, a ich motto podane podczas prezentacji biblioteki brzmiało Whenever possible, the software should be able to protect users from committing mistakes. Software should make it easy for users to do the right thing\n\n\nbroom - przekształcanie obiektów statystycznych w tibble;\ndials - narzędzia do tuningowania parametrów modelu;\ndplyr - narzędzia do manipulacji danymi;\nggplot2 - narzędzia do wizualizacji;\ninfer - narzędzia do wnioskowania statystycznego;\nmodeldata - pakiet przykładowych danych do budowania modeli;\nparsnip - narzędzia do tworzenia i manipulowania funkcjami powszechnie używanymi podczas modelowania;\npurrr - zestaw narzędzi do programowania funkcyjnego;\nrecipes - narzędzie do tworzenia modeli;\nrsample - narzędzie do resamplingu;\ntibble - narzędzie do operacji na ramkach danych;\ntidyr - narzędzie do oczyszczania o transformacji danych;\ntune - narzędzie do tuningu hiperparametrów modelu;\nworkflows - narzędzia do zarządzania procesem uczenia modeli;\nworkflowsets - narzędzie do tworzenia zestawów workflow;\nyardstick - narzędzia do oceny dopasowania modelu.\n\nOprócz wspomnianych pakietów w ramach tidymodels występują także: applicable, baguette, butcher, discrim, embed, hardhat, corrr, rules, text recipes, tidypredict, modeldb i tidyposterior.\n\n\n\nWybór padł na tidymodels ponieważ filozofia tworzenia i walidacji modeli przypominająca pieczenie ciasta bardzo mi przypadła do gustu."
  },
  {
    "objectID": "modeling.html#typy-modeli",
    "href": "modeling.html#typy-modeli",
    "title": "\n2  Modelowanie statystyczne\n",
    "section": "\n2.1 Typy modeli",
    "text": "2.1 Typy modeli\nZanim przejdziemy dalej, opiszmy taksonomię rodzajów modeli, pogrupowanych według przeznaczenia. Taksonomia ta informuje zarówno o tym, jak model jest używany, jak i o wielu aspektach tego, jak model może być tworzony lub oceniany. Chociaż lista ta nie jest wyczerpująca, większość modeli należy do przynajmniej jednej z tych kategorii:\n\n2.1.1 Modele opisowe\nCelem modelu opisowego jest opis lub zilustrowanie pewnych cech danych. Analiza może nie mieć innego celu niż wizualne podkreślenie jakiegoś trendu lub artefaktu (lub defektu) w danych.\nNa przykład, pomiary RNA na dużą skalę są możliwe od pewnego czasu przy użyciu mikromacierzy. Wczesne metody laboratoryjne umieszczały próbkę biologiczną na małym mikrochipie. Bardzo małe miejsca na chipie mogą mierzyć sygnał oparty na bogactwie specyficznej sekwencji RNA. Chip zawierałby tysiące (lub więcej) wyników, z których każdy byłby kwantyfikacją RNA związanego z procesem biologicznym. Jednakże na chipie mogłyby wystąpić problemy z jakością, które mogłyby prowadzić do słabych wyników. Na przykład, odcisk palca przypadkowo pozostawiony na części chipa mógłby spowodować niedokładne pomiary podczas skanowania.\n\n\nRysunek 2.1: Ocena jakości odczytów z mikroczipa\n\n\nWczesną metodą oceny takich zagadnień były modele na poziomie sondy, czyli PLM (Bolstad 2004). Tworzono model statystyczny, który uwzględniał pewne różnice w danych, takie jak chip, sekwencja RNA, typ sekwencji i tak dalej. Jeśli w danych występowałyby inne, nieznane czynniki, to efekty te zostałyby uchwycone w resztach modelu. Gdy reszty zostały wykreślone według ich lokalizacji na chipie, dobrej jakości chip nie wykazywałby żadnych wzorców. W przypadku wystąpienia problemu, pewien rodzaj wzorca przestrzennego byłby dostrzegalny. Często typ wzorca sugerowałby problem (np. odcisk palca) oraz możliwe rozwiązanie (wytarcie chipa i ponowne skanowanie, powtórzenie próbki, itp.) Rysunek 2.1 pokazuje zastosowanie tej metody dla dwóch mikromacierzy (Gentleman i in. 2005). Obrazy pokazują dwie różne wartości kolorystyczne; obszary, które są ciemniejsze to miejsca, gdzie intensywność sygnału była większa niż oczekuje model, podczas gdy jaśniejszy kolor pokazuje wartości niższe niż oczekiwane. Lewy panel pokazuje w miarę losowy wzór, podczas gdy prawy panel wykazuje niepożądany artefakt w środku chipa.\n\n2.1.2 Modele do wnioskowania\nCelem modelu inferencyjnego jest podjęcie decyzji dotyczącej pytania badawczego lub sprawdzenie określonej hipotezy, podobnie jak w przypadku testów statystycznych. Model inferencyjny zaczyna się od wcześniej zdefiniowanego przypuszczenia lub pomysłu na temat populacji i tworzy wniosek statystyczny, taki jak szacunek przedziału lub odrzucenie hipotezy.\nNa przykład, celem badania klinicznego może być potwierdzenie, że nowa terapia pozwala wydłużyć życie w porównaniu z istniejącą terapią lub brakiem leczenia. Jeśli kliniczny punkt końcowy dotyczyłby przeżycia pacjenta, hipoteza zerowa mogłaby brzmieć, że nowa terapia ma równą lub niższą medianę czasu przeżycia, a hipoteza alternatywna, że nowa terapia ma wyższą medianę czasu przeżycia. Jeśli ta próba byłaby oceniana przy użyciu tradycyjnego testowania istotności hipotezy zerowej poprzez modelowanie, testowanie istotności dałoby wartość \\(p\\) przy użyciu jakiejś wcześniej zdefiniowanej metodologii opartej na zestawie założeń. Małe wartości dla wartości \\(p\\) w wynikach modelu wskazywałyby na istnienie przesłanek, że nowa terapia pomaga pacjentom żyć dłużej. Duże wartości \\(p\\) w wynikach modelu wskazywałyby, że nie udało się wykazać takiej różnicy; ten brak przesłanek mógłby wynikać z wielu powodów, w tym z tego, że terapia nie działa.\nJakie są ważne aspekty tego typu analizy? Techniki modelowania inferencyjnego zazwyczaj dają pewien rodzaj danych wyjściowych o charakterze probabilistycznym, takich jak wartość \\(p\\), przedział ufności lub prawdopodobieństwo a posteriori. Zatem, aby obliczyć taką wielkość, należy przyjąć formalne założenia probabilistyczne dotyczące danych i procesów, które je wygenerowały. Jakość wyników modelowania statystycznego w dużym stopniu zależy od tych wcześniej określonych założeń, jak również od tego, w jakim stopniu obserwowane dane wydają się z nimi zgadzać. Najbardziej krytycznymi czynnikami są tutaj założenia teoretyczne: “Jeśli moje obserwacje były niezależne, a reszty mają rozkład X, to statystyka testowa Y może być użyta do uzyskania wartości \\(p\\). W przeciwnym razie wynikowa wartość \\(p\\) może być niewłaściwa.”\nJednym z aspektów analiz inferencyjnych jest to, że istnieje tendencja do opóźnionego sprzężenia zwrotnego w zrozumieniu, jak dobrze dane odpowiadają założeniom modelu. W naszym przykładzie badania klinicznego, jeśli znaczenie statystyczne (i kliniczne) wskazuje, że nowa terapia powinna być dostępna do stosowania przez pacjentów, mogą minąć lata zanim zostanie ona zastosowana w terenie i zostanie wygenerowana wystarczająca ilość danych do niezależnej oceny, czy pierwotna analiza statystyczna doprowadziła do podjęcia właściwej decyzji.\n\n2.1.3 Modele predykcyjne\nCzasami modelowania używamy w celu uzyskania jak najdokładniejszej prognozy dla nowych danych. W tym przypadku głównym celem jest, aby przewidywane wartości (ang. prediction) miały najwyższą możliwą zgodność z prawdziwą wartością (ang. observed).\nProstym przykładem może być przewidywanie przez sprzedającego książki, ile egzemplarzy danej książki powinno być zamówionych do jego sklepu w następnym miesiącu. Nadmierna prognoza powoduje marnowanie miejsca i pieniędzy z powodu nadmiaru książek. Jeśli przewidywanie jest mniejsze niż powinno, następuje utrata potencjału i mniejszy zysk.\nCelem tego typu modeli jest raczej estymacja niż wnioskowanie. Na przykład nabywca zwykle nie jest zainteresowany pytaniem typu “Czy w przyszłym miesiącu sprzedam więcej niż 100 egzemplarzy książki X?”, ale raczej “Ile egzemplarzy książki X klienci kupią w przyszłym miesiącu?”. Również, w zależności od kontekstu, może nie być zainteresowania tym, dlaczego przewidywana wartość wynosi X. Innymi słowy, bardziej interesuje go sama wartość niż ocena formalnej hipotezy związanej z danymi. Prognoza może również zawierać miary niepewności. W przypadku nabywcy książek podanie błędu prognozy może być pomocne przy podejmowaniu decyzji, ile książek należy kupić. Może też służyć jako metryka pozwalająca ocenić, jak dobrze zadziałała metoda predykcji.\nJakie są najważniejsze czynniki wpływające na modele predykcyjne? Istnieje wiele różnych sposobów, w jaki można stworzyć model predykcyjny, dlatego w ocenie wpływu poszczególnych czynników kluczowej jest to jak model został opracowany.\nModel mechanistyczny może być wyprowadzony przy użyciu podstawowych zasad w celu uzyskania równania modelowego, które zależy od pewnych założeń. Na przykład przy przewidywaniu ilości leku, która znajduje się w organizmie danej osoby w określonym czasie, przyjmuje się pewne formalne założenia dotyczące sposobu podawania, wchłaniania, metabolizowania i eliminacji leku. Na tej podstawie można wykorzystać układ równań różniczkowych do wyprowadzenia określonego równania modelowego. Dane są wykorzystywane do oszacowania nieznanych parametrów tego równania, tak aby można było wygenerować prognozy. Podobnie jak modele inferencyjne, mechanistyczne modele predykcyjne w dużym stopniu zależą od założeń, które definiują ich równania modelowe. Jednakże, w przeciwieństwie do modeli inferencyjnych, łatwo jest formułować oparte na danych stwierdzenia dotyczące tego, jak dobrze model działa, na podstawie tego, jak dobrze przewiduje istniejące dane. W tym przypadku pętla sprzężenia zwrotnego dla osoby zajmującej się modelowaniem jest znacznie szybsza niż w przypadku testowania hipotez.\nModele empiryczne są tworzone przy bardziej niejasnych założeniach. Modele te należą zwykle do kategorii uczenia maszynowego. Dobrym przykładem jest model K-najbliższych sąsiadów (KNN). Biorąc pod uwagę zestaw danych referencyjnych, nowa obserwacja jest przewidywana przy użyciu wartości K najbardziej podobnych danych w zestawie referencyjnym. Na przykład, jeśli kupujący książkę potrzebuje prognozy dla nowej książki, a dodatkowo posiada dane historyczne o istniejących książkach, wówczas model 5-najbliższych sąsiadów może posłużyć do estymacji liczby nowych książek do zakupu na podstawie liczby sprzedaży pięciu książek, które są najbardziej podobne do nowej książki (dla pewnej definicji “podobnej”). Model ten jest zdefiniowany jedynie przez samą funkcję predykcji (średnia z pięciu podobnych książek). Nie przyjmuje się żadnych teoretycznych lub probabilistycznych założeń dotyczących sprzedaży lub zmiennych, które są używane do określenia podobieństwa pomiędzy książkami. W rzeczywistości podstawową metodą oceny adekwatności modelu jest ocena jego precyzji przy użyciu istniejących danych. Jeśli model jest dobrym wyborem, predykcje powinny być zbliżone do wartości rzeczywistych."
  },
  {
    "objectID": "modeling.html#związki-pomiędzy-typami-modeli",
    "href": "modeling.html#związki-pomiędzy-typami-modeli",
    "title": "\n2  Modelowanie statystyczne\n",
    "section": "\n2.2 Związki pomiędzy typami modeli",
    "text": "2.2 Związki pomiędzy typami modeli\nZwykły model regresji może należeć do którejś z tych trzech klas modeli, w zależności od sposobu jego wykorzystania:\n\nmodel regresji liniowej może być użyty do opisania trendów w danych;\nmodel analizy wariancji (ANOVA) jest specjalnym rodzajem modelu liniowego, który może być użyty do wnioskowania o prawdziwości hipotezy;\nmodel regresji liniowej wykorzystywany jako model predykcyjny.\n\nIstnieje dodatkowy związek między typami modeli, ponieważ konstrukcje, których celem był opis zjawiska lub wnioskowanie o nim, nie są zwykle wykorzystywane do predykcji, to nie należy całkowicie ignorować ich zdolności predykcyjnych. W przypadku pierwszych dwóch typów modeli, badacz skupia się głównie na wyselekcjonowaniu statystycznie istotnych zmiennych w modelu oraz spełnieniu szeregu założeń pozwalających na bezpieczne wnioskowanie. Takie podejście może być niebezpieczne, gdy istotność statystyczna jest używana jako jedyna miara jakości modelu. Jest możliwe, że ten statystycznie zoptymalizowany model ma słabą dokładność wyrażoną pewną miarą dopasowania. Wiemy więc, że model może nie być używany do przewidywania, ale jak bardzo należy ufać wnioskom z modelu, który ma istotne wartości \\(p\\), ale fatalną dokładność?\n\n\n\n\n\n\n\n\n\nWażne\n\n\n\nJeśli model nie jest dobrze dopasowany do danych, wnioski uzyskane na jego podstawie mogą być wysoce wątpliwe. Innymi słowy, istotność statystyczna może nie być wystarczającym dowodem na to, że model jest właściwy.\n\n\nIstnieje również podział samych modeli uczenia maszynowego. Po pierwsze, wiele modeli można skategoryzować jako nadzorowane lub nienadzorowane. Modele nienadzorowane to takie, które uczą się wzorców, skupisk lub innych cech danych, ale nie mają zmiennej wynikowej (nauczyciela). Analiza głównych składowych (PCA), analiza skupień czy autoenkodery są przykładami modeli nienadzorowanych; są one używane do zrozumienia relacji pomiędzy zmiennymi lub zestawami zmiennych bez wyraźnego związku pomiędzy predyktorami i wynikiem. Modele nadzorowane to takie, które mają zmienną wynikową. Regresja liniowa, sieci neuronowe i wiele innych metodologii należą do tej kategorii.\nW ramach modeli nadzorowanych można wyróżnić dwie główne podkategorie:\n\nregresyjne - przewidujące zmienną wynikową będącą zmienną o charakterze ilościowym;\nklasyfikacyjne - przewidujące zmienną wynikową będącą zmienną o charakterze jakościowym.\n\nRóżne zmienne modelu mogą pełnić różne role, zwłaszcza w nadzorowanym uczeniu maszynowym. Zmienna zależna lub objaśniana (ang. outcome) to wartość przewidywana w modelach nadzorowanych. Zmienne niezależne, które są podłożem do tworzenia przewidywań wyniku, są również określane jako predyktory, cechy lub kowarianty (w zależności od kontekstu)."
  },
  {
    "objectID": "modeling.html#proces-tworzenie-modelu",
    "href": "modeling.html#proces-tworzenie-modelu",
    "title": "\n2  Modelowanie statystyczne\n",
    "section": "\n2.3 Proces tworzenie modelu",
    "text": "2.3 Proces tworzenie modelu\nPo pierwsze, należy pamiętać o chronicznie niedocenianym procesie czyszczenia danych. Bez względu na okoliczności, należy przeanalizować dane pod kątem tego, czy są one odpowiednie do celów projektu i czy są właściwe. Te kroki mogą z łatwością zająć więcej czasu niż cała reszta procesu analizy danych (w zależności od okoliczności).\nCzyszczenie danych może również pokrywać się z drugą fazą eksploracji danych, często określaną jako eksploracyjna analiza danych (ang. exploratory data analysis - EDA). EDA wydobywa na światło dzienne to, jak różne zmienne są ze sobą powiązane, ich rozkłady, typowe zakresy zmienności i inne atrybuty. Dobrym pytaniem, które należy zadać w tej fazie, jest “Jak dotarłem do tych danych?”. To pytanie może pomóc zrozumieć, w jaki sposób dane, o których mowa, były próbkowane lub filtrowane i czy te operacje były właściwe. Na przykład podczas łączenia tabel bazy danych może dojść do nieudanego złączenia, które może przypadkowo wyeliminować jedną lub więcej subpopulacji.\nWreszcie, przed rozpoczęciem procesu analizy danych, powinny istnieć jasne oczekiwania co do celu modelu i sposobu oceny jego wydajności. Należy zidentyfikować przynajmniej jedną metrykę wydajności z realistycznymi celami dotyczącymi tego, co można osiągnąć. Typowe metryki statystyczne, to dokładność klasyfikacji (ang. accuracy), odsetek poprawnie i niepoprawnie zaklasyfikowanych sukcesów (przez sukces rozumiemy wyróżnioną klasę), pierwiastek błędu średniokwadratowego i tak dalej. Należy rozważyć względne korzyści i wady tych metryk. Ważne jest również, aby metryka była zgodna z szerszymi celami analizy danych.\n\n\nTypowy przebieg budowy modelu\n\n\nProces badania danych może nie być prosty. Wickham i in. (2019) przedstawili doskonałą ilustrację ogólnego procesu analizy danych. Import danych i czyszczenie/porządkowanie są pokazane jako początkowe kroki. Kiedy rozpoczynają się kroki analityczne dla zrozumienia relacji panujących pomiędzy predyktorami i/lub zmienną wynikową, nie możemy wstępnie określić, ile czasu mogą zająć. Cykl transformacji, modelowania i wizualizacji często wymaga wielu iteracji.\nW ramach czynności zaznaczonych na szarym polu możemy wyróżnić:\n\neksploracyjna analiza danych - to kombinacja pewnych obliczeń statystycznych i wizualizacji, w celu odpowiedzi na podstawowe pytania i postawienia kolejnych. Przykładowo jeśli na wykresie histogramu lub gęstości zmiennej wynikowej w zadaniu regresyjnym zauważymy wyraźną dwumodalność, to może ona świadczyć, że badana zbiorowość nie jest homogeniczna w kontekście analizowanej zmiennej, a co w konsekwencji może skłonić nas do oddzielnego modelowania zjawisk w każdej z podpopulacji.\ninżynieria cech (ang. feature engineering) - zespół czynności mający na celu transformację i selekcję cech w procesie budowania modelu.\ntuning modeli - zespół czynności mający na celu optymalizację hiperparametrów modeli, poprzez wybór różnych ich konfiguracji oraz porównanie efektów uczenia.\nocena dopasowania modeli - ocena jakości otrzymanych modeli na podstawie miar oraz wykresów diagnostycznych.\n\n\n\nPrzykładowy przebieg budowy modelu\n\n\nPrzykładowo w pracy Kuhn i Johnson (2021) autorzy badając natężenie ruchu kolei publicznej w Chicago, przeprowadzili następujące rozumowanie podczas budowy modelu (oryginalna pisownia):\n\nKoddt &lt;- tibble::tribble(\n                                                                                                                           ~Thoughts,             ~Activity,\n                                                             \"The daily ridership values between stations are extremely correlated.\",                 \"EDA\",\n                                                                                \"Weekday and weekend ridership look very different.\",                 \"EDA\",\n                                                           \"One day in the summer of 2010 has an abnormally large number of riders.\",                 \"EDA\",\n                                                                             \"Which stations had the lowest daily ridership values?\",                 \"EDA\",\n                                                                    \"Dates should at least be encoded as day-of-the-week, and year.\", \"Feature Engineering\",\n                                \"Maybe PCA could be used on the correlated predictors to make it easier for the models to use them.\", \"Feature Engineering\",\n                                                     \"Hourly weather records should probably be summarized into daily measurements.\", \"Feature Engineering\",\n                                      \"Let’s start with simple linear regression, K-nearest neighbors, and a boosted decision tree.\",       \"Model Fitting\",\n                                                                                                \"How many neighbors should be used?\",        \"Model Tuning\",\n                                                                         \"Should we run a lot of boosting iterations or just a few?\",        \"Model Tuning\",\n                                                                           \"How many neighbors seemed to be optimal for these data?\",        \"Model Tuning\",\n                                                                            \"Which models have the lowest root mean squared errors?\",    \"Model Evaluation\",\n                                                                                                 \"Which days were poorly predicted?\",                 \"EDA\",\n  \"Variable importance scores indicate that the weather information is not predictive. We’ll drop them from the next set of models.\",    \"Model Evaluation\",\n                                                     \"It seems like we should focus on a lot of boosting iterations for that model.\",    \"Model Evaluation\",\n                                            \"We need to encode holiday features to improve predictions on (and around) those dates.\", \"Feature Engineering\",\n                                                                                               \"Let’s drop KNN from the model list.\",    \"Model Evaluation\"\n  )\ndt |&gt; \n  gt::gt()\n\n\n\n\n\n\nThoughts\n      Activity\n    \n\n\nThe daily ridership values between stations are extremely correlated.\nEDA\n\n\nWeekday and weekend ridership look very different.\nEDA\n\n\nOne day in the summer of 2010 has an abnormally large number of riders.\nEDA\n\n\nWhich stations had the lowest daily ridership values?\nEDA\n\n\nDates should at least be encoded as day-of-the-week, and year.\nFeature Engineering\n\n\nMaybe PCA could be used on the correlated predictors to make it easier for the models to use them.\nFeature Engineering\n\n\nHourly weather records should probably be summarized into daily measurements.\nFeature Engineering\n\n\nLet’s start with simple linear regression, K-nearest neighbors, and a boosted decision tree.\nModel Fitting\n\n\nHow many neighbors should be used?\nModel Tuning\n\n\nShould we run a lot of boosting iterations or just a few?\nModel Tuning\n\n\nHow many neighbors seemed to be optimal for these data?\nModel Tuning\n\n\nWhich models have the lowest root mean squared errors?\nModel Evaluation\n\n\nWhich days were poorly predicted?\nEDA\n\n\nVariable importance scores indicate that the weather information is not predictive. We’ll drop them from the next set of models.\nModel Evaluation\n\n\nIt seems like we should focus on a lot of boosting iterations for that model.\nModel Evaluation\n\n\nWe need to encode holiday features to improve predictions on (and around) those dates.\nFeature Engineering\n\n\nLet’s drop KNN from the model list.\nModel Evaluation\n\n\n\n\n\n\n\n\n\n\nBolstad, Benjamin Milo. 2004. Low-Level Analysis of High-Density Oligonucleotide Array Data: Background, Normalization and Summarization. University of California, Berkeley.\n\n\nGentleman, Robert, Vincent Carey, Wolfgang Huber, Sandrine Dudoit, i Rafael Irizarry. 2005. Bioinformatics and Computational Biology Solutions Using R and Bioconductor. Springer.\n\n\nKuhn, Max, i Kjell Johnson. 2021. Feature Engineering and Selection: A Practical Approach for Predictive Models. Taylor & Francis Group.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, i in. 2019. „Welcome to the Tidyverse”. Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686."
  },
  {
    "objectID": "infer.html",
    "href": "infer.html",
    "title": "\n3  Modele inferencyjne\n",
    "section": "",
    "text": "Jak to zostało wspomniane w poprzednim rozdziale modele inferencyjne służą do wyciągania wniosków na podstawie modelu. W większości przypadków dotyczy to przedziałów ufności pewnych charakterystyk, czy weryfikacji hipotez. Pod pojęciem modeli inferencyjnych będziemy rozumieli wszelkie modele stosowane w procedurze wnioskowania.\nW tym rozdziale zostanie przedstawiona procedura weryfikacyjna mająca na celu ocenę jakości przedstawionych rozwiązań w klasycznym podejściu do przedziałów ufności i weryfikacji hipotez. Największa trudnością w szacowaniu parametrów rozkładu za pomocą przedziałów ufności oraz w weryfikacji hipotez jest konieczność spełnienia założeń stosowalności tych metod. Bardzo często badacz nie posiada wystarczającej wiedzy o konsekwencji naruszenia tych założeń, a czasem nawet o ich istnieniu. Nawet wówczas, gdy badacz jest świadom konieczności spełnienia założeń w estymacji przedziałowej i weryfikacji hipotez, wymagania te mogą się okazać trudne do wypełnienia. W wielu przypadkach podczas weryfikacji hipotez za pomocą testu t-Studenta, weryfikując hipotezę o normalności rozkładu badanej cechy pojawiają się pewne wątpliwości. Po pierwsze, czy wybrany test mogę stosować do weryfikacji hipotezy o normalności w przypadku tak mało licznej próby lub tak licznej próby. Wiemy bowiem, że często stosowany test Shapiro-Wilka do weryfikacji hipotezy o zgodności populacji z rozkładem normalnym, może zbyt często odrzucać hipotezę o zgodności z rozkładem jeśli test jest wykonywany na dużej próbie1. Z drugiej strony dla prób o małej liczności test w większości nie odrzuca hipotezy o normalności, a to dlatego, że nie sposób jej odrzucić np. na podstawie 5 obserwacji. Podniesiony problem normalności rozkładu badanej cechy nie jest jedynym z jakim badacz może się spotkać chcąc spełnić wszystkie założenia modelu2 . Założenia o równości wariancji badanej cechy pomiędzy grupami, czy brak nadmiarowości3, to kolejne przykłady problemów z jakimi może spotkać się badac z.1 moc tego testu rośnie bardzo szybko wraz ze wzrostem liczebności próby2 mam na myśli zarówno modele przedziałów ufności, jaki i modele statystyczne do testowania hipotez3 w przypadku badania efektu za pomocą modelu liniowego\nKonieczność spełnienia wymienionych w stosowanej metodzie wnioskowania założeń jest wymagana, ponieważ w przeciwnym przypadku nie możemy być pewni czy wyniki zastosowanej metody są trafne4. Konsekwencją niespełnienia warunków początkowych metody jest to, że nie możemy być pewni czy rozkład statystyki testowej jest taki jak głosi metoda. I choć istnieją prace, które wyraźnie wskazują na odporność pewnych metod statystycznych na niespełnienie założeń, to nie zwalniają nas z weryfikacji tychże, ponieważ w przypadku niektórych z nich nie znamy konsekwencji ich naruszenia.4 czy wniosek wyciągnięty na podstawie modelu jest właściwy\nW przypadku wspomnianych wyżej wątpliwości co do stosowalności poszczególnych metod weryfikacyjnych należy poszukać rozwiązań, które uprawdopodobnią wyniki uzyskane metodami klasycznymi. Powszechnie polecane w takiej sytuacji są rozwiązania opierające się na próbkowaniu (ang. resampling), wśród których najbardziej znane, to bootstrap i metody permutacyjne.\n\n\n\n\nNiezależnie od stawianej hipotezy, badacz zadaje sobie ten sam rodzaj pytania podczas wnioskowania statystycznego: czy efekt/różnica w obserwowanych danych jest rzeczywista, czy wynika z przypadku? Aby odpowiedzieć na to pytanie, analityk zakłada, że efekt w obserwowanych danych był spowodowany przypadkiem i nazywa to założenie hipotezą zerową5. Analityk następnie oblicza statystykę testową z danych, która opisuje obserwowany efekt. Może użyć tej statystyki testowej do obliczenia wartości \\(p\\) poprzez zestawienie jej z rozkładem wynikającym z hipotezy zerowej. Jeśli to prawdopodobieństwo jest poniżej jakiegoś wcześniej zdefiniowanego poziomu istotności \\(\\alpha\\), to analityk powinien odrzucić hipotezę zerową.5 W rzeczywistości, może nie wierzyć, że hipoteza zerowa jest prawdziwa - hipoteza zerowa jest w opozycji do hipotezy alternatywnej, która zakłada, że efekt obecny w obserwowanych danych jest rzeczywiście spowodowany faktem, że “coś się dzieje”\nPoniżej przedstawione zostaną przykłady zastosowania obu metod we wnioskowaniu. Można te zadania realizować na różne sposoby, my natomiast wykorzystamy bibliotekę infer (Couch i in. 2021) ekosystemu tidymodels (Kuhn i Wickham 2020).\n\nPrzykład 3.1 W tym przykładzie przetestujemy hipotezę o równości średniej z wartością teoretyczną. Dane weźmiemy ze zbioru gss biblioteki infer zawierającego podzbiór wyników spisu powszechnego przeprowadzonego w 1972 r. w USA.\n\nKodlibrary(tidymodels)\nlibrary(nord) # palettes\nglimpse(gss)\n\nRows: 500\nColumns: 11\n$ year    &lt;dbl&gt; 2014, 1994, 1998, 1996, 1994, 1996, 1990, 2016, 2000, 1998, 20…\n$ age     &lt;dbl&gt; 36, 34, 24, 42, 31, 32, 48, 36, 30, 33, 21, 30, 38, 49, 25, 56…\n$ sex     &lt;fct&gt; male, female, male, male, male, female, female, female, female…\n$ college &lt;fct&gt; degree, no degree, degree, no degree, degree, no degree, no de…\n$ partyid &lt;fct&gt; ind, rep, ind, ind, rep, rep, dem, ind, rep, dem, dem, ind, de…\n$ hompop  &lt;dbl&gt; 3, 4, 1, 4, 2, 4, 2, 1, 5, 2, 4, 3, 4, 4, 2, 2, 3, 2, 1, 2, 5,…\n$ hours   &lt;dbl&gt; 50, 31, 40, 40, 40, 53, 32, 20, 40, 40, 23, 52, 38, 72, 48, 40…\n$ income  &lt;ord&gt; $25000 or more, $20000 - 24999, $25000 or more, $25000 or more…\n$ class   &lt;fct&gt; middle class, working class, working class, working class, mid…\n$ finrela &lt;fct&gt; below average, below average, below average, above average, ab…\n$ weight  &lt;dbl&gt; 0.8960034, 1.0825000, 0.5501000, 1.0864000, 1.0825000, 1.08640…\n\n\nPrzetestujmy hipotezę, że średnia wieku wynosi 40 lat. Zacznijmy od sprawdzenia jak wygląda rozkład badanej cechy.\n\nKodgss |&gt; \n  ggplot(aes(age))+\n  geom_histogram(color = \"white\", bins = 15)\n\n\n\nRysunek 3.1: Histogram wieku\n\n\n\n\nMożna mieć pewne wątpliwości co do normalności rozkładu, ponieważ zarysowuje się delikatna asymetria prawostronna. Nie będziemy jednak weryfikować hipotezy o normalności, tylko przeprowadzimy klasyczny test, nie mając pewności czy może on być stosowany w tej sytuacji.\n\nKodt.test(gss$age, mu = 40)\n\n\n    One Sample t-test\n\ndata:  gss$age\nt = 0.44656, df = 499, p-value = 0.6554\nalternative hypothesis: true mean is not equal to 40\n95 percent confidence interval:\n 39.09567 41.43633\nsample estimates:\nmean of x \n   40.266 \n\n\nWynik testu nie daje podstaw do odrzucenia hipotezy o tym, że przeciętny wiek w badanej populacji wynosi 40 lat. Przeprowadzimy teraz wnioskowanie w oparciu o techniki bootstrap i permutacyjną.\n\nKodnull_mean &lt;- gss |&gt; \n  specify(response = age) |&gt; # określenie zmiennej\n  hypothesise(null = \"point\", mu = 40) |&gt; # ustalienie hipotezy\n  generate(1000, type = \"bootstrap\") |&gt; # generujemy dane\n  calculate(stat = \"mean\")\nnull_mean\n\nResponse: age (numeric)\nNull Hypothesis: point\n# A tibble: 1,000 × 2\n   replicate  stat\n       &lt;int&gt; &lt;dbl&gt;\n 1         1  40.2\n 2         2  39.8\n 3         3  39.3\n 4         4  40.3\n 5         5  39.9\n 6         6  39.4\n 7         7  41.4\n 8         8  41.3\n 9         9  40.2\n10        10  39.3\n# ℹ 990 more rows\n\nKodsample_mean &lt;- gss |&gt; \n  specify(response = age) |&gt; \n  calculate(stat = \"mean\")\nsample_mean\n\nResponse: age (numeric)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  40.3\n\n\nTeraz możemy przyjrzeć się rozkładowi średnich w próbach bootstrapowych.\n\nKodci &lt;- null_mean |&gt; \n  get_confidence_interval(point_estimate = sample_mean,\n                          level = .95,\n                          type = \"se\")\n\nnull_mean |&gt; \n  visualise() + \n  shade_ci(endpoints = ci)\n\n\n\nRysunek 3.2: Histogram średnich bootstrapowych wraz z 95% przedziałem ufności dla średniej\n\n\n\n\nKoncentracja wokół wartości 40 może przemawiać za przyjęciem hipotezy \\(H_0\\). Ponadto wygląda na to, że otrzymany przedział ufności zawiera teoretyczną średnią wieku 40, co jest kolejny argumentem za przyjęciem hipotezy zerowej. Na koniec wyliczymy \\(p\\) dla testu bootstrapowego. Wyliczanie \\(p\\) dla testu bootstrapowego odbywa się wg następujących kroków:\n\nWyznaczamy na podstawie próby statystykę interesującą nas w teście (w naszym przypadku średnią \\(\\bar{x}\\)).\nNastępnie przesuwamy wszystkie obserwację o różnicę pomiędzy średnią teoretyczną a \\(\\bar{x}\\), tak aby rozkład miał średnią teoretyczną.\nLosujemy próby bootstrapowe z nowej (przesuniętej) próby.\nNa podstawie prób bootstrapowych wyznaczamy rozkład średnich poszczególnych prób.\nNa koniec sprawdzamy prawdopodobieństwo (szacując na podstawie rozkładu bootstrapowego) otrzymania wartości większych niż średnia oryginalnej próby. Dla hipotez dwustronnych dodajemy do tego prawdopodobieństwo otrzymania wartości mniejszej niż \\(-\\bar{x}\\) (patrz Rys. -Rysunek 3.3)\n\n\nKodnull_mean |&gt; \n  get_p_value(obs_stat = sample_mean, direction = \"two.sided\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.632\n\n\nWartość otrzymana z testu bootstrapowego różni się tylko nieznacznie od otrzymanej testem klasycznym.\n\nKodnull_mean |&gt; \n  visualise()+\n  shade_p_value(obs_stat = sample_mean, direction = \"two-sided\")\n\n\n\nRysunek 3.3: Histogram średnich bootstrapowych z zacienonym obszarem ilustrującym na ile ekstremalna jest średnia naszej próby w stosunku do średniej wynikającej z hipotezy zerowej\n\n\n\n\n\n\n\n\n\n\n\nWskazówka\n\n\n\nW przypadku weryfikacji hipotez dotyczących jednej zmiennej wyniki testu bootstrapowego i permutacyjnego są identyczne, ponieważ te dwa rodzaje próbkowania są w tym przypadku identyczne.\n\n\n\nPrzykład 3.2 Tym razem przetestujemy nico bardziej ambitną hipotezę. Będzie to hipoteza o równości median. Dane zostaną wygenerowane z rozkładów asymetrycznych, a w tych przypadkach porównywanie median ma więcej sensu niż średnich.\n\nKodset.seed(44)\nx1 &lt;- rchisq(20, 2)\nx2 &lt;- -rchisq(15, 2)+10\n\ndt &lt;- tibble(x = c(x1,x2)) |&gt; \n  mutate(gr = rep(c(\"A\", \"B\"), times = c(20,15)))\n\ndt |&gt; \n  ggplot(aes(x, fill = gr))+\n  geom_density(alpha = 0.6)+\n  xlim(c(-2, 12))+\n  scale_fill_nord(palette = \"victory_bonds\")+\n  theme_minimal()\n\n\n\nRysunek 3.4: Porównanie rozkładów obu prób\n\n\n\n\nJak widać na Rysunek 3.4 rozkłady różnią się zarówno położeniem, jak i kształtem. Ponieważ różnią się kształtem to porównanie obu rozkładów testem Manna-Whitneya nie odpowie nam na pytanie o równość median6, a jedynie o tym, że z prawdopodobieństwem 50% losowa wartość z jednego rozkładu będzie większa niż losowa wartość z drugiego rozkładu. Zatem wyniki testu klasycznego nie będą wystarczające do oceny postawionej hipotezy. Przeprowadzimy zatem test metodami próbkowania. Najpierw techniką bootstrapową.6 tylko w przypadku jednakowych kształtów rozkładów test ten weryfikuje równość median\n\nKodset.seed(44)\nsample_diff &lt;- dt |&gt; \n  specify(x~gr) |&gt; \n  calculate(stat = \"diff in medians\", order = c(\"A\",\"B\"))\nsample_diff\n\nResponse: x (numeric)\nExplanatory: gr (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1 -6.83\n\nKodnull_diff &lt;- dt |&gt; \n  specify(x~gr) |&gt; \n  hypothesise(null = \"independence\") |&gt; \n  generate(reps = 1000, type = \"bootstrap\") |&gt; \n  calculate(stat = \"diff in medians\", order = c(\"A\",\"B\"))\n\n\nOceńmy rozkład różnic z prób bootstrapowych. Jeśli hipoteza zerowa jest prawdziwa to rozkład różnicy median powinien oscylować wokół zera.\n\nKodci &lt;- null_diff |&gt; \n  get_confidence_interval(level = .95, type = \"percentile\")\n\nnull_diff |&gt; \n  visualise()+\n  shade_ci(endpoints = ci)+\n  geom_vline(xintercept = 0, \n             linewidth = 2,\n             color = \"red\")\n\n\n\nRysunek 3.5: Rozkład różnic pomiędzy medianami z 95% przedziałem ufności\n\n\n\n\nWidać wyraźnie, że rozkład różnic pomiędzy medianami nie oscyluje wokół zera, co może świadczyć o konieczności odrzucenia hipotezy zerowej.\n\nKodnull_diff |&gt; \n  get_p_value(obs_stat = 0, direction = \"two-sided\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\nKodwilcox.test(x1,x2)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  x1 and x2\nW = 32, p-value = 2.453e-05\nalternative hypothesis: true location shift is not equal to 0\n\n\nWartość \\(p\\) mniejsza od \\(\\alpha\\) każe odrzucić hipotezę zerową na korzyść alternatywnej, czyli mediany nie są równe.\nTeraz przeprowadzimy test permutacyjny.\n\nKodset.seed(44)\nnull_diff &lt;- dt |&gt; \n  specify(x~gr) |&gt; \n  hypothesise(null = \"independence\") |&gt; \n  generate(reps = 1000, type = \"permute\") |&gt; \n  calculate(stat = \"diff in medians\", order = c(\"A\",\"B\"))\n\nci &lt;- null_diff |&gt; \n  get_ci(level = .95, \n         type = \"percentile\")\n\nnull_diff |&gt; \n  visualise()+\n  shade_ci(endpoints = ci)+\n  geom_vline(xintercept = sample_diff$stat,\n             linewidth = 2,\n             color = \"red\")\n\n\n\nRysunek 3.6: Rozkład różnic pomiędzy medianami z 95% przedziałem ufności\n\n\n\n\nJak widać z Rysunek 3.6 rozkład różnic pomiędzy medianami dla całkowicie losowego układu obserwacji7 oscyluje wokół zera. Wartość różnicy median obliczona na podstawie próby -6.83 nie leży wewnątrz przedziału ufności dla różnicy, należy zatem odrzucić hipotezę o równości median. Potwierdza to wynik testu permutacyjnego.7 co odzwierciedla warunek z hipotezy zerowej\n\nKodnull_diff |&gt; \n  visualise() +\n  shade_p_value(obs_stat = sample_diff, direction = \"two-sided\")\n\nnull_diff |&gt; \n  get_p_value(obs_stat = sample_diff, direction = \"two-sided\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\n\n\n\nRysunek 3.7: Rozkład różnic pomiędzy medianami z naniesionym p-value\n\n\n\n\n\n\nPrzykład 3.3 W tym przykładzie zbadamy niezależność cech partyid i class ze zbioru gss, które oznaczają odpowiednio sprzyjanie danej partii politycznej i subiektywną identyfikację klasy społeczno-ekonomicznej.\n\nKodlibrary(ggstatsplot)\nggbarstats(data = gss,\n           x = class,\n           y = partyid, \n           label = \"count\",\n           proportion.test = F, \n           bf.message = F)\n\n\n\nRysunek 3.8: Wykres słupkowy ilustrujący udziały poszczególnych grup wraz z wynikiem testu \\(\\chi^2\\) niezależności\n\n\n\n\nPowyższy test każe odrzucić hipotezę o niezależności cech, a ponieważ nie wymaga on spełnienia założeń (poza niezależnością obserwacji w próbie), to można uznać wynik ten za wiarygodny. Jedyny czynnik jaki mógłby wpłynąć na wynik testu \\(\\chi^2\\) to liczebności w tabeli kontyngencji poniżej 5. Dlatego tym bardziej warto przeprowadzić testowanie tej hipotezy za pomocą próbkowania.\n\nKodset.seed(44)\n# obliczamy statystykę testową dla próby\nsample_stat &lt;- gss |&gt; \n  drop_na(partyid, class) |&gt; \n  droplevels() |&gt; \n  specify(partyid~class) |&gt; \n  hypothesise(null = \"independence\") |&gt;\n  calculate(stat = \"Chisq\")\n\n# generujemy próby zgodne z hipotezą zerową za pomocą próbkowania\nnull_chisq_sim &lt;- gss |&gt; \n  drop_na(partyid, class) |&gt; \n  droplevels() |&gt;\n  specify(formula = partyid ~ class) |&gt; \n  hypothesise(null = \"independence\") |&gt; \n  generate(1000, type = \"permute\") |&gt; \n  calculate(stat = \"Chisq\")\n\n# porównujemy rozkład wynikający z hipotezy zerowej z\n# z wartością statystyki obliczoną z próby\nnull_chisq_sim |&gt; \n  visualise() + \n  shade_p_value(obs_stat = sample_stat, direction = \"right\")\n\n# obliczamy p-value\nnull_chisq_sim |&gt; \n  get_p_value(obs_stat = sample_stat, direction = \"right\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.002\n\n\n\n\nRysunek 3.9: Porównanie rozkładu statystyki testowej przy założeniu prawdziwości hipotezy zerowej z wartością statystyki testowej z próby\n\n\n\n\nWidzimy, że test każe odrzucić hipotezę o niezależności cech, podobnie jak test teoretyczny. Pakiet infer daje nam możliwość generowania rozkładu teoretycznego zgodnego z hipotezą zerową nieco inaczej niż na podstawie resamplingu.\n\nKodnull_chisq_theory &lt;- gss |&gt; \n  specify(partyid~class) |&gt; \n  assume(distribution = \"Chisq\", df = 9)\n\nnull_chisq_theory |&gt; \n  visualise() + \n  shade_p_value(obs_stat = sample_stat, \n                direction = \"right\")\npchisq(sample_stat$stat, df = 9, lower.tail = F)\n\n   X-squared \n0.0001314536 \n\n\n\n\nRysunek 3.10: Porównanie rozkładu teoretycznego statystyki testowej przy założeniu prawdziwości hipotezy zerowej z wartością statystyki testowej z próby\n\n\n\n\nOczywiście wynik otrzymany metodą teoretyczną pokrywa się dokładnie z wynikiem klasycznego testu \\(\\chi^2\\).\n\n\n\n\n\nCouch, Simon P., Andrew P. Bray, Chester Ismay, Evgeni Chasnovski, Benjamin S. Baumer, i Mine Çetinkaya-Rundel. 2021. „infer: An R package for tidyverse-friendly statistical inference” 6: 3661. https://doi.org/10.21105/joss.03661.\n\n\nKuhn, Max, i Hadley Wickham. 2020. „Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles.” https://www.tidymodels.org."
  },
  {
    "objectID": "measures.html#miary-dopasowania-modeli-regresyjnych",
    "href": "measures.html#miary-dopasowania-modeli-regresyjnych",
    "title": "\n4  Przegląd miar dopasowania modelu\n",
    "section": "\n4.1 Miary dopasowania modeli regresyjnych",
    "text": "4.1 Miary dopasowania modeli regresyjnych\nPrzegląd zaczniemy od najlepiej znanych miar, a skończymy na rzadziej stosowanych, jak funkcja straty Hubera.\n\n4.1.1 \\(R^2\\)\n\nMiara stosowana najczęściej do oceny dopasowania modeli liniowych, a definiowana jako:\n\\[\nR^2=1-\\frac{\\sum_i(y_i-\\hat{y}_i)^2}{\\sum_i(y_i-\\bar{y})^2},\n\\tag{4.1}\\]\ngdzie \\(\\hat{y}_i\\) jest \\(i\\)-tą wartością przewidywaną na podstawie modelu, \\(\\bar{y}\\) jest średnią zmiennej wynikowej, a \\(y_i\\) jest \\(i\\)-tą wartością obserwowaną. Już na kursie modeli liniowych dowiedzieliśmy się o wadach tak zdefiniowanej miary. Wśród nich należy wymienić przede wszystkim fakt, iż dołączając do modelu zmienne, których zdolność predykcyjna jest nieistotna3, to i tak rośnie \\(R^2\\)3 czyli nie mają znaczenia w przewidywaniu wartości wynikowej\nW przypadku modeli liniowych wprowadzaliśmy korektę eliminującą tą wadę, jednak w przypadku modeli predykcyjnych skorygowana miara \\(R^2_{adj}\\) nie wystarcza. W sytuacji gdy modele mają bardzo słabą moc predykcyjną, czyli są np. drzewem regresyjnym bez żadnej reguły podziału4, wówczas można otrzymać ujemne wartości obu miar. Zaleca się zatem wprowadzenie miary, która pozbawiona jest tej wady, a jednocześnie ma tą sama interpretację. Definiuję się ją następująco:4 drzewo składa się tylko z korzenia\n\\[\n\\tilde{R}^2=[\\operatorname{Cor}(Y, \\hat{Y})]^2.\n\\tag{4.2}\\]\nMiara zdefiniowana w (4.2) zapewnia nam wartości w przedziale (0,1), a klasyczna miara (4.1) nie (Kvalseth 1985). Tradycyjna jest zdefiniowana w bibliotece yardstick5 pod nazwą rsq_trad, natomiast miara oparta na korelacji jako rsq. Oczywiście interpretacja jest następująca, że jeśli wartość \\(\\tilde{R}^2\\) jest bliska 1, to model jest dobrze dopasowany, a bliskie 0 oznacza słabe dopasowanie.5 będącej częścią ekosystemu tidymodels\n\n4.1.2 RMSE\nInną powszechnie stosowaną miarą do oceny dopasowania modeli regresyjnych jest pierwiastek błędu średnio-kwadratowego (ang. Root Mean Square Error), zdefiniowany następująco:\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{n}},\n\\tag{4.3}\\]\ngdzie \\(n\\) oznacza liczebność zbioru danych na jakim dokonywana jest ocena dopasowania. Im mniejsza jest wartość błędu RMSE tym lepiej dopasowany jest model. Niestety wadą tej miary jest brak odporności na wartości odstające. Błąd w tym przypadku jest mierzony w tych samych jednostkach co mierzona wielkość wynikowa \\(Y\\). Do wywołania jej używamy funkcji rmse.\n\n4.1.3 MSE\nŚciśle powiązaną miarą dopasowania modelu z RMSE jest błąd średnio-kwadratowy (ang. Mean Square Error). Oczywiście jest on definiowany jako kwadrat RMSE. Interpretacja jest podobna jak w przypadku RMSE. W tym przypadku błąd jest mierzony w jednostkach do kwadratu i również jak w przypadku RMSE miara ta jest wrażliwa na wartości odstające. Wywołujemy ją funkcją mse.\n\n\n\n\n4.1.4 MAE\nChcąc uniknąć (choćby w części) wrażliwości na wartości odstające stosuje się miarę średniego absolutnego błędu (ang. Mean Absolut Error). Definiujemy go następująco:\n\\[\nMAE=\\frac{\\sum_{i=1}^n\\vert y_i-\\hat{y}_i\\vert}{n}.\n\\tag{4.4}\\]\nPonieważ wartości błędów \\(y_i-\\hat{y}_i\\) nie są podnoszone do kwadratu, to miara ta jest mniej wrażliwa na punkty odstające. Interpretacja jej jest podobna jak MSE i RMSE. Do wywołania jej używamy funkcji mae. Błąd w tym przypadku jest również mierzony w tych samych jednostkach co \\(Y\\).\nWymienione miary błędów są nieunormowane, a dopasowania modeli możemy dokonywać jedynie porównując wynik błędu z wartościami \\(Y\\), lub też przez porównanie miar dla różnych modeli.\n\n4.1.5 MAPE\nŚredni bezwzględny błąd procentowy (ang. Mean Absolute Percentage Error) jest przykładem miary błędu wyrażanego w procentach. Definiuje się go następująco:\n\\[\nMAPE=\\frac{1}{n}\\sum_{i=1}^n\\left|\\frac{y_i-\\hat{y}_i}{y_i}\\right|\\cdot 100\\%.\n\\tag{4.5}\\]\nInterpretujemy ten błąd podobnie jak poprzednie pomimo, że jest wyrażony w procentach. Do wywołania go w pakiecie yardstick używamy funkcji mape.\n\n4.1.6 MASE\nŚredni bezwzględny błąd skalowany (ang. Mean Absolute Scaled Error) jest miarą dokładności prognoz. Została zaproponowana w 2005 roku przez statystyka Roba J. Hyndmana i profesora Anne B. Koehler, którzy opisali ją jako “ogólnie stosowaną miarę dokładności prognoz bez problemów widocznych w innych miarach” (Hyndman i Koehler 2006). Średni bezwzględny błąd skalowany ma korzystne właściwości w porównaniu z innymi metodami obliczania błędów prognoz, takimi jak RMSE, i dlatego jest zalecany do określania dokładności prognoz w szeregach czasowych (Franses 2016). Definiujemy go następująco\n\\[\nMASE = \\frac{\\sum_{i=1}^n\\vert y_i-\\hat{y}_i\\vert}{\\sum_{i=1}^n\\vert y_i-\\bar{y}_i\\vert}.\n\\tag{4.6}\\]\nDla szeregów czasowych z sezonowością i bez sezonowości definiuje się go jeszcze nieco inaczej (Hyndman i Koehler 2006; 3.4 Evaluating Forecast Accuracy | Forecasting: Principles and Practice (2nd Ed), b.d.). Oczywiście interpretacja jest też podobna jak w przypadku innych miar błędów. Wywołujemy go funkcją mase.\n\n4.1.7 MPE\nŚredni błąd procentowy (ang. Mean Percentage Error) jest miarą błędu względnego definiowaną nastepująco\n\\[\nMPE = \\frac{1}{n}\\sum_{i=1}^n\\frac{y_i-\\hat{y}_i}{y_i}.\n\\tag{4.7}\\]\nPonieważ we wzorze wykorzystywane są rzeczywiste, a nie bezwzględne wartości błędów prognozy, dodatnie i ujemne błędy prognozy mogą się wzajemnie kompensować. W rezultacie wzór ten można wykorzystać jako miarę błędu systematycznego w prognozach. Wadą tej miary jest to, że jest ona zawsze określona, gdy pojedyncza wartość rzeczywista wynosi zero. Wywołujemy ją za pomocą mpe.\n\n4.1.8 MSD\nŚrednia znakowa różnic (ang. Mean Signed Deviation), znana również jako średnie odchylenie znakowe i średni błąd znakowy, jest statystyką próbkową, która podsumowuje, jak dobrze szacunki \\(\\hat{Y}\\) pasują do wielkości obserwowanych \\(Y\\). Definiujemy ją następująco:\n\\[\nMSD = \\frac{1}{n}\\sum_{i=1}^n(\\hat{y}_i-y_i).\n\\tag{4.8}\\]\nInterpretacja podobnie jak w przypadku innych błędów i mniej wynosi miara tym lepiej dopasowany model. Wywołujemy go funkcją msd.\nIstnieje cały szereg miar specjalistycznych rzadziej stosowanych w zagadnieniach regresyjnych. Wśród nich należy wymienić\n\n4.1.9 Funkcja straty Hubera\nFunkcja straty Hubera (ang. Huber loss) jest miarą błędu nieco bardziej odporną na punkty odstające niż RMSE. Definiujemy ją następująco:\n\\[\nL_{\\delta}(y, \\hat{y})= \\begin{cases}\n  \\frac12 (y_i-\\hat{y}_i)^2, &\\text{ jeśli }\\vert y_i-\\hat{y}_i\\vert\\leq\\delta\\\\\n  \\delta\\cdot \\vert y_i-\\hat{y}_i\\vert-\\tfrac12\\delta, &\\text{ w przeciwnym przypadku}.\n\\end{cases}\n\\tag{4.9}\\]\nW implementacji yardstick \\(\\delta=1\\) natomiast wyliczanie funkcji straty następuje przez uśrednienie po wszystkich obserwacjach. Z definicji widać, że funkcja straty Hubera jest kombinacją MSE i odpowiednio przekształconej miary MAE, w zależności od tego czy predykcja znacząco odbiegają od obserwowanych wartości. Wywołujemy ją przez funkcję huber_loss.\n\n4.1.10 Funkcja straty Pseudo-Hubera\nFunkcja straty Pseudo-Hubera (ang. Pseudo-Huber loss) może być stosowana jako gładkie przybliżenie funkcji straty Hubera. Łączy ona najlepsze właściwości straty kwadratowej6 i straty bezwzględnej7, będąc silnie wypukłą, gdy znajduje się blisko celu (minimum) i mniej stromą dla wartości ekstremalnych . Skala, przy której funkcja straty Pseudo-Hubera przechodzi od straty L2 dla wartości bliskich minimum do straty L1 może być kontrolowana przez parametr \\(\\delta\\). Funkcja straty Pseudo-Hubera zapewnia, że pochodne są ciągłe dla wszystkich stopni . Definiujemy ją następująco :6 inaczej w normie L27 w normie L1\n\\[\nL_{\\delta}(y-\\hat{y})=\\delta^2\\left(\\sqrt{1+((y-\\hat{y})/\\delta)^2}-1\\right).\n\\tag{4.10}\\] Wywołujemy ją za pomocą funkcji huber_loss_pseudo.\n\n4.1.11 Logarytm funkcji straty dla rozkładu Poissona\nLogarytm funkcji straty dla rozkładu Poissona (ang. Mean log-loss for Poisson data) definiowany jest w następujący sposób:\n\\[\n\\mathcal{L}=\\frac1n\\sum_{i=11}^n(\\hat{y}_i-y_i\\cdot \\ln(\\hat{y}_i)).\n\\]\nWywołujemy go funkcją poisson_log_los.\n\n4.1.12 SMAPE\nSymetryczny średni bezwzględny błąd procentowy (ang. Symmetric Mean Absolute Percentage Error) jest miarą dokładności opartą na błędach procentowych (lub względnych). Definiujemy ją następująco:\n\\[\nSMAPE = \\frac1n\\sum_{i=1}^n\\frac{\\vert y_i-\\hat{y}_i\\vert}{(|y_i|+|\\hat{y}_i|)/2}\\cdot100\\%.\n\\tag{4.11}\\]\nWywołujemy go funkcją smape.\n\n4.1.13 RPD\nStosunek wydajności do odchylenia standardowego (ang. Ratio of Performance to Deviation) definiujemy jako\n\\[\nRPD = \\frac{SD}{RMSE},\n\\tag{4.12}\\]\ngdzie \\(SD\\) oczywiście oznacza odchylenie standardowe zmiennej zależnej. Tym razem interpretujemy go w ten sposób, że im wyższa jest wartość RPD tym lepiej dopasowany model. Wywołujemy za pomocą rpd.\nW szczególności w dziedzinie spektroskopii, stosunek wydajności do odchylenia (RPD) został użyty jako standardowy sposób raportowania jakości modelu. Jest to stosunek odchylenia standardowego zmiennej do błędu standardowego przewidywania tej zmiennej przez dany model. Jednak jego systematyczne stosowanie zostało skrytykowane przez kilku autorów, ponieważ użycie odchylenia standardowego do reprezentowania rozrzutu zmiennej może być niewłaściwe w przypadku zbiorów danych z asymetrią rozkładów. Stosunek wydajności do rozstępu międzykwartylowego został wprowadzony przez Bellon-Maurel i in. (2010) w celu rozwiązania niektórych z tych problemów i uogólnienia RPD na zmienne o rozkładzie nienormalnym.\n\n4.1.14 RPIQ\nStosunek wartości do rozstępu międzykwartylowego (ang. Ratio of Performance to Inter-Quartile) definiujemy następująco:\n\\[\nRPIQ = \\frac{IQ}{RMSE},\n\\tag{4.13}\\]\ngdzie \\(IQ\\) oznacza rozstęp kwartylowy zmiennej zależnej. Wywołujemy go przez funkcję rpiq.\n\n4.1.15 CCC\nKorelacyjny współczynnik zgodności (ang. Concordance Correlation Coefficient) mierzy zgodność pomiędzy wartościami predykcji i obserwowanymi. Definiujemy go w następujący sposób:\n\\[\nCCC = \\frac{2\\rho\\sigma_y\\sigma_{\\hat{y}}}{\\sigma^2_{y}+\\sigma^2_{\\hat{y}}+(\\mu_y-\\mu_{\\hat{y}})^2},\n\\]\ngdzie \\(\\mu_y,\\mu_{\\hat{y}}\\) oznaczają średnią wartości obserwowanych i przewidywanych odpowiednio, \\(\\sigma_{y},\\sigma_{\\hat{y}}\\) stanowią natomiast odchylenia standardowe tych wielkości. \\(\\rho\\) jest współczynnikiem korelacji pomiędzy \\(Y\\) i \\(\\hat{Y}\\). Wywołanie w R to funkcja ccc.\n\n4.1.16 Podsumowanie miar dla modeli regresyjnych\nWśród miar dopasowania modelu można wyróżnić, te które mierzą zgodność pomiędzy wartościami obserwowanymi a przewidywanymi, wyrażone często pewnego rodzaju korelacjami (lub ich kwadratami), a interpretujemy je w ten sposób, że im wyższe wartości tych współczynników tym bardziej zgodne są predykcje z obserwacjami. Drugą duża grupę miar stanowią błędy (bezwzględne i względne), które mierzą w różny sposób różnice pomiędzy wartościami obserwowanymi i przewidywanymi. Jedne są bardziej odporne wartości odstające inne mniej, a wszystkie interpretujemy tak, że jeśli ich wartość jest mniejsza tym lepiej jest dopasowany model.\n\nPrzykład 4.1 Dla zilustrowania działania wspomnianych miar przeanalizujemy przykład modelu regresyjnego. Dla przykładu rozwiążemy zadanie przewidywania wytrzymałości betonu na podstawie jego parametrów. Do tego celu użyjemy danych ze zbioru concrete pakietu modeldata.(Yeh 2006)\n\nKodlibrary(tidymodels)\n\n# charakterystyka danych\nglimpse(concrete)\n\nRows: 1,030\nColumns: 9\n$ cement               &lt;dbl&gt; 540.0, 540.0, 332.5, 332.5, 198.6, 266.0, 380.0, …\n$ blast_furnace_slag   &lt;dbl&gt; 0.0, 0.0, 142.5, 142.5, 132.4, 114.0, 95.0, 95.0,…\n$ fly_ash              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ water                &lt;dbl&gt; 162, 162, 228, 228, 192, 228, 228, 228, 228, 228,…\n$ superplasticizer     &lt;dbl&gt; 2.5, 2.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,…\n$ coarse_aggregate     &lt;dbl&gt; 1040.0, 1055.0, 932.0, 932.0, 978.4, 932.0, 932.0…\n$ fine_aggregate       &lt;dbl&gt; 676.0, 676.0, 594.0, 594.0, 825.5, 670.0, 594.0, …\n$ age                  &lt;int&gt; 28, 28, 270, 365, 360, 90, 365, 28, 28, 28, 90, 2…\n$ compressive_strength &lt;dbl&gt; 79.99, 61.89, 40.27, 41.05, 44.30, 47.03, 43.70, …\n\nKod# modelowania dokonamy bez szczególnego uwzględnienia charakteru zmiennych,\n# tuningowania i innych czynności, które będą nam towarzyszyć w normalnej\n# budowie modelu\n\n# podział danych na uczące i testowe\nset.seed(44)\nsplit &lt;- initial_split(data = concrete,\n                       prop = 0.7)\ntrain_data &lt;- training(split)\ntest_data &lt;- testing(split)\n\n# określenie modeli, wybrałem kNN\nknn5 &lt;-\n  nearest_neighbor(neighbors = 5) |&gt; \n  set_engine('kknn') %&gt;%\n  set_mode('regression')\n\nknn25 &lt;-\n  nearest_neighbor(neighbors = 25) |&gt; \n  set_engine('kknn') %&gt;%\n  set_mode('regression')\n\n# uczymy modele\nfit5 &lt;- knn5 |&gt; \n  fit(compressive_strength~., data = train_data)\n\nfit25 &lt;- knn25 |&gt; \n  fit(compressive_strength~., data = train_data)\n\n# obliczamy predykcję dla obu modeli na obu zbiorach\npred_train5 &lt;- predict(fit5, train_data)\npred_train25 &lt;- predict(fit25, train_data)\npred_test5 &lt;- predict(fit5, test_data)\npred_test25 &lt;- predict(fit25, test_data)\n\n\n\nKodbind_cols(obs = c(train_data$compressive_strength, test_data$compressive_strength),\n          pred5 = c(pred_train5$.pred, pred_test5$.pred),\n          pred25 = c(pred_train25$.pred, pred_test25$.pred)) |&gt; \n  mutate(sample = rep(c(\"train\", \"test\"), c(nrow(train_data), nrow(test_data)))) |&gt; \n  pivot_longer(cols = c(pred5, pred25),\n               names_to = \"model\",\n               values_to = \"pred\") |&gt; \n  mutate(model = case_when(\n    model == \"pred5\" ~ \"knn5\",\n    model == \"pred25\" ~ \"knn25\"\n  )) |&gt; \n  ggplot(aes(x = obs, y = pred))+\n  geom_point(alpha = 0.1)+\n  geom_abline(intercept = 0, \n              slope = 1)+\n  facet_grid(sample~model)+\n  coord_obs_pred()\n\n\n\nRysunek 4.1: Graficzne porównanie obu modeli na obu zbiorach\n\n\n\n\n\nKod# podsumowanie za pomocą miary R2\nbind_cols(obs = c(train_data$compressive_strength, test_data$compressive_strength),\n          pred5 = c(pred_train5$.pred, pred_test5$.pred),\n          pred25 = c(pred_train25$.pred, pred_test25$.pred)) |&gt; \n  mutate(sample = rep(c(\"train\", \"test\"), c(nrow(train_data), nrow(test_data)))) |&gt; \n  pivot_longer(cols = c(pred5, pred25),\n               names_to = \"model\",\n               values_to = \"pred\") |&gt; \n  group_by(model, sample) |&gt; \n  rsq(truth = obs, estimate = pred) |&gt; \n  arrange(model)\n\n# A tibble: 4 × 5\n  model  sample .metric .estimator .estimate\n  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 pred25 test   rsq     standard       0.645\n2 pred25 train  rsq     standard       0.787\n3 pred5  test   rsq     standard       0.737\n4 pred5  train  rsq     standard       0.929\n\nKod# można też podsumować od razu kilkoma miarami\n# będa miary domyślne dla modelu regresyjnego\nbind_cols(obs = c(train_data$compressive_strength, test_data$compressive_strength),\n          pred5 = c(pred_train5$.pred, pred_test5$.pred),\n          pred25 = c(pred_train25$.pred, pred_test25$.pred)) |&gt; \n  mutate(sample = rep(c(\"train\", \"test\"), c(nrow(train_data), nrow(test_data)))) |&gt; \n  pivot_longer(cols = c(pred5, pred25),\n               names_to = \"model\",\n               values_to = \"pred\") |&gt; \n  group_by(model, sample) |&gt; \n  metrics(truth = obs, estimate = pred) |&gt; \n  arrange(model, .metric)\n\n# A tibble: 12 × 5\n   model  sample .metric .estimator .estimate\n   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n 1 pred25 test   mae     standard       7.73 \n 2 pred25 train  mae     standard       6.50 \n 3 pred25 test   rmse    standard       9.74 \n 4 pred25 train  rmse    standard       8.22 \n 5 pred25 test   rsq     standard       0.645\n 6 pred25 train  rsq     standard       0.787\n 7 pred5  test   mae     standard       6.33 \n 8 pred5  train  mae     standard       3.45 \n 9 pred5  test   rmse    standard       8.26 \n10 pred5  train  rmse    standard       4.68 \n11 pred5  test   rsq     standard       0.737\n12 pred5  train  rsq     standard       0.929\n\nKod# możemy zmienić parametry niektórych miar\nhuber_loss2 &lt;- metric_tweak(\"huber_loss2\", huber_loss, delta = 2)\n\n# można również wybrać jakie miary zostana użyte\nselected_metrics &lt;- metric_set(ccc, rpd, mape, huber_loss2)\n\n\nbind_cols(obs = c(train_data$compressive_strength, test_data$compressive_strength),\n          pred5 = c(pred_train5$.pred, pred_test5$.pred),\n          pred25 = c(pred_train25$.pred, pred_test25$.pred)) |&gt; \n  mutate(sample = rep(c(\"train\", \"test\"), c(nrow(train_data), nrow(test_data)))) |&gt; \n  pivot_longer(cols = c(pred5, pred25),\n               names_to = \"model\",\n               values_to = \"pred\") |&gt; \n  group_by(model, sample) |&gt; \n  selected_metrics(truth = obs, estimate = pred) |&gt; \n  arrange(model, sample)\n\n# A tibble: 16 × 5\n   model  sample .metric     .estimator .estimate\n   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n 1 pred25 test   ccc         standard       0.750\n 2 pred25 test   rpd         standard       1.64 \n 3 pred25 test   mape        standard      30.9  \n 4 pred25 test   huber_loss2 standard      13.6  \n 5 pred25 train  ccc         standard       0.851\n 6 pred25 train  rpd         standard       2.07 \n 7 pred25 train  mape        standard      24.8  \n 8 pred25 train  huber_loss2 standard      11.1  \n 9 pred5  test   ccc         standard       0.844\n10 pred5  test   rpd         standard       1.93 \n11 pred5  test   mape        standard      24.1  \n12 pred5  test   huber_loss2 standard      10.8  \n13 pred5  train  ccc         standard       0.958\n14 pred5  train  rpd         standard       3.64 \n15 pred5  train  mape        standard      12.8  \n16 pred5  train  huber_loss2 standard       5.19 \n\n\n\nW przypadku gdybyśmy chcieli zdefiniować własną miarę, to oczywiście jest taka możliwość8 polecam stronę pakietu yardstick - https://www.tidymodels.org/learn/develop/metrics/.8 choć liczba już istniejących jest imponująca"
  },
  {
    "objectID": "measures.html#miary-dopasowania-modeli-klasyfikacyjnych",
    "href": "measures.html#miary-dopasowania-modeli-klasyfikacyjnych",
    "title": "\n4  Przegląd miar dopasowania modelu\n",
    "section": "\n4.2 Miary dopasowania modeli klasyfikacyjnych",
    "text": "4.2 Miary dopasowania modeli klasyfikacyjnych\nJak to zostało wspomniane wcześniej w modelach klasyfikacyjnych można podzielić miary dopasowania na te, które dotyczą modeli z binarną zmienną wynikową i ze zmienna wielostanową. Miary można też podzielić na te, które zależą od prawdopodobieństwa poszczególnych stanów i te, które zależą tylko od klasyfikacji wynikowej.\nDo wyliczenia miar probabilistycznych konieczne jest wyliczenie predykcji z prawdopodobieństwami poszczególnych stanów. Aby uzyskać taki efekt wystarczy w predykcji modelu użyć parametru type = \"prob\". W przykładzie podsumowującym miary będzie to zilustrowane.\nNa to, aby przybliżyć miary dopasowania opartych o prawdopodobieństwa stanów, konieczne jest wprowadzenie pojęcia macierzy klasyfikacji (ang. confusion matrix). Można je stosować zarówno do klasyfikacji dwustanowej, jak i wielostanowej. Użyjemy przykładu binarnego aby zilustrować szczegóły tej macierzy.\n\nKod# import danych do przykładu\ndata(two_class_example)\n\n# kilka pierwszych wierszy wyników predykcji\nhead(two_class_example)\n\n   truth      Class1       Class2 predicted\n1 Class2 0.003589243 0.9964107574    Class2\n2 Class1 0.678621054 0.3213789460    Class1\n3 Class2 0.110893522 0.8891064779    Class2\n4 Class1 0.735161703 0.2648382969    Class1\n5 Class2 0.016239960 0.9837600397    Class2\n6 Class1 0.999275071 0.0007249286    Class1\n\nKod# confusion matrix\ntwo_class_example |&gt; \n  conf_mat(truth, predicted) |&gt; \n  autoplot(type = \"heatmap\")\n\n\n\nRysunek 4.2: Przykładowa macierz klasyfikacji\n\n\n\n\nAby przedstawić poszczególne miary na podstawie macierzy klasyfikacji wystarczy przywołać ilustrację z Wikipedii, która w genialny sposób podsumowuje większość miar.\n\n\nRysunek 4.3: Macierz klasyfikacji\n\n\nNa podstawie tej macierzy możemy ocenić dopasowanie modelu za pomocą:\n\naccuacy - informuje o odsetku poprawnie zaklasyfikowanych obserwacji. Jest bardzo powszechnie stosowaną miarą dopasowania modelu choć ma jedną poważną wadę. Mianowicie w przypadku modeli dla danych z wyraźną dysproporcją jednej z klas (powiedzmy jedna stanowi 95% wszystkich obserwacji), może się zdarzyć sytuacja, że nawet bezsensowny model, czyli taki, który zawsze wskazuje tą właśnie wartość, będzie miał accuracy na poziomie 95%.\nkappa - miara podobna miarą do accuracy i jest bardzo przydatna, gdy jedna lub więcej klas dominuje. Definiujemy ją następująco \\(\\kappa = \\frac{p_o-p_e}{1-p_e}\\), gdzie \\(p_o,p_e\\) są odpowiednio zgodnością obserwowaną i oczekiwaną. Zgodność obserwowana jest odsetkiem obserwacji poprawnie zaklasyfikowanych, a oczekiwana to zgodność wynikająca z przypadku.\nprecision - oznaczana też czasem jako PPV (ang. Positive Predictive Value) oznacza stosunek poprawnie zaklasyfikowanych wartości true positive (TP) do wszystkich przewidywanych wartości pozytywnych (ang. positive predictive).\nrecall - nazywany także sensitivity lub True Positive Rate (TPR), który stanowi stosunek true positive do wszystkich przypadków positive.\nspecificity - nazywane również True Negative Rate (TNR), wyraża się stosunkiem pozycji true negative do wszystkich obserwacji negative.\nnegative predictive value (NPV) - oznaczane czasem też jako false omission rate jest liczone jako stosunek false negative do wszystkich przewidywanych negative (PN).\n\\(F_1\\) - jest miarą zdefiniowaną jako \\(\\frac{2PPV*TPR}{PPV+TPR}\\).\nMCC - Mathews Correlation Coeficient - jest zdefiniowany jako \\(\\sqrt{TPR*TNR*PPV*NPV}-\\sqrt{FNR*FPR*NPV*FDR}\\). Istnieje również odmiana tej miary dla przypadku wielostanowej zmiennej wynikowej.\nbalanced accuracy - liczona jako średnia sensitivity i specificity.\ndetection prevalence - zdefiniowana jako stosunek poprawnie przewidywanych obserwacji do liczby wszystkich przewidywanych wartości.\nJ index - metryka Youden’a definiowana jako sensitivity + specificity -1, często jest wykorzystywana do określenia poziomu odcięcia prawdopodobieństwa zdarzenia wyróżnionego (ang. threshold).\nkoszt niepoprawnej klasyfikacji - czasami niektóre błędy klasyfikacji są mniej kosztowne z punktu widzenie badacza, a inne bardziej. Wówczas można przypisać koszty błędnych klasyfikacji do poszczególnych klas, nakładając kary za błędne przypisane do innej klasy i w ten sposób zapobiegać takim sytuacjom.\n\nśrednia logarytmu funkcji straty (ang. log loss) - określana też w literaturze jako binary cross-entropy dla przypadku zmiennej wynikowej dwustanowej i multilevel cross-entropy albo categorical cross-entropy w przypadku wielostanowej klasyfikacji. Definiuje się ją następująco:\n\\[\n    \\mathcal{L} = \\frac1n\\sum_{i=1}^n\\left[y_i\\log(\\hat{y}_i)+(1-y_i)\\log(1-\\hat{y}_i)\\right],\n\\tag{4.14}\\] gdzie \\(y_i\\) jest indykatorem klasy wyróżnionej dla \\(i\\)-tej obserwacji, a \\(\\hat{y}_i\\) prawdopodobieństwem wyróżnionego stanu \\(i\\)-tej obserwacji. Piękną rzeczą w tej definicji jest to, że jest ona ściśle związana z teorią informacji: log-loss jest entropią krzyżową pomiędzy rozkładem prawdziwych etykiet a przewidywaniami i jest bardzo blisko związana z tym, co jest znane jako entropia względna lub rozbieżność Kullbacka-Leiblera. Entropia mierzy nieprzewidywalność czegoś. Entropia krzyżowa zawiera entropię prawdziwego rozkładu plus dodatkową nieprzewidywalność, gdy ktoś zakłada inny rozkład niż prawdziwy. Tak więc log-loss jest miarą z teorii informacji pozwalającą zmierzyć “dodatkowy szum”, który powstaje w wyniku użycia predykcji w przeciwieństwie do prawdziwych etykiet. Minimalizując entropię krzyżową, maksymalizujemy dokładność klasyfikatora. Log-loss, czyli strata logarytmiczna, wnika w najdrobniejsze szczegóły działania klasyfikatora.\nPrawdopodobieństwo można rozumieć jako miernik zaufania. Jeśli prawdziwa etykieta to 0, ale klasyfikator ocenia, że należy ona do klasy 1 z prawdopodobieństwem 0,51, to pomimo tego, że klasyfikator popełniłby błąd, jest to niewielki błąd, ponieważ prawdopodobieństwo jest bardzo bliskie granicy decyzji 0,5. Log-loss jest subtelną miarą dokładności.\n\n\nNależy pamiętać, że większość wspomnianych miar opiera się na wartościach z macierzy klasyfikacji. Przy czym aby obserwacje zaklasyfikować do jednej z klas należy przyjąć pewien punkt odcięcia (threshold) prawdopodobieństwa, od którego przewidywana wartość będzie przyjmowała stan “1”. Domyślnie w wielu modelach ten punkt jest ustalony na poziomie 0,5. Nie jest on jednak optymalny ze względu na jakość klasyfikacji. Zmieniając ten próg otrzymamy różne wartości specificity, sensitivity, precision, recall, itd. Istnieją kryteria doboru progu odcięcia, np. oparte na wartości Youdena, F1, średniej geometrycznej itp. W przykładzie prezentowanym poniżej pokażemy zastosowanie dwóch z tych technik. Bez względu na przyjęty poziom odcięcia istnieją również miary i wykresy, które pozwalają zilustrować jakość modelu. Należą do nich:\n\nwykresy:\n\nROC - Receiver Operating Characteristic - krzywa, która przedstawia kompromis pomiędzy sensitivity i specificity dla różnych poziomów odcięcia. Ta egzotycznie brzmiąca nazwa wywodzi się z analizy sygnałów radiowych i została spopularyzowana w 1978 roku przez Charlesa Metza w artykule “Basic Principles of ROC Analysis”. Krzywa ROC pokazuje czułość klasyfikatora poprzez wykreślenie TPR do FPR. Innymi słowy, pokazuje ona, ile poprawnych pozytywnych klasyfikacji można uzyskać, gdy dopuszcza się coraz więcej fałszywych pozytywów. Idealny klasyfikator, który nie popełnia żadnych błędów, osiągnąłby natychmiast 100% wskaźnik prawdziwych pozytywów, bez żadnych fałszywych pozytywów - w praktyce prawie nigdy się to nie zdarza.\nPRC - Precision-Recall Curve - krzywa, która pokazuje kompromis pomiędzy precision i recall. Precision i recall to tak naprawdę dwie metryki. Jednak często są one używane razem. Precision odpowiada na pytanie: “Z elementów, które klasyfikator przewidział jako pozytywnych ile jest rzeczywiście pozytywnych?”. Natomiast recall odpowiada na pytanie: “Spośród wszystkich elementów, które są pozytywne, ile zostało przewidzianych jako takie przez klasyfikator?”. Krzywa PRC to po prostu wykres z wartościami Precision na osi y i Recall na osi x. Innymi słowy, krzywa PRC zawiera TP/(TP+FP) na osi y oraz TP/(TP+FN) na osi x.\nKrzywa wzrostu (ang. Gain Curve) - to krzywa przedstawiająca stosunek skumulowanej liczby pozytywnych (wyróżnionych) obserwacji w decylu do całkowitej liczby pozytywnych obserwacji w danych.\nKrzywa wyniesienia (ang. Lift Curve) - jest stosunkiem liczby pozytywnych obserwacji w \\(i\\)-tym decylu na podstawie modelu do oczekiwanej liczby pozytywnych obserwacji należących do \\(i\\)-tego decyla na podstawie modelu losowego.\n\n\nmiary:\n\nAUC - Area Under ROC Curve - mierzy pole pod krzywą ROC. Krzywa ROC nie jest pojedynczą liczbą ale całą krzywą. Dostarcza ona szczegółowych informacji o zachowaniu klasyfikatora, ale trudno jest szybko porównać wiele krzywych ROC ze sobą. W szczególności, jeśli ktoś chciałby zastosować jakiś automatyczny mechanizm tuningowania hiperparametrów, maszyna potrzebowałaby wymiernego wyniku zamiast wykresu, który wymaga wizualnej inspekcji. AUC jest jednym ze sposobów podsumowania krzywej ROC w jedną liczbę, tak aby można było ją łatwo i automatycznie porównać. Dobra krzywa ROC ma dużo miejsca pod sobą (ponieważ prawdziwy wskaźnik pozytywny bardzo szybko wzrasta do 100%). Zła krzywa ROC obejmuje bardzo mały obszar. Tak więc wysokie AUC jest sygnałem dobrego dopasowania modelu.\nPRAUC - Area Under Precision-Racall Curve - mierzy pole pod krzywą P-R.\nPole pod krzywą wzrosu.\nPole pod krzywą wyniesienia.\n\n\n\n\nPrzykład 4.2 Tym razem zbudujemy model klasyfikacyjny dla zmiennej wynikowej dwustanowej. Dane pochodzą ze zbioru attrition pakietu modeldata. Naszym zadaniem będzie zbudować modeli przewidujący odejścia z pracy.\n\nKodstr(attrition)\n\n'data.frame':   1470 obs. of  31 variables:\n $ Age                     : int  41 49 37 33 27 32 59 30 38 36 ...\n $ Attrition               : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 1 1 1 1 1 1 ...\n $ BusinessTravel          : Factor w/ 3 levels \"Non-Travel\",\"Travel_Frequently\",..: 3 2 3 2 3 2 3 3 2 3 ...\n $ DailyRate               : int  1102 279 1373 1392 591 1005 1324 1358 216 1299 ...\n $ Department              : Factor w/ 3 levels \"Human_Resources\",..: 3 2 2 2 2 2 2 2 2 2 ...\n $ DistanceFromHome        : int  1 8 2 3 2 2 3 24 23 27 ...\n $ Education               : Ord.factor w/ 5 levels \"Below_College\"&lt;..: 2 1 2 4 1 2 3 1 3 3 ...\n $ EducationField          : Factor w/ 6 levels \"Human_Resources\",..: 2 2 5 2 4 2 4 2 2 4 ...\n $ EnvironmentSatisfaction : Ord.factor w/ 4 levels \"Low\"&lt;\"Medium\"&lt;..: 2 3 4 4 1 4 3 4 4 3 ...\n $ Gender                  : Factor w/ 2 levels \"Female\",\"Male\": 1 2 2 1 2 2 1 2 2 2 ...\n $ HourlyRate              : int  94 61 92 56 40 79 81 67 44 94 ...\n $ JobInvolvement          : Ord.factor w/ 4 levels \"Low\"&lt;\"Medium\"&lt;..: 3 2 2 3 3 3 4 3 2 3 ...\n $ JobLevel                : int  2 2 1 1 1 1 1 1 3 2 ...\n $ JobRole                 : Factor w/ 9 levels \"Healthcare_Representative\",..: 8 7 3 7 3 3 3 3 5 1 ...\n $ JobSatisfaction         : Ord.factor w/ 4 levels \"Low\"&lt;\"Medium\"&lt;..: 4 2 3 3 2 4 1 3 3 3 ...\n $ MaritalStatus           : Factor w/ 3 levels \"Divorced\",\"Married\",..: 3 2 3 2 2 3 2 1 3 2 ...\n $ MonthlyIncome           : int  5993 5130 2090 2909 3468 3068 2670 2693 9526 5237 ...\n $ MonthlyRate             : int  19479 24907 2396 23159 16632 11864 9964 13335 8787 16577 ...\n $ NumCompaniesWorked      : int  8 1 6 1 9 0 4 1 0 6 ...\n $ OverTime                : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 1 1 2 1 1 1 ...\n $ PercentSalaryHike       : int  11 23 15 11 12 13 20 22 21 13 ...\n $ PerformanceRating       : Ord.factor w/ 4 levels \"Low\"&lt;\"Good\"&lt;\"Excellent\"&lt;..: 3 4 3 3 3 3 4 4 4 3 ...\n $ RelationshipSatisfaction: Ord.factor w/ 4 levels \"Low\"&lt;\"Medium\"&lt;..: 1 4 2 3 4 3 1 2 2 2 ...\n $ StockOptionLevel        : int  0 1 0 0 1 0 3 1 0 2 ...\n $ TotalWorkingYears       : int  8 10 7 8 6 8 12 1 10 17 ...\n $ TrainingTimesLastYear   : int  0 3 3 3 3 2 3 2 2 3 ...\n $ WorkLifeBalance         : Ord.factor w/ 4 levels \"Bad\"&lt;\"Good\"&lt;\"Better\"&lt;..: 1 3 3 3 3 2 2 3 3 2 ...\n $ YearsAtCompany          : int  6 10 0 8 2 7 1 1 9 7 ...\n $ YearsInCurrentRole      : int  4 7 0 7 2 7 0 0 7 7 ...\n $ YearsSinceLastPromotion : int  0 1 0 3 2 3 0 0 1 7 ...\n $ YearsWithCurrManager    : int  5 7 0 0 2 6 0 0 8 7 ...\n\nKod# podział zbioru na uczący i testowy\nset.seed(44)\nsplit &lt;- initial_split(attrition, prop = 0.7, strata = \"Attrition\")\ntrain_data &lt;- training(split)\ntest_data &lt;- testing(split)\n\n# określam model\nlr_mod &lt;- logistic_reg() |&gt; \n  set_engine(\"glm\") |&gt; \n  set_mode(\"classification\")\n\n# uczę model\nlr_fit &lt;- lr_mod |&gt; \n  fit(Attrition ~ ., data = train_data)\n\n# podsumowanie modelu\nlr_fit |&gt; \n  tidy() \n\n# A tibble: 58 × 5\n   term                             estimate   std.error statistic   p.value\n   &lt;chr&gt;                               &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)                     -3.41     1261.       -0.00270  0.998    \n 2 Age                             -0.0375      0.0179   -2.09     0.0364   \n 3 BusinessTravelTravel_Frequently  2.14        0.546     3.92     0.0000875\n 4 BusinessTravelTravel_Rarely      1.31        0.505     2.60     0.00942  \n 5 DailyRate                       -0.000213    0.000279 -0.766    0.444    \n 6 DepartmentResearch_Development   0.783    1261.        0.000621 1.00     \n 7 DepartmentSales                 13.7      1115.        0.0123   0.990    \n 8 DistanceFromHome                 0.0452      0.0137    3.30     0.000964 \n 9 Education.L                      0.353       0.435     0.811    0.417    \n10 Education.Q                      0.170       0.372     0.458    0.647    \n# ℹ 48 more rows\n\n\nTeraz korzystając z różnych miar podsumujemy dopasowanie modelu.\n\nKod# predykcja z modelu przyjmując threshold = 0.5\npred_class &lt;- predict(lr_fit, new_data = test_data)\n\n# predkcja (prawdopodobieństwa klas)\npred_prob &lt;- predict(lr_fit, new_data = test_data, type = \"prob\")\n\n# ale można też tak\npred &lt;- augment(lr_fit, test_data) |&gt; \n  select(Attrition, .pred_class, .pred_No, .pred_Yes)\n\n# macierz klasyfikacji\ncm &lt;- pred |&gt; \n  conf_mat(truth = Attrition, estimate = .pred_class)\ncm\n\n          Truth\nPrediction  No Yes\n       No  352  39\n       Yes  18  33\n\nKodsummary(cm)\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.871\n 2 kap                  binary         0.464\n 3 sens                 binary         0.951\n 4 spec                 binary         0.458\n 5 ppv                  binary         0.900\n 6 npv                  binary         0.647\n 7 mcc                  binary         0.474\n 8 j_index              binary         0.410\n 9 bal_accuracy         binary         0.705\n10 detection_prevalence binary         0.885\n11 precision            binary         0.900\n12 recall               binary         0.951\n13 f_meas               binary         0.925\n\n\nMożemy też narysować krzywe, które nam pokażą, czy dla innych wartości progu model też dobrze przewiduje klasy wynikowe.\n\nKod#ROC\npred |&gt; \n  roc_curve(truth = Attrition, .pred_Yes, event_level = \"second\") |&gt; \n  autoplot()\n\n\n\nKod#PRC\npred |&gt; \n  pr_curve(truth = Attrition, .pred_Yes, event_level = \"second\") |&gt; \n  autoplot()\n\n\n\nKod#AUC\npred |&gt; \n  roc_auc(truth = Attrition, .pred_Yes, event_level = \"second\") \n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.808\n\nKod#PRAUC\npred |&gt; \n  pr_auc(truth = Attrition, .pred_Yes, event_level = \"second\") \n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 pr_auc  binary         0.569\n\n\nWybór optymalnego progu odcięcia.\n\nKodlibrary(probably)\n\n# ustalam zakres threshold\nthresholds &lt;- seq(0.01,1, by = 0.01)\n\n# poszukuje najlepszego progu ze względu kryterium Youden'a\npred |&gt;\n  threshold_perf(Attrition, .pred_Yes, thresholds, event_level = \"second\") |&gt;\n  pivot_wider(names_from = .metric, values_from = .estimate) |&gt;\n  arrange(-j_index)\n\n# A tibble: 100 × 5\n   .threshold .estimator sensitivity specificity j_index\n        &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n 1       0.19 binary           0.722       0.838   0.560\n 2       0.17 binary           0.736       0.816   0.552\n 3       0.18 binary           0.722       0.824   0.547\n 4       0.16 binary           0.736       0.808   0.544\n 5       0.2  binary           0.694       0.846   0.540\n 6       0.22 binary           0.681       0.857   0.537\n 7       0.15 binary           0.736       0.797   0.533\n 8       0.21 binary           0.681       0.851   0.532\n 9       0.12 binary           0.75        0.773   0.523\n10       0.26 binary           0.639       0.884   0.523\n# ℹ 90 more rows\n\nKod# predykjca dla optymalnego progu\npred.optim &lt;- pred |&gt; \n  mutate(class = as.factor(ifelse(.pred_Yes &gt; 0.19, \"Yes\", \"No\")))\n\n# macierz klasyfikacji\ncm2 &lt;- pred.optim |&gt; \n  conf_mat(truth = Attrition, estimate = class)\ncm2\n\n          Truth\nPrediction  No Yes\n       No  310  20\n       Yes  60  52\n\nKodsummary(cm2)\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.819\n 2 kap                  binary         0.458\n 3 sens                 binary         0.838\n 4 spec                 binary         0.722\n 5 ppv                  binary         0.939\n 6 npv                  binary         0.464\n 7 mcc                  binary         0.475\n 8 j_index              binary         0.560\n 9 bal_accuracy         binary         0.780\n10 detection_prevalence binary         0.747\n11 precision            binary         0.939\n12 recall               binary         0.838\n13 f_meas               binary         0.886\n\n\n\n\n4.2.1 Miary dopasowania dla modeli ze zmienną wynikową wieloklasową\nWspomniane zostało, że miary dedykowane dla modeli binarnych można również wykorzystać do modeli ze zmienną zależną wielostanową. Oczywiście wówczas trzeba użyć pewnego rodzaju uśredniania. Implementacje wieloklasowe wykorzystują mikro, makro i makro-ważone uśrednianie, a niektóre metryki mają swoje własne wyspecjalizowane implementacje wieloklasowe.\n\n4.2.1.1 Makro uśrednianie\nMakro uśrednianie redukuje wieloklasowe predykcje do wielu zestawów przewidywań binarnych. Oblicza się odpowiednią metrykę dla każdego z przypadków binarnych, a następnie uśrednia wyniki. Jako przykład, rozważmy precision. W przypadku wieloklasowym, jeśli istnieją poziomy A, B, C i D, makro uśrednianie redukuje problem do wielu porównań jeden do jednego. Kolumny truth i estimate są rekodowane tak, że jedynymi dwoma poziomami są A i inne, a następnie precision jest obliczana w oparciu o te rekodowane kolumny, przy czym A jest “wyróżnioną” kolumną. Proces ten jest powtarzany dla pozostałych 3 poziomów, aby uzyskać łącznie 4 wartości precyzji. Wyniki są następnie uśredniane.\nFormuła dla \\(k\\) klas wynikowych prezentuje się następująco:\n\\[\nPr_{macro} = \\frac{Pr_1+Pr_2+\\ldots+Pr_k}{k},\n\\tag{4.15}\\]\ngdzie \\(Pr_i\\) oznacza precision dla \\(i\\)-tej klasy.\n\n4.2.1.2 Makro-ważone uśrednianie\nMakro-ważone uśrednianie jest co do zasady podobne do metody makro uśredniania, z tą jednak zmianą, że wagi poszczególnych czynników w średniej zależą od liczności tych klas, co sprawia, że miara ta jest bardziej optymalna w przypadku wyraźnych dysproporcji zmiennej wynikowej. Formalnie obliczamy to wg reguły:\n\\[\nPr_{weighted-macro}=Pr_1\\frac{\\#Obs_1}{n}+Pr_2\\frac{\\#Obs_2}{n}+\\ldots+Pr_k\\frac{\\#Obs_k}{n},\n\\tag{4.16}\\]\ngdzie \\(\\#Obs_i\\) oznacza liczbę obserwacji w grupie \\(i\\)-tej, a \\(n\\) jest liczebnością całego zbioru.\n\n4.2.1.3 Mikro uśrednianie\nMikro uśrednianie traktuje cały zestaw danych jako jeden wynik zbiorczy i oblicza jedną metrykę zamiast \\(k\\) metryk, które są uśredniane. Dla precision działa to poprzez obliczenie wszystkich true positive wyników dla każdej klasy i użycie tego jako licznika, a następnie obliczenie wszystkich true positive i false positive wyników dla każdej klasy i użycie tego jako mianownika.\n\\[\nPr_{micro} = \\frac{TP_1+TP_2+\\ldots TP_k}{(TP_1+TP_2+\\ldots TP_k)+(FP_1+FP_2+\\ldots FP_k)}.\n\\tag{4.17}\\]\nW tym przypadku, zamiast klas o równej wadze, mamy obserwacje z równą wagą. Dzięki temu klasy z największą liczbą obserwacji mają największy wpływ na wynik.\n\nPrzykład 4.3 Przykład użycia miar dopasowania modelu dla zmiennych wynikowych wieloklasowych.\n\nKod# predykcja wykonana dla sprawdzianu krzyżowego\n# bedzie nas interesować tylko wynik pierwszego folda\nhead(hpc_cv)\n\n  obs pred        VF          F           M            L Resample\n1  VF   VF 0.9136340 0.07786694 0.008479147 1.991225e-05   Fold01\n2  VF   VF 0.9380672 0.05710623 0.004816447 1.011557e-05   Fold01\n3  VF   VF 0.9473710 0.04946767 0.003156287 4.999849e-06   Fold01\n4  VF   VF 0.9289077 0.06528949 0.005787179 1.564496e-05   Fold01\n5  VF   VF 0.9418764 0.05430830 0.003808013 7.294581e-06   Fold01\n6  VF   VF 0.9510978 0.04618223 0.002716177 3.841455e-06   Fold01\n\nKod# macierz klasyfikacji\ncm &lt;- hpc_cv |&gt; \n  conf_mat(truth = obs, estimate = pred)\ncm\n\n          Truth\nPrediction   VF    F    M    L\n        VF 1620  371   64    9\n        F   141  647  219   60\n        M     6   24   79   28\n        L     2   36   50  111\n\nKod# poniższe miary są makro uśrednione\nsummary(cm)\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             multiclass     0.709\n 2 kap                  multiclass     0.508\n 3 sens                 macro          0.560\n 4 spec                 macro          0.879\n 5 ppv                  macro          0.631\n 6 npv                  macro          0.896\n 7 mcc                  multiclass     0.515\n 8 j_index              macro          0.440\n 9 bal_accuracy         macro          0.720\n10 detection_prevalence macro          0.25 \n11 precision            macro          0.631\n12 recall               macro          0.560\n13 f_meas               macro          0.570\n\nKod# poniższe miary są makro uśrednione\nsummary(cm, estimator = \"micro\")\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             multiclass     0.709\n 2 kap                  multiclass     0.508\n 3 sens                 micro          0.709\n 4 spec                 micro          0.903\n 5 ppv                  micro          0.709\n 6 npv                  micro          0.903\n 7 mcc                  multiclass     0.515\n 8 j_index              micro          0.612\n 9 bal_accuracy         micro          0.806\n10 detection_prevalence micro          0.25 \n11 precision            micro          0.709\n12 recall               micro          0.709\n13 f_meas               micro          0.709\n\nKod# ROC\nhpc_cv |&gt; \n  roc_curve(truth = obs, VF:L) |&gt; \n  autoplot()\n\n\n\nKod# AUC\nhpc_cv |&gt; \n  roc_auc(truth = obs, VF:L)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc hand_till      0.829"
  },
  {
    "objectID": "measures.html#uwagi-końcowe",
    "href": "measures.html#uwagi-końcowe",
    "title": "\n4  Przegląd miar dopasowania modelu\n",
    "section": "\n4.3 Uwagi końcowe",
    "text": "4.3 Uwagi końcowe\nStosując jedna miarę dopasowania modelu (bez względu na to czy jest to model klasyfikacyjny czy regresyjny) możemy nie otrzymać optymalnego modelu. Ze względu na definicje miary dopasowania różnią się pomiędzy sobą eksponując nieco inne aspekty. To powoduje, że może się zdarzyć sytuacja, że optymalny model ze względu na \\(R^2\\) będzie się różnił (nawet znacznie) od modelu optymalizowanego z użyciem RMSE (patrz Rysunek 4.4).\n\n\nRysunek 4.4: Porównanie jakości modeli z wykorzystaniem różnych miar\n\n\nModel zoptymalizowany pod kątem RMSE ma większą zmienność, ale ma stosunkowo jednolitą dokładność w całym zakresie wyników. Prawy panel pokazuje, że istnieje silniejsza korelacja między wartościami obserwowanymi i przewidywanymi, ale model ten słabo radzi sobie w przewidywaniu skrajnych wartości. Na marginesie można dodać, że jeśli model miałby być stosowany do predykcji (co zdarza się bardzo często w modelach ML), to miara RMSE jest lepsza, natomiast gdy interesują nas poszczególne efekty modelu regresji, wówczas \\(R^2\\) jest częściej stosowaną miarą oceny dopasowania modelu.\nPodobny przykład można przytoczyć również dla modeli klasyfikacyjnych.\nOcena skuteczności danego modelu zależy od tego, do czego będzie on wykorzystywany. Model inferencyjny jest używany przede wszystkim do zrozumienia związków i zazwyczaj kładzie nacisk na wybór (i ważność) rozkładów probabilistycznych i innych cech generatywnych, które definiują model. Dla modelu używanego głównie do przewidywania, w przeciwieństwie do tego, siła predykcyjna ma podstawowe znaczenie, a inne obawy dotyczące podstawowych właściwości statystycznych mogą być mniej ważne. Siła predykcyjna jest zwykle określana przez to, jak blisko nasze przewidywania zbliżają się do obserwowanych danych, tj. wierność przewidywań modelu do rzeczywistych wyników. W tym rozdziale skupiono się na funkcjach, które można wykorzystać do pomiaru siły predykcji. Jednakże naszą radą dla osób opracowujących modele inferencyjne jest stosowanie tych technik nawet wtedy, gdy model nie będzie używany z głównym celem przewidywania.\nOd dawna problemem w praktyce statystyki inferencyjnej jest to, że skupiając się wyłącznie na wnioskowaniu, trudno jest ocenić wiarygodność modelu. Na przykład, rozważ dane dotyczące choroby Alzheimera z Craig-Schapiro i in. (2011), gdzie 333 pacjentów było badanych w celu określenia czynników wpływających na upośledzenie funkcji poznawczych. W analizie można uwzględnić znane czynniki ryzyka i zbudować model regresji logistycznej, w którym wynik jest binarny (upośledzony/nieupośledzony). Rozważmy predyktory wieku, płci i genotypu apolipoproteiny E. Ta ostatnia jest zmienną kategoryczną zawierającą sześć możliwych kombinacji trzech głównych wariantów tego genu. Wiadomym jest, że apolipoproteina E ma związek z demencją (Kim, Basak, i Holtzman 2009).\nPowierzchowne, ale nierzadkie podejście do tej analizy polegałoby na dopasowaniu dużego modelu z głównymi efektami i interakcjami, a następnie zastosowaniu testów statystycznych w celu znalezienia minimalnego zestawu efektów modelu, które są statystycznie istotne na jakimś wcześniej zdefiniowanym poziomie. Jeśli użyto pełnego modelu z trzema czynnikami i ich dwu- i trójstronnymi interakcjami, wstępnym etapem byłoby przetestowanie interakcji przy użyciu sekwencyjnych testów ilorazu prawdopodobieństwa (Hosmer i Lemeshow 2000). Przejdźmy przez takie podejście dla przykładowych danych dotyczących choroby Alzheimera:\n\nPorównując model ze wszystkimi interakcjami dwukierunkowymi z modelem z dodatkową interakcją trójkierunkową, testy ilorazu wiarygodności dają wartość \\(p\\) równą 0,888. Oznacza to, że nie ma dowodów na to, że cztery dodatkowe terminy modelu związane z interakcją trójkierunkową wyjaśniają wystarczająco dużo zmienności w danych, aby zachować je w modelu.\nNastępnie, dwukierunkowe interakcje są podobnie oceniane w stosunku do modelu bez interakcji. Wartość \\(p\\) wynosi tutaj 0,0382. Biorąc pod uwagę niewielki rozmiar próbki, byłoby rozsądnie stwierdzić, że istnieją dowody na to, że niektóre z 10 możliwych dwukierunkowych interakcji są ważne dla modelu.\nNa tej podstawie wyciągnęlibyśmy pewne wnioski z uzyskanych wyników. Interakcje te byłyby szczególnie ważne do omówienia, ponieważ mogą one zapoczątkować interesujące fizjologiczne lub neurologiczne hipotezy, które należy dalej badać.\n\nChociaż jest to strategia uproszczona, jest ona powszechna zarówno w praktyce, jak i w literaturze. Jest to szczególnie częste, jeśli praktykujący ma ograniczone formalne wykształcenie w zakresie analizy danych.\nJedną z brakujących informacji w tym podejściu jest to, jak blisko model ten pasuje do rzeczywistych danych. Stosując metody resamplingu, omówione w rozdziale 10, możemy oszacować dokładność tego modelu na około 73,4%. Dokładność jest często słabą miarą wydajności modelu; używamy jej tutaj, ponieważ jest powszechnie rozumiana. Jeśli model ma 73,4% dokładności w stosunku do danych, to czy powinniśmy zaufać wnioskom, które produkuje? Moglibyśmy tak myśleć, dopóki nie zdamy sobie sprawy, że wskaźnik bazowy nieupośledzonych pacjentów w danych wynosi 72,7%. Oznacza to, że pomimo naszej analizy statystycznej, model dwuczynnikowy okazuje się być tylko o 0,8% lepszy od prostej heurystyki, która zawsze przewiduje, że pacjenci nie są upośledzeni, niezależnie od obserwowanych danych.\n\n\n\n\n3.4 Evaluating Forecast Accuracy | Forecasting: Principles and Practice (2nd Ed). b.d.\n\n\nBellon-Maurel, Véronique, Elvira Fernandez-Ahumada, Bernard Palagos, Jean-Michel Roger, i Alex McBratney. 2010. „Critical Review of Chemometric Indicators Commonly Used for Assessing the Quality of the Prediction of Soil Attributes by NIR Spectroscopy”. TrAC Trends in Analytical Chemistry 29 (9): 1073–81. https://doi.org/10.1016/j.trac.2010.05.006.\n\n\nCraig-Schapiro, Rebecca, Max Kuhn, Chengjie Xiong, Eve H. Pickering, Jingxia Liu, Thomas P. Misko, Richard J. Perrin, i in. 2011. „Multiplexed Immunoassay Panel Identifies Novel CSF Biomarkers for Alzheimer’s Disease Diagnosis and Prognosis”. Zredagowane przez Ashley I. Bush. PLoS ONE 6 (4): e18850. https://doi.org/10.1371/journal.pone.0018850.\n\n\nFranses, Philip Hans. 2016. „A Note on the Mean Absolute Scaled Error”. International Journal of Forecasting 32 (1): 20–22. https://doi.org/10.1016/j.ijforecast.2015.03.008.\n\n\nHosmer, David W., i Stanley Lemeshow. 2000. Applied Logistic Regression. John Wiley & Sons, Inc. https://doi.org/10.1002/0471722146.\n\n\nHyndman, Rob J., i Anne B. Koehler. 2006. „Another Look at Measures of Forecast Accuracy”. International Journal of Forecasting 22 (4): 679–88. https://doi.org/10.1016/j.ijforecast.2006.03.001.\n\n\nKim, Jungsu, Jacob M. Basak, i David M. Holtzman. 2009. „The Role of Apolipoprotein E in Alzheimer’s Disease”. Neuron 63 (3): 287–303. https://doi.org/10.1016/j.neuron.2009.06.026.\n\n\nKvalseth, Tarald O. 1985. „Cautionary Note about R2”. The American Statistician 39 (4): 279–85. https://doi.org/10.2307/2683704.\n\n\nYeh, I.-Cheng. 2006. „Analysis of Strength of Concrete Using Design of Experiments and Neural Networks”. Journal of Materials in Civil Engineering 18 (4): 597–604. https://doi.org/10.1061/(ASCE)0899-1561(2006)18:4(597)."
  },
  {
    "objectID": "resampling.html#podział-na-próbę-uczącą-i-testową",
    "href": "resampling.html#podział-na-próbę-uczącą-i-testową",
    "title": "\n5  Zarządzanie danymi\n",
    "section": "\n5.1 Podział na próbę uczącą i testową",
    "text": "5.1 Podział na próbę uczącą i testową\nPodstawowym podejściem w empirycznej walidacji modelu jest podział istniejącej puli danych na dwa odrębne zbiory - treningowy i testowy. Jedna część danych jest wykorzystywana do budowy i optymalizacji modelu. Ten zbiór treningowy stanowi zazwyczaj większość danych. Dane te służą dla budowy modelu, poszukiwania optymalnych parametrów modelu, czy selekcji cech istotnych z punktu widzenia predykcji.\nPozostała część danych stanowi zbiór testowy. Jest on trzymany aż do momentu, gdy jeden lub dwa modele zostaną wybrane jako metody najlepiej opisujące badane zjawisko. Zestaw testowy jest wtedy używany jako ostateczny arbiter do określenia dopasowania modelu. Krytyczne jest, aby użyć zbiór testowy tylko raz; w przeciwnym razie staje się on częścią procesu modelowania.\nProporcje w jakich należy podzielić dane nie są wyraźnie sprecyzowane. Choć istnieją prace, jak np. Joseph (2022), które wskazują konkretne reguły podziału zbioru danych na uczący i testowy dla modeli regresyjnych, to nie ma co do tego powszechnej zgody, że tylko jedna proporcja jest optymalna. W przypadku wspomnianej metody próba testowa powinna stanowić\n\\[\n\\frac{1}{\\sqrt{p}+1},\n\\]\nzbioru danych, gdzie \\(p\\) oznacza liczbę predyktorów modelu.\n\n\n\n\n\n\nWskazówka\n\n\n\nDecydując się na wybór proporcji podziału należy pamiętać, że przy mniejszej ilości danych uczących, oszacowania parametrów mają wysoką wariancję. Z drugiej strony, mniejsza ilość danych testowych prowadzi do wysokiej wariancji w miarach dopasowania (czyli mamy duże różnice pomiędzy dopasowaniem na zbiorze uczącym i testowym).\n\n\nW podziale zbioru danych na uczący i testowy, ważny jest jeszcze jeden aspekt. W jaki sposób podziału dokonujemy. Najpowszechniej stosowany jest podział losowy (losowanie proste). Próbkowanie losowe jest najstarszą i najbardziej popularną metodą dzielenia zbioru danych. Jak sama nazwa wskazuje, losowane są indeksy obserwacji, które będą przyporządkowane do zbioru uczącego. Pozostałe obserwacje będą należeć do zbioru testowego.\nMetoda ta ma jednak istotną wadę. Próbkowanie losowe działa poprawnie na zbiorach danych zbalansowanych klasowo, czyli takich, w których liczba próbek w każdej kategorii jest mniej więcej taka sama. W przypadku zbiorów danych niezbalansowanych klasowo, taka metoda podziału danych może tworzyć obciążenie modelu.\nNa przykład, jeśli zbiór danych zawiera 100 obrazów, z których 80 należy do kategorii “pies” i 20 należy do kategorii “kot”, a losowe próbkowanie jest stosowane do podziału danych na zbiory uczący i testowy w stosunku 80%-20% (odpowiednio), może się tak zdarzyć, że zbiór treningowy składa się tylko z obrazów psów, podczas gdy zbiór testowy składa się tylko z obrazów kotów. Nawet jeśli nie zdarzy się tak ekstremalny przypadek, to nierównowaga rozkładów w obu zbiorach może być wyraźna.\nLosowanie warstwowe zastosowane do podziału zbioru danych łagodzi problem próbkowania losowego w zbiorach danych z niezrównoważonym rozkładem klas. W tym przypadku, rozkład klas w każdym z zestawów treningowych i testowych jest zachowany.\nZałóżmy, że zbiór danych składa się z 100 obrazów, z których 60 to obrazy psów, a 40 to obrazy kotów. W takim przypadku próbkowanie warstwowe zapewnia, że 60% obrazów należy do kategorii “pies”, a 40% do kategorii “kot” w zbiorach uczącym i testowym. Oznacza to, że jeśli pożądany jest podział w proporcji 80%-20%, z 80 obrazów w zbiorze treningowym, 48 obrazów (60%) będzie należało do psów, a pozostałe 32 (40%) do kotów.\nRozważmy inny przykład. W zadaniach wykrywania obiektów, pojedyncze obrazy mogą zawierać kilka różnych obiektów należących do różnych kategorii. W zbiorze danych niektóre obrazy mogą zawierać 10 psów, ale tylko 1 osobę, inne mogą zawierać 10 osób i 2 psy, a jeszcze inne mogą zawierać 5 kotów i 5 psów, bez ludzi. W takich przypadkach, losowy podział obrazów może zakłócić rozkład kategorii obiektów. Próbkowanie warstwowe z drugiej strony pozwala podzielić dane w taki sposób, że wynikowy rozkład kategorii obiektów jest zrównoważony.\n\n\n\n\n\n\nWskazówka\n\n\n\nW przypadku dużych zbiorów danych różnice pomiędzy losowaniem prostym i warstwowym się zacierają.\n\n\n\nPrzykład 5.1 Przykładem zastosowania obu wspomnianych technik będzie podział zbioru attrition pakietu modeldata.\n\nKodlibrary(tidymodels)\n# proporcje zmiennej wynikowej w całym zbiorze\nset.seed(4)\ndane &lt;- attrition |&gt; \n  slice_sample(n = 100)\n\ndane|&gt; \n  count(Attrition) |&gt; \n  mutate(prop = round(n/sum(n)*100, 0))\n\n  Attrition  n prop\n1        No 83   83\n2       Yes 17   17\n\nKodset.seed(4)\n# podział losowy\nrandom_split &lt;- initial_split(dane, prop = 0.9)\nrandom_split\n\n&lt;Training/Testing/Total&gt;\n&lt;90/10/100&gt;\n\nKodtr_random &lt;- training(random_split)\nte_random &lt;- testing(random_split)\n\n# podział warstwowy\nstrata_split &lt;- initial_split(dane, prop = 0.9, strata = Attrition)\nstrata_split\n\n&lt;Training/Testing/Total&gt;\n&lt;89/11/100&gt;\n\nKodtr_strata &lt;- training(strata_split)\nte_strata &lt;- testing(strata_split)\n\n# proporcje zmiennej wynikowej w obu zbiorach uczących\ntr_random |&gt; \n  count(Attrition) |&gt; \n  mutate(prop = round(n/sum(n)*100, 0))\n\n  Attrition  n prop\n1        No 73   81\n2       Yes 17   19\n\nKodtr_strata |&gt; \n  count(Attrition) |&gt; \n  mutate(prop = round(n/sum(n)*100, 0))\n\n  Attrition  n prop\n1        No 74   83\n2       Yes 15   17\n\n\n\nCo ciekawe parametru strata w funkcji initial_split można też użyć do zmiennej ilościowej. Wówczas algorytm podzieli próbę z zachowaniem proporcji obserwacji w kolejnych kwartylach. Ta technika może się okazać szczególnie ważna w przypadku gdy zmienna wynikowa ma znaczącą asymetrię rozkładu.\n\nPrzykład 5.2  \n\nKod# rozkład zmiennej Sale_Price\nset.seed(44)\ndane &lt;- ames |&gt; \n  slice_sample(n = 200)\np1 &lt;- dane |&gt; \n  ggplot(aes(Sale_Price))+\n  geom_density(fill = \"grey\")+\n  labs(title = \"Oryginał\")\n\n# podział losowy\nset.seed(44)\names_split &lt;- initial_split(dane, prop = 0.8)\names_train1 &lt;- training(ames_split)\names_test1 &lt;- testing(ames_split)\n\np2 &lt;- bind_rows(ames_train1, ames_test1) |&gt; \n  mutate(sample = rep(c(\"train\", \"test\"), \n                      c(nrow(ames_train1), nrow(ames_test1)))) |&gt;\n  ggplot(aes(Sale_Price))+\n  geom_density(aes(fill = sample), alpha = 0.5)+\n  labs(title = \"Losowanie proste\")\n\n\names_split &lt;- initial_split(dane, prop = 0.8, strata = Sale_Price)\names_train2 &lt;- training(ames_split)\names_test2 &lt;- testing(ames_split)\n\np3 &lt;- bind_rows(ames_train2, ames_test2) |&gt; \n  mutate(sample = rep(c(\"train\", \"test\"), \n                      c(nrow(ames_train1), nrow(ames_test1)))) |&gt;\n  ggplot(aes(Sale_Price))+\n  geom_density(aes(fill = sample), alpha = 0.5)+\n  labs(title = \"Losowanie warstwowe\")\n\nlibrary(patchwork)\np1/p2/p3\n\n\n\nRysunek 5.1: Porównanie próbkowań dla zmiennej typu ilościowego\n\n\n\n\n\n\n\n\n\n\n\nWskazówka\n\n\n\nProporcja użyta do podziału, w dużym stopniu zależy od kontekstu rozpatrywanego problemu. Zbyt mała ilość danych w zbiorze treningowym ogranicza zdolność modelu do znalezienia odpowiednich oszacowań parametrów. I odwrotnie, zbyt mała ilość danych w zbiorze testowym obniża jakość oszacowań wydajności. Niektórzy statystycy unikają zbiorów testowych w ogóle, ponieważ uważają, że wszystkie dane powinny być używane do estymacji parametrów. Chociaż argument ten jest słuszny, dobrą praktyką modelowania jest posiadanie bezstronnego zestawu obserwacji jako ostatecznego arbitra jakości modelu. Zestawu testowego należy unikać tylko wtedy, gdy dane są drastycznie małe.\n\n\nJeszcze jedna uwaga na temat losowego podziału zbioru na uczący i testowy. W jednej sytuacji podział losowy i warstwowy nie są najlepszym rozwiązaniem - chodzi o szeregi czasowe lub dane zawierające znaczący czynnik zmienności w czasie. Wówczas stosuje się podział zbioru za pomocą funkcji initial_time_split, która parametrem prop określa jaka proporcja obserwacji z początku zbioru danych będzie wybrana do zbioru uczącego1.1 zakłada się, że dane są posortowane wg czasu\nOpisując cele podziału danych, wyróżniliśmy zbiór testowy jako dane, które powinny być wykorzystane do właściwej oceny działania modelu na modelu końcowym (modelach). To rodzi pytanie: “Jak możemy określić, co jest najlepsze, jeśli nie mierzymy wydajności aż do zbioru testowego?”.\nCzęsto słyszy się o zbiorach walidacyjnych jako odpowiedzi na to pytanie, szczególnie w literaturze dotyczącej sieci neuronowych i głębokiego uczenia. U początków sieci neuronowych badacze zdali sobie sprawę, że mierzenie wydajności poprzez przewidywanie na podstawie zbioru treningowego prowadziło do wyników, które były zbyt optymistyczne (nierealistycznie). Prowadziło to do modeli, które były nadmiernie dopasowane, co oznaczało, że osiągały bardzo dobre wyniki na zbiorze treningowym, ale słabe na zbiorze testowym. Aby ograniczyć ten problem, wybierano ze zbioru uczącego niewielki zbiór walidacyjny i użyto go do pomiaru wydajności podczas trenowania sieci. Gdy poziom błędu na zbiorze walidacyjnym zaczynał rosnąć, trening był wstrzymywany. Innymi słowy, zbiór walidacyjny był środkiem do uzyskania przybliżonego poczucia, jak dobrze model działał przed zbiorem testowym.\nW przypadku danych dotyczących mieszkań w Ames, nieruchomość jest traktowana jako niezależna jednostka eksperymentalna. Można bezpiecznie założyć, że statystycznie dane z danej nieruchomości są niezależne od innych nieruchomości. W przypadku innych zastosowań nie zawsze tak jest:\n\nNa przykład w przypadku danych podłużnych (ang. longitudinal data) ta sama niezależna jednostka eksperymentalna może być mierzona w wielu punktach czasowych. Przykładem może być osoba w badaniu medycznym.\nPartia wyprodukowanego produktu może być również uważana za niezależną jednostkę doświadczalną. W powtarzanych pomiarach, replikowane punkty danych z partii są zbierane w wielu momentach.\n\nJohnson i in. (2018) opisują eksperyment, w którym próbki różnych drzew były pobierane w górnej i dolnej części pnia. Tutaj drzewo jest jednostką eksperymentalną, a hierarchię danych stanowi próbka w obrębie części pnia dla każdego drzewa.\n\nKuhn i Johnson (2019) zawiera inne przykłady, w których dla pojedyncze obserwacje mierzone są kilkukrotnie, losowanie przypadków wśród wierszy nie zapewni niezależności obserwacji. Wówczas powinno się losować całe zestawy obserwacji przyporządkowanych do jednostki eksperymentalnej. Operatem losowania powinna być lista jednostek eksperymentalnych.\n\n\n\n\n\n\nWażne\n\n\n\nProblem wycieku informacji (ang. data leakage) występuje, gdy w procesie modelowania wykorzystywane są dane spoza zbioru treningowego\n\n\nNa przykład, w konkursach uczenia maszynowego, dane zbioru testowego mogą być dostarczone bez prawdziwych wartości zmiennej wynikowej, aby model mógł być rzetelnie oceniony. Jedną z potencjalnych metod poprawy wyniku może być dopasowanie modelu przy użyciu obserwacji zbioru treningowego, które są najbardziej podobne do wartości zbioru testowego. Chociaż zbiór testowy nie jest bezpośrednio wykorzystywany do dopasowania modelu, nadal ma duży wpływ. Ogólnie rzecz biorąc, ta technika jest wysoce problematyczna, ponieważ zmniejsza błąd generalizacji modelu w celu optymalizacji wydajności na określonym zbiorze danych. Istnieją bardziej subtelne sposoby, w jakie dane z zestawu testowego mogą być wykorzystywane podczas szkolenia. Zapisywanie danych treningowych w oddzielnej ramce danych od zbioru testowego jest jednym sposobem zapewnienia, aby wyciek informacji nie nastąpił przez przypadek.\nPo drugie, techniki podpróbkowania (ang. subsampling) zbioru treningowego mogą łagodzić pewne problemy (np. nierównowagę klas). Jest to ważna i powszechna technika, która celowo powoduje, że dane zbioru treningowego odbiegają od populacji, z której dane zostały pobrane. Ważne jest, aby zbiór testowy nadal odzwierciedlał to, co model napotkałby w środowisku naturalnym.\nNa początku tego rozdziału, ostrzegaliśmy przed używaniem tych samych danych do różnych zadań. W dalszej części tej książki omówimy metodologie wykorzystania danych, które pozwolą zmniejszyć ryzyko związane z tendencyjnością, nadmiernym dopasowaniem i innymi problemami.\n\n\n\n\nJohnson, Dan, Phoebe Eckart, Noah Alsamadisi, Hilary Noble, Celia Martin, i Rachel Spicer. 2018. „Polar Auxin Transport Is Implicated in Vessel Differentiation and Spatial Patterning During Secondary Growth in Populus”. American Journal of Botany 105 (2): 186–96. https://doi.org/10.1002/ajb2.1035.\n\n\nJoseph, V. Roshan. 2022. „Optimal Ratio for Data Splitting”. Statistical Analysis and Data Mining: The ASA Data Science Journal 15 (4): 531–38. https://doi.org/10.1002/sam.11583.\n\n\nKuhn, Max, i Kjell Johnson. 2019. Feature Engineering and Selection. Chapman; Hall/CRC. https://doi.org/10.1201/9781315108230."
  },
  {
    "objectID": "tidymodels.html#budowa-modelu",
    "href": "tidymodels.html#budowa-modelu",
    "title": "\n6  Praca z tidymodels\n",
    "section": "\n6.1 Budowa modelu",
    "text": "6.1 Budowa modelu\nOddzielny pakiet przeznaczony do budowy modeli zawarty w ekosystemie tidymodels o nazwie parsnip pozwala w uniwersalny sposób budować i dopasowywać modele. Wracając do przykładu modelu liniowego postaramy się pokazać wszystkie zalety tego podejścia. Choć regresje liniową możemy zbudować z wykorzystaniem 11 różnych pakietów, to my się ograniczymy tylko do stats, glmnet i rstanarm.\n\nKod# w wersji klasycznej należałoby je budować następująco\nmod_stat &lt;- lm(formula, data = ...)\nmod_glmnet &lt;- glmnet(x = matrix, y = vector, family = \"gaussian\", ...)\nmod_stan &lt;- stan_glm(formula, data, family = \"gaussian\", ...)\n\n\nJuż na poziomie definiowana modeli widzimy różnice w definicjach, np. glmnet potrzebuje danych w formacie \\((X,y)\\). W przypadku tidymodels podejście do określania modelu ma być bardziej zunifikowane:\n\nOkreśl typ modelu na podstawie jego struktury matematycznej (np. regresja liniowa, las losowy, KNN, itp.).\nOkreślenie silnika do dopasowania modelu. Najczęściej odzwierciedla to pakiet, który powinien być użyty, jak stan1 lub glmnet. Są to modele same w sobie, a parsnip zapewnia spójne interfejsy, używając ich jako silników do modelowania.\nJeśli jest to wymagane, zadeklaruj tryb pracy modelu2. Tryb odzwierciedla typ przewidywanego wyniku. Dla wyników liczbowych trybem jest regresja; dla wyników jakościowych jest to klasyfikacja. Jeśli algorytm modelu może realizować tylko jeden typ wyniku, takim jak regresja liniowa, tryb jest już ustawiony.\n\n1 jest to bibliotek języka C++2 klasyfikacja czy regresja\nKodlibrary(tidymodels)\n\n# to samo z wykorzystaniem parsnip\nlinear_reg() %&gt;% set_engine(\"lm\")\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nKodlinear_reg() %&gt;% set_engine(\"glmnet\")\n\nLinear Regression Model Specification (regression)\n\nComputational engine: glmnet \n\nKodlinear_reg() %&gt;% set_engine(\"stan\")\n\nLinear Regression Model Specification (regression)\n\nComputational engine: stan \n\n\nPo ustaleniu modeli można je podać uczeniu, za pomocą funkcji fit w przypadku gdy określaliśmy zależność formułą lub fit_xy gdy zmienne niezależne i zależna były określone oddzielnie. Drugi przypadek ma miejsce gdy w procedurze przygotowania danych mamy je w postaci \\((X,y)\\). Nie mniej jednak pakiet parsnip pozwala na użycie fit nawet gdy oryginalna funkcja wymagała podania zmiennych niezależnych i zależnej. Ponadto funkcja translate pozwala na przetłumaczenie modelu parsnip na język danego pakietu.\n\n\n\n\nKodlinear_reg() %&gt;% set_engine(\"lm\") |&gt; translate()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nModel fit template:\nstats::lm(formula = missing_arg(), data = missing_arg(), weights = missing_arg())\n\nKodlinear_reg(penalty = 1) %&gt;% set_engine(\"glmnet\") |&gt; translate()\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1\n\nComputational engine: glmnet \n\nModel fit template:\nglmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    family = \"gaussian\")\n\nKodlinear_reg() %&gt;% set_engine(\"stan\") |&gt; translate()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: stan \n\nModel fit template:\nrstanarm::stan_glm(formula = missing_arg(), data = missing_arg(), \n    weights = missing_arg(), family = stats::gaussian, refresh = 0)\n\n\nWykorzystując dane ames dopasujemy cenę (Sale_Price) na podstawie długości i szerokości geograficznej domu.\n\nKodset.seed(44)\names &lt;- ames %&gt;% mutate(Sale_Price = log10(Sale_Price))\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\nlm_model &lt;- \n  linear_reg() %&gt;% \n  set_engine(\"lm\")\n\nlm_form_fit &lt;- \n  lm_model %&gt;% \n  fit(Sale_Price ~ Longitude + Latitude, data = ames_train)\n\nlm_xy_fit &lt;- \n  lm_model %&gt;% \n  fit_xy(\n    x = ames_train %&gt;% select(Longitude, Latitude),\n    y = ames_train %&gt;% pull(Sale_Price)\n  )\n\nlm_form_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Sale_Price ~ Longitude + Latitude, data = data)\n\nCoefficients:\n(Intercept)    Longitude     Latitude  \n   -321.755       -2.091        3.120  \n\nKodlm_xy_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)    Longitude     Latitude  \n   -321.755       -2.091        3.120  \n\n\nKolejną zaletą pakietu parsnip jest unifikacja nazw parametrów modeli. Dla przykładu gdybyśmy chcieli dopasować trzy różne modele lasów losowych, korzystając z pakietów ranger, randomForest i sparklyr, musielibyśmy określać parametry modelu używając za każdym razem innych nazw.\n\n\nRysunek 6.1: Różne sposoby określania parametrów modelu\n\n\nW przypadku budowy w parsnip nazwy parametrów zostały zunifikowane:\n\n\nmtry - liczba wybranych predyktorów;\n\ntrees - liczba drzew;\n\nmin_n - minimalna liczba obserwacji aby dokonać podziału węzła.\n\nUnifikacja po pierwsze pozwala lepiej zapamiętać nazwy parametrów, a po drugie ich nazwy są zrozumiałe dla czytelnika, który nie koniecznie musi się znać na różnicach pomiędzy pakietami.\nDla wspomnianego przykładu lasów losowych, model można zdefiniować następująco.\n\nKodrand_forest(trees = 1000, min_n = 5) %&gt;% \n  set_engine(\"ranger\") %&gt;% \n  set_mode(\"regression\") %&gt;% \n  translate() # translate nie musi być używane, w tym przypadku było\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n  min_n = 5\n\nComputational engine: ranger \n\nModel fit template:\nranger::ranger(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    num.trees = 1000, min.node.size = min_rows(~5, x), num.threads = 1, \n    verbose = FALSE, seed = sample.int(10^5, 1))\n\nKod# użyte aby pokazać jak parsnip zamienił z unikalnej funkcji \n# rand_forest na model ranger\n\n\nGłówne parametry modelu są przekazywane przez główną funkcję (w przykładzie była to rand_forest), ale pozostałe parametry, charakterystyczne dla danego silnika można przekazać przez argumenty silnika.\n\nKodrand_forest(trees = 1000, min_n = 5) %&gt;% \n  set_engine(\"ranger\", verbose = TRUE) %&gt;% \n  set_mode(\"regression\") # parametr verbose = T przekazany został oddzielnie\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n  min_n = 5\n\nEngine-Specific Arguments:\n  verbose = TRUE\n\nComputational engine: ranger"
  },
  {
    "objectID": "tidymodels.html#użycie-modelu",
    "href": "tidymodels.html#użycie-modelu",
    "title": "\n6  Praca z tidymodels\n",
    "section": "\n6.2 Użycie modelu",
    "text": "6.2 Użycie modelu\nPo utworzeniu i dopasowaniu modelu możemy wykorzystać wyniki na wiele sposobów; możemy chcieć narysować, podsumować lub w inny sposób zbadać model wyjściowy. W obiekcie modelu parsnip przechowywanych jest kilka wielkości, w tym dopasowany model. Można go znaleźć w elemencie o nazwie fit, który może być zwrócony za pomocą funkcji extract_fit_engine.\n\nKodlm_form_fit %&gt;% extract_fit_engine()\n\n\nCall:\nstats::lm(formula = Sale_Price ~ Longitude + Latitude, data = data)\n\nCoefficients:\n(Intercept)    Longitude     Latitude  \n   -321.755       -2.091        3.120  \n\nKodlm_form_fit %&gt;% extract_fit_engine() %&gt;% vcov()\n\n            (Intercept)    Longitude     Latitude\n(Intercept)  214.194437  1.611665120 -1.505256159\nLongitude      1.611665  0.016726120 -0.001079561\nLatitude      -1.505256 -0.001079561  0.033404800\n\n\n\n\n\n\n\n\nZagrożenie\n\n\n\nNigdy nie przekazuj elementu fit modelu parsnip do funkcji predict(lm_form_fit), tzn. nie używaj predict(lm_form_fit$fit). Jeśli dane zostały wstępnie przetworzone w jakikolwiek sposób, zostaną wygenerowane nieprawidłowe predykcje (czasami bez błędów). Funkcja predykcji modelu bazowego nie ma pojęcia czy jakiekolwiek przekształcenia zostały dokonane na danych przed uruchomieniem modelu.\n\n\nKolejną zaletę unifikacji parsnip możemy dostrzec przeglądając podsumowanie modeli. Nie zawsze wyniki modelu są przedstawiane w jednakowy sposób. Czasami różnice są niewielkie, gdy w jednym podsumowaniu zobaczymy p-value a w innym Pr(&gt;|t|) ale czasem mogą być większe. I o ile nie da się zunifikować wszystkich podsumować modeli, ponieważ zawierają różne elementy, to w pakiecie parsnip korzysta się z funkcji tidy pakietu broom do podsumowania modelu.\n\n\n\n\n\n\n\nKodtidy(lm_form_fit)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  -322.      14.6       -22.0 1.57e-97\n2 Longitude      -2.09     0.129     -16.2 7.71e-56\n3 Latitude        3.12     0.183      17.1 1.17e-61\n\n\nOczywiście nie wszystkie modele da się w ten sposób podsumować."
  },
  {
    "objectID": "tidymodels.html#predykcja-z-modelu",
    "href": "tidymodels.html#predykcja-z-modelu",
    "title": "\n6  Praca z tidymodels\n",
    "section": "\n6.3 Predykcja z modelu",
    "text": "6.3 Predykcja z modelu\n\n\n\nPredykcja z modelu jest kolejnym elementem, w którym unifikacja daje o sobie znać:\n\nwyniki zawsze są w formacie tibble;\nnazwy kolumn są zawsze przewidywalne;\nw tibble wynikowej wierszy jest zawsze tyle samo co w zbiorze, na którym predykcja była przeprowadzona;\nkolejność wierszy jest ta sama co w oryginalnym zbiorze.\n\n\nKodames_test_small &lt;- ames_test %&gt;% slice(1:5)\npredict(lm_form_fit, new_data = ames_test_small)\n\n# A tibble: 5 × 1\n  .pred\n  &lt;dbl&gt;\n1  5.23\n2  5.29\n3  5.25\n4  5.25\n5  5.23\n\n\nTo sprawia, że łatwiej można korzystać z wyników predykcji, ponieważ zawsze jesteśmy pewni jaki układ ramki danych predykcji się pojawi.\n\nKodames_test_small %&gt;% \n  select(Sale_Price) %&gt;% \n  bind_cols(predict(lm_form_fit, ames_test_small)) %&gt;% \n  # Add 95% prediction intervals to the results:\n  bind_cols(predict(lm_form_fit, ames_test_small, type = \"pred_int\")) \n\n# A tibble: 5 × 4\n  Sale_Price .pred .pred_lower .pred_upper\n       &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1       5.24  5.23        4.91        5.54\n2       5.33  5.29        4.97        5.60\n3       5.06  5.25        4.93        5.56\n4       5.26  5.25        4.93        5.56\n5       5.08  5.23        4.92        5.55\n\n\n\n\nNazwy zmiennych jakie mogą się pojawić w wyniku predykcji\n\n\n\nKodtree_model &lt;- \n  decision_tree(min_n = 2) %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"regression\")\n\ntree_fit &lt;- \n  tree_model %&gt;% \n  fit(Sale_Price ~ Longitude + Latitude, data = ames_train)\n\names_test_small %&gt;% \n  select(Sale_Price) %&gt;% \n  bind_cols(predict(tree_fit, ames_test_small))\n\n# A tibble: 5 × 2\n  Sale_Price .pred\n       &lt;dbl&gt; &lt;dbl&gt;\n1       5.24  5.16\n2       5.33  5.31\n3       5.06  5.16\n4       5.26  5.16\n5       5.08  5.16\n\n\n\n6.3.1 Rozszerzenia\nSam pakiet parsnip zawiera interfejsy do wielu modeli. Jednakże, dla ułatwienia instalacji i konserwacji pakietu, istnieją inne pakiety tidymodels, które posiadają definicje modeli nie zawartych w parsnip. Np. pakiet discrim posiada definicje modeli klasyfikacyjnych zwanych metodami analizy dyskryminacyjnej (takich jak liniowa lub kwadratowa analiza dyskryminacyjna). Lista wszystkich modeli, które mogą być używane z parsnip znajduje się na stronie https://www.tidymodels.org/find/.\nPrzydatnym narzędziem w budowaniu modeli z wykorzystaniem pakietu tidymodels jest dodatek programu Rstudio3.3 addin instalowany razem z pakietem parsnip\n\nVideo\nPrzykład działania parsnip_addin()"
  },
  {
    "objectID": "tidymodels.html#przepływy-w-modelowaniu",
    "href": "tidymodels.html#przepływy-w-modelowaniu",
    "title": "\n6  Praca z tidymodels\n",
    "section": "\n6.4 Przepływy w modelowaniu",
    "text": "6.4 Przepływy w modelowaniu\nDo tej pory o modelowaniu myśleliśmy w uproszczony sposób, ponieważ zakładaliśmy pewną strukturę modelu, dobieraliśmy silnik i uczyliśmy model na zbiorze treningowym. W “prawdziwych” zadaniach z zakresu uczenia maszynowego, proces ten jest znacznie bardziej złożony. W fazie, którą się powszechnie nazywa przygotowaniem danych (ang. pre-processing), dokonuje się transformacji, agregacji i imputacji danych w celu wykształcenia predyktorów o większej mocy predykcyjnej. W tej fazie dochodzi również do inżynierii cech4 (ang. feature engineering), która ma na celu odfiltrowanie nieużytecznych cech zbioru danych.4 chodzi o wszelkiego rodzaju modyfikacje i selekcje cech\nKolejna faza budowania poprawnego modelu to jego optymalizacja (ang. tuning). Często bowiem budowane modele zawierają hiperparametry, których nie oszacujemy podczas uczenia modelu, dlatego należy je skalibrować na podstawie innych metod5.5 szerzej o tej części będziemy mówić w dalszej części tej książki\nRównież w końcowej fazie uczenia modelu tzw. post-processing-u dokonuje się jego modyfikacji, np. dobierając optymalny poziom odcięcia dla regresji logistycznej.\nTo wszystko powoduje, że procedura modelowania składa się z kilku elementów. Do ich połączenia w ekosystemie tidymodels używa się przepływów (ang. workflow). Pakiet workflow zawiera szereg funkcji pozwalających skutecznie obsługiwać potoki workflow6.6 tak nazywa się funkcja do tworzenia potoku\nPomimo złożoności procedury modelowania można się dalej zastawiać nad koniecznością stosowania przepływów, skoro można te czynności wykonywać oddzielnie. Postaramy się na przykładzie pokazać zasadności stosowania przepływów.\nWeźmy, dajmy na to, że predyktory w zbiorze danych są wysoce skorelowane. Wiem, że zjawisko współliniowości może przeszkodzić w modelowaniu zjawiska, np. za pomocą modelu liniowego, ponieważ znacznie rosną wówczas błędy standardowe estymacji. Jednym ze sposobów radzenia sobie z tym problemem jest zrzutowanie danych na nową przestrzeń mniej wymiarową za pomocą PCA. I gdyby PCA była metodą deterministyczną, czyli nie towarzyszyła jej żadna niepewność7, to tę procedurę preprocessingu użyli byśmy do zbioru uczącego w procesie uczenia modelu, a w predykcji do zbioru testowego, bez konsekwencji w postaci niedokładnego oszacowania wartości wynikowych. Jednak PCA wiąże się z niepewnością, dlatego procedura ta powinna być włączona do przepływu, czyli być immanentną częścią procesu modelowania.7 jak np. logarytmowanie zmiennej\nChoć workflow pozwalają na łączenie preprocessingu, tuningu i postprocessingu, to w następnym przykładzie pokażemy zastosowanie workflow do prostego ucznia modelu bez tych elementów.\n\nKod# określenie modelu\nlm_model &lt;- \n  linear_reg() |&gt; \n  set_engine(\"lm\")\n\n# zebranie elementów w workflow\nlm_wflow &lt;- \n  workflow() |&gt; \n  add_model(lm_model) |&gt; \n  add_formula(Sale_Price ~ Longitude + Latitude)\n\n# workflow\nlm_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nSale_Price ~ Longitude + Latitude\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nKod# uczenie modelu\nlm_fit &lt;- fit(lm_wflow, ames_train)\nlm_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nSale_Price ~ Longitude + Latitude\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)    Longitude     Latitude  \n   -321.755       -2.091        3.120  \n\nKod# predykcja\npredict(lm_fit, ames_test %&gt;% slice(1:3))\n\n# A tibble: 3 × 1\n  .pred\n  &lt;dbl&gt;\n1  5.23\n2  5.29\n3  5.25\n\n\nPomimo tego, że model został zebrany w jedną całość (przepływ), to cały czas możemy modyfikować jego elementy. Przykładowo ujmijmy jeden z predyktorów.\n\nKodlm_fit %&gt;% update_formula(Sale_Price ~ Longitude)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nSale_Price ~ Longitude\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nJeszcze inny przykład modyfikacji przepływu pokazuje przeformatowanie zależności opisywanej modelem.\n\nKodlm_wflow &lt;- \n  lm_wflow %&gt;% \n  remove_formula() %&gt;% \n  add_variables(outcome = Sale_Price, predictors = c(Longitude, Latitude))\n\nlm_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Variables\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nOutcomes: Sale_Price\nPredictors: c(Longitude, Latitude)\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nKodfit(lm_wflow, ames_train)\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Variables\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nOutcomes: Sale_Price\nPredictors: c(Longitude, Latitude)\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)    Longitude     Latitude  \n   -321.755       -2.091        3.120  \n\n\nGenialną właściwością przepływów jest to, że gdy uczymy model wymagający zamiany zmiennych typu faktor na indykatory (ang. dummy variables), to przepływ to zrobi za nas. Przykładowo gdy uczymy model boost_tree z silnikiem xgboost to przepływ zamieni faktory na indykatory, a gdy uczymy z silnikiem C5.0 to już nie, ponieważ ten pakiet tego nie wymaga. Są jednak sytuacje, w których niewielka interwencja w workflow jest potrzebna. Np. jeśli uczymy model z efektami losowymi.\n\nKodlibrary(lme4)\nlibrary(nlme)\nlmer(distance ~ Sex + (age | Subject), data = Orthodont)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: distance ~ Sex + (age | Subject)\n   Data: Orthodont\nREML criterion at convergence: 471.1635\nRandom effects:\n Groups   Name        Std.Dev. Corr \n Subject  (Intercept) 7.3912        \n          age         0.6943   -0.97\n Residual             1.3100        \nNumber of obs: 108, groups:  Subject, 27\nFixed Effects:\n(Intercept)    SexFemale  \n     24.517       -2.145  \n\nKod# tej formuły nie możemy bezpośrednio przekazać do workflow\n# za pomocą add_formula\nlibrary(multilevelmod)\n\nmultilevel_spec &lt;- linear_reg() %&gt;% set_engine(\"lmer\")\n\nmultilevel_workflow &lt;- \n  workflow() %&gt;% \n  # Pass the data along as-is: \n  add_variables(outcome = distance, predictors = c(Sex, age, Subject)) %&gt;% \n  add_model(multilevel_spec, \n            # This formula is given to the model\n            formula = distance ~ Sex + (age | Subject))\n\nmultilevel_fit &lt;- fit(multilevel_workflow, data = Orthodont)\nmultilevel_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Variables\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nOutcomes: distance\nPredictors: c(Sex, age, Subject)\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear mixed model fit by REML ['lmerMod']\nFormula: distance ~ Sex + (age | Subject)\n   Data: data\nREML criterion at convergence: 471.1635\nRandom effects:\n Groups   Name        Std.Dev. Corr \n Subject  (Intercept) 7.3912        \n          age         0.6943   -0.97\n Residual             1.3100        \nNumber of obs: 108, groups:  Subject, 27\nFixed Effects:\n(Intercept)    SexFemale  \n     24.517       -2.145  \n\n\nKolejną zaletą pakietu workflowset jest możliwość jednoczesnego uczenia wielu wariantów modeli.\n\nKod# określamy potencjalne formuły modeli\nlocation &lt;- list(\n  longitude = Sale_Price ~ Longitude,\n  latitude = Sale_Price ~ Latitude,\n  coords = Sale_Price ~ Longitude + Latitude,\n  neighborhood = Sale_Price ~ Neighborhood\n)\n\n\nlibrary(workflowsets)\n\n# zestaw przepływów do uczenia\nlocation_models &lt;- workflow_set(preproc = location, models = list(lm = lm_model))\nlocation_models\n\n# A workflow set/tibble: 4 × 4\n  wflow_id        info             option    result    \n  &lt;chr&gt;           &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 longitude_lm    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 latitude_lm     &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 coords_lm       &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 neighborhood_lm &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\nKod# pierwszy przepływ\nlocation_models$info[[1]]\n\n# A tibble: 1 × 4\n  workflow   preproc model      comment\n  &lt;list&gt;     &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;  \n1 &lt;workflow&gt; formula linear_reg \"\"     \n\nKod# wyciągamy informacje o przepływie trzecim\nextract_workflow(location_models, id = \"coords_lm\")\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nSale_Price ~ Longitude + Latitude\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nKod# uczymy modele\nlocation_models &lt;-\n   location_models %&gt;%\n   mutate(fit = map(info, ~ fit(.x$workflow[[1]], ames_train)))\n\n# wyniki uczenia wszystkich modeli\nlocation_models\n\n# A workflow set/tibble: 4 × 5\n  wflow_id        info             option    result     fit       \n  &lt;chr&gt;           &lt;list&gt;           &lt;list&gt;    &lt;list&gt;     &lt;list&gt;    \n1 longitude_lm    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; &lt;workflow&gt;\n2 latitude_lm     &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; &lt;workflow&gt;\n3 coords_lm       &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; &lt;workflow&gt;\n4 neighborhood_lm &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; &lt;workflow&gt;\n\nKod# wynik uczenia modelu 1\nlocation_models$fit[[1]]\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nSale_Price ~ Longitude\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)    Longitude  \n   -181.180       -1.991  \n\n\nJeszcze jedną wygodną funkcja do oceny ostatecznego modelu jest funkcja last_fit, której używamy do ostatecznego modelu. Wywołanie jej powoduje uczenie modelu na zbiorze uczącym i predykcje na zbiorze testowym.\n\nKod# ostatnie dopasowanie\nfinal_lm_res &lt;- last_fit(lm_wflow, ames_split)\n\n# wynik dopasowania\nfinal_lm_res\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits             id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;             &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [2342/588]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\nKod# podsumowanie modelu\ncollect_metrics(final_lm_res)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.163 Preprocessor1_Model1\n2 rsq     standard       0.131 Preprocessor1_Model1\n\nKod# predykjca na pierwszych pięciu obserwacjach\ncollect_predictions(final_lm_res) %&gt;% slice(1:5)\n\n# A tibble: 5 × 5\n  id               .pred  .row Sale_Price .config             \n  &lt;chr&gt;            &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;               \n1 train/test split  5.23     3       5.24 Preprocessor1_Model1\n2 train/test split  5.29     7       5.33 Preprocessor1_Model1\n3 train/test split  5.25    28       5.06 Preprocessor1_Model1\n4 train/test split  5.25    29       5.26 Preprocessor1_Model1\n5 train/test split  5.23    35       5.08 Preprocessor1_Model1"
  },
  {
    "objectID": "tidymodels.html#inżynieria-cech",
    "href": "tidymodels.html#inżynieria-cech",
    "title": "\n6  Praca z tidymodels\n",
    "section": "\n6.5 Inżynieria cech",
    "text": "6.5 Inżynieria cech\nWspomniana już inżynieria cech jest bardzo ważnym elementem budowy modelu. Inżynieria cech polega na przeformatowaniu wartości predyktorów, aby ułatwić ich efektywne wykorzystanie przez model. Obejmuje to transformacje i kodowanie danych, aby najlepiej reprezentować ich ważne cechy. Wyobraź sobie, że masz dwa predyktory w zestawie danych, które mogą być bardziej efektywnie reprezentowane w modelu jako stosunek; stworzenie nowego predyktora ze stosunku oryginalnych dwóch jest prostym przykładem inżynierii cech.\nWeźmy lokalizację domu w Ames jako bardziej ambitny przykład. Istnieje wiele sposobów, w jakie te informacje przestrzenne mogą być eksponowane w modelu, w tym sąsiedztwo (miara jakościowa), długość/szerokość geograficzna, odległość do najbliższej szkoły lub Uniwersytetu Stanowego Iowa, i tak dalej. Wybierając sposób kodowania tych danych w modelowaniu, możemy wybrać opcję, która naszym zdaniem jest najbardziej związana z wynikiem. Oryginalny format danych, na przykład numeryczny (jak odległość) lub kategoryczny (np. sąsiedztwo), jest również czynnikiem decydującym o przeprowadzeniu inżynierii cech.\nInne przykłady przetwarzania wstępnego w celu zbudowania lepszych cech dla modelowania to:\n\nKorelacja między predyktorami może być zmniejszona poprzez ekstrakcję cech lub usunięcie niektórych predyktorów.\nGdy niektóre predyktory mają brakujące wartości, mogą być imputowane przy użyciu modelu pomocniczego.\nWymuszenie symetryczności predyktorów poprzez zastosowanie transformacji.\n\nInżynieria cech i wstępne przetwarzanie danych może również obejmować przeformatowanie, które może być wymagane przez model. Niektóre modele używają metryki odległości geometrycznej i w konsekwencji predyktory numeryczne powinny być wyśrodkowane i przeskalowane tak, aby wszystkie były w tych samych jednostkach8. W przeciwnym razie wartości odległości byłyby zniekształcone przez skalę każdej kolumny.8 przykładem może być model kNN\nW ekosystemie tidymodels do realizowania inżynierii cech został dedykowany pakiet recipes.\n\nPrzykład 6.1 Dla przykładu przeprowadzimy transformację kilku cech zbioru ames:\n\nsąsiedztwa (zmienna jakościowa o 29 stanach w zbiorze uczącym);\npowierzchnia mieszkalna nad poziomem terenu (ciągła);\nrok budowy;\ntyp budynku.\n\nZe względu na asymetrię rozkładu zmiennej Gr_Liv_Area załóżmy, że początkowy model regresji będzie opisany równaniem:\n\nKodlm(Sale_Price ~ Neighborhood + log10(Gr_Liv_Area) + Year_Built + Bldg_Type, data = ames_train)\n\n\nWywołanie tej funkcji dokonało by następujących czynności na predyktorach i zmiennej zależnej:\n\nzmienna Sale_Price została by przypisana jako zależna;\nzmienna Gr_Liv_Area została by przekształcona logarytmicznie;\nzmienne Neighborhood i Bldg_Type zostały by zamienione na indykatory stanów.\n\nTe czynności byłyby wykonane również podczas predykcji na podstawie tego modelu. Chcąc przeprowadzić te czynności w pakiecie recipes wykorzystujemy kroki step_:\n\nKodsimple_ames &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,\n         data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_dummy(all_nominal_predictors())\n\nsimple_ames\n\n\nJaka jest przewaga stosowania przepisów, nad prostą formułą lub surowymi predykatorami? Jest ich kilka, w tym:\n\nObliczenia te mogą być przetwarzane w różnych modelach, ponieważ nie są ściśle sprzężone z funkcją modelowania.\nReceptura umożliwia szerszy zestaw wyborów przetwarzania danych niż formuły mogą zaoferować.\nSkładnia może być bardzo zwarta. Na przykład, all_nominal_predictors może być użyta do uchwycenia wielu zmiennych dla określonych typów przetwarzania, podczas gdy formuła wymagałaby wyraźnego wymienienia każdej z nich.\nCałe przetwarzanie danych może być uchwycone w pojedynczym obiekcie R zamiast w skryptach, które są powtarzane, a nawet rozłożone w różnych plikach.\n\n\nOczywiście przepisy można (a nawet trzeba) łączyć z przepływami.\n\nKodlm_wflow &lt;- \n  lm_wflow %&gt;% \n  remove_variables() %&gt;% # musieliśmy usunąć wcześniejszy preprocessing\n  add_recipe(simple_ames)\n\nlm_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_log()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nKodlm_fit &lt;- fit(lm_wflow, ames_train)\n\n\nPrzez to, że przepis na preprocessing został zapisany w przepływie, to zostanie on zastosowany (co jest konieczne) również do zbioru testowego używając funkcji predict.\n\nKodpredict(lm_fit, ames_test %&gt;% slice(1:3))\n\n# A tibble: 3 × 1\n  .pred_res\n      &lt;dbl&gt;\n1      5.17\n2      5.37\n3      5.08\n\n\nMożemy się przekonać, że faktycznie przepisy zostały wykonane podczas uczenia modelu.\n\nKodlm_fit %&gt;% \n  extract_recipe(estimated = TRUE)\n\n\nOtrzymany model jest postaci:\n\nKodlm_fit %&gt;% \n  extract_fit_parsnip() %&gt;% \n  tidy() %&gt;% \n  slice(1:5) # oczywiście parametrów modelu jest dużo więcej\n\n# A tibble: 5 × 5\n  term                       estimate std.error statistic   p.value\n  &lt;chr&gt;                         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)                -0.876    0.233        -3.76 1.77e-  4\n2 Gr_Liv_Area                 0.631    0.0141       44.8  2.50e-316\n3 Year_Built                  0.00208  0.000118     17.6  2.38e- 65\n4 Neighborhood_College_Creek  0.0135   0.00822       1.64 1.01e-  1\n5 Neighborhood_Old_Town      -0.0251   0.00837      -3.00 2.71e-  3\n\n\nDane są przekazywane do receptur na różnych etapach.\nPo pierwsze, podczas wywołania recipe(..., dane), zestaw danych jest wykorzystywany do określenia typów danych dla każdej kolumny, tak aby można było użyć selektorów takich jak all_numeric() lub all_numeric_predictors().\nPo drugie, podczas przygotowywania danych za pomocą fit(workflow, data), dane treningowe są używane do wszystkich operacji estymacji, w tym czynności z przepisu, który może być częścią przepływu, od określenia poziomów czynników do obliczania komponentów PCA i wszystkiego pomiędzy.\nWreszcie, gdy używamy predict(workflow, new_data), żadne parametry modelu czy preprocessingu, jak te z przepisu, nie są ponownie szacowane przy użyciu wartości z new_data. Weźmy jako przykład centrowanie i skalowanie przy użyciu step_normalize(). Używając tego kroku, średnie i odchylenia standardowe z odpowiednich kolumn są określane z zestawu treningowego; nowe próbki w czasie predykcji są normalizowane przy użyciu tych wartości z treningu, gdy wywoływana jest funkcja predict().\n\n\n\n\n\n\nOstrzeżenie\n\n\n\nWszystkie kroki przetwarzania wstępnego i inżynierii cech wykorzystują tylko dane treningowe. W przeciwnym razie wyciek informacji może negatywnie wpłynąć na wydajność modelu, gdy jest on używany z nowymi danymi.\n\n\n\n\n\n\n\n\n\n6.5.1 Kodowanie faktorów\nCzynności, które można wykonać przy użyciu kroków/przepisów jest bardzo wiele, od bardzo prostych, jak centrowanie i skalowanie (step_nomalize()), po wyrafinowane, jak wybór spośród dni tygodnia tylko tych, które nie wypadały w święta (step_holiday()). W trakcie budowania modelu różne czynności będą wymagane, a wśród nich bardzo często zdarza się, że trzeba obsłużyć zmienne niezależne typu faktor. Możemy to zrobić na kilka sposobów. Oczywiście najczęściej stosowaną jest zamiana poziomów czynnika na indykatory. Co jednak w przypadku gdy pewien poziom czynnika nie wystąpił w zbiorze uczącym. Zamiana go na indykator powoduje powstanie stałej zmiennej, która jest bezużyteczna w uczeniu modelu. Można oczywiście przystąpić do imputacji takiej wartości, a sposobów na jej realizację jest bardzo wiele. Pomijając techniki korzystające z modeli pomocniczych, jak step_impute_knn czy step_impute_bag, można też zastąpić brak stałą wartością (step_unknown). Podobnie, jeśli przewidujemy, że w testowych danych może pojawić się nowy poziom czynnika, step_novel() może w tym celu przydzielić mu nowy poziom czynnika.\nDodatkowo, funkcja step_other() może być użyta do przeanalizowania częstotliwości poziomów czynników w zbiorze treningowym i przekonwertowania rzadko występujących wartości do poziomu “inne”, z progiem, który można określić. Dobrym przykładem jest predyktor Neighborhood w naszych danych, pokazany na Rysunek 6.2.\n\nKodames_train |&gt; \n  ggplot(aes(Neighborhood))+\n  geom_bar()+\n  coord_flip()\n\n\n\nRysunek 6.2: Wykres częstości występowania poszczególnych dzielnic\n\n\n\n\nWidzimy, że dwie dzielnice mają mniej niż pięć nieruchomości w danych treningowych (Landmark i Green Hills); w tym przypadku żadne domy w dzielnicy Landmark nie zostały włączone do zbioru testowego. Dla niektórych modeli może być problematyczne posiadanie zmiennych dummy z jednym niezerowym wpisem w kolumnie. Przynajmniej jest wysoce nieprawdopodobne, że te cechy byłyby ważne dla modelu. Jeśli dodamy step_other(Neighborhood, threshold = 0.01) do naszego przepisu, 1% dzielnic9 zostanie wrzucony do nowego poziomu o nazwie “inne”. W tym zbiorze treningowym próg ten obejmie siedem sąsiedztw.9 najrzadziej występujący\n\nKodsimple_ames &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,\n         data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors())\n\n\nW kodowaniu zmiennych typu faktor na indykatory stanów zazwyczaj korzysta się z pełno-rzędowego przekształcenia, czyli dla faktora z 5 kategoriami, jeden traktuje się jako referencyjny, a pozostałe się koduje indykatorami. Powstała w ten sposób macierz modelu jest pełnego rzędu. Takie zachowanie jest domyślne używając funkcji step_dummy. Jednak niektóre modele wymagają kodowania one-hot edcoding, które tworzy indykatory wszystkich kategorii. Chcąc wywołać kodowanie one-hot używamy funkcji step_dummy(…, one_hot = TRUE).\n\n6.5.2 Interakcje efektów\nJeszcze innym ciekawym zagadnieniem podczas budowy jest odpowiedź na pytanie, czy model powinien zawierać interakcje efektów. Pozwalają to na stwierdzenie czy wpływ jednej zmiennej na wynik jest modyfikowany przez inną zmienną. W naszym przykładzie domów ames też możemy podejrzewać istnienie interakcji, ponieważ na Rysunek 6.3 można zauważyć inny charakter zależności dla różnych typów budynków.\n\nKodggplot(ames_train, aes(x = Gr_Liv_Area, y = 10^Sale_Price)) + \n  geom_point(alpha = .2) + \n  facet_wrap(~ Bldg_Type) + \n  geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = \"lightblue\") + \n  scale_x_log10() + \n  scale_y_log10() + \n  labs(x = \"Gross Living Area\", y = \"Sale Price (USD)\")\n\n\n\nRysunek 6.3: Potencjalne interakcje zmiennych Gr_Liv_Area i Bldg_Type\n\n\n\n\nW pakiecie recipes istnieje oddzielny krok to tworzenia interakcji.\n\nKodsimple_ames &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,\n         data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  # Gr_Liv_Area is on the log scale from a previous step\n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") )\n\n\nMożna ją było również dodać w klasyczny sposób +var1:var2 wpisując do formuły. Jednak w naszym przypadku ten sposób nie byłby poprawny, ponieważ to kazałoby funkcji step_interact(), aby stworzyła indykatory, a następnie utworzyć interakcje. W rzeczywistości, program nie miałby jak stworzyć interakcji, ponieważ powstałyby już nowe zmienne z prefiksem Bldg_Type_.\n\n\n\n\n\n\nWażne\n\n\n\nPowyższy przykład pokazuje, że kolejność włączania kroków jest bardzo ważna.\n\n\n\n\n\n\n\n\n\n6.5.3 Nieliniowość zależności\nBardzo często analizowane zależności są bardzo złożone i wykazują nieliniowy charakter. Jednym ze sposobów modelowania takich relacji jest aproksymacja ich za pomocą rozbudowanych modeli liniowych składających się z olbrzymiej liczby kombinacji liniowych predyktorów10. Innym sposobem obsługi tego zjawiska jest odpowiednie przekształcenie predyktorów, tak aby zamodelować złożony charakter zależności. Często w tym miejscu są polecane funkcje wielomianowe. Mają one jednak jedną poważną wadę, ponieważ o ile mogą dobrze opisywać zależność w analizowanej dziedzinie, to ekstrapolacja tej zależności często nie ma sensu. Z pomocą mogą przyjść splajny (ang. spline), czyli funkcje bazowe modeli GAM (ang. Generalized Additive Models). Nie wchodząc w szczegóły modeli GAM, splajny pozwalają na lokalną estymację zależności11 .10 przykładem może być sieć neuronowa11 lokalną - czyli pomiędzy punktami węzłowymi, im więcej punktów węzłowych wym większa elastyczność splajnów\n\nKodlibrary(patchwork)\nlibrary(splines)\n\nplot_smoother &lt;- function(deg_free) {\n  ggplot(ames_train, aes(x = Latitude, y = 10^Sale_Price)) + \n    geom_point(alpha = .2) + \n    scale_y_log10() +\n    geom_smooth(\n      method = lm,\n      formula = y ~ ns(x, df = deg_free),\n      color = \"lightblue\",\n      se = FALSE\n    ) +\n    labs(title = paste(deg_free, \"Spline Terms\"),\n         y = \"Sale Price (USD)\")\n}\n\n( plot_smoother(2) + plot_smoother(5) ) / ( plot_smoother(20) + plot_smoother(100) )\n\n\n\nRysunek 6.4: Opis zależności za pomocą splajnów z różna liczbą punktów węzłowych\n\n\n\n\nRozwiązanie z 2 i 100 punktami węzłowymi wykazuje niedopasowanie i nadmierne dopasowanie odpowiednio. Wybór 5 lub 20 punktów wydaje się dużo lepszy. Ostatecznie ten hiperparametr modelu można kalibrować, ale to nie będzie naszym celem w tym przykładzie.\n\nKodrecipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Latitude,\n         data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;% \n  step_ns(Latitude, deg_free = 20)\n\n\n\n6.5.4 Ekstrakcja cech\nInna powszechna metoda reprezentowania wielu cech jednocześnie nazywana jest ekstrakcją cech. Większość z tych technik tworzy nowe cechy z predyktorów, które wychwytują informacje w szerszym zestawie jako całości. Na przykład, analiza składowych głównych (PCA) próbuje wyodrębnić jak najwięcej oryginalnej informacji w zestawie predyktorów przy użyciu mniejszej liczby cech. PCA jest liniową metodą ekstrakcji, co oznacza, że każda nowa cecha jest liniową kombinacją oryginalnych predyktorów. Jednym z ciekawych aspektów PCA jest to, że każda z nowych cech, zwanych głównymi składowymi, jest nieskorelowana z innymi. Z tego powodu PCA może być bardzo skuteczne w redukcji korelacji pomiędzy predyktorami.\nW danych ames, kilka predyktorów mierzy powierzchnię nieruchomości, takich jak całkowita powierzchnia piwnicy (Total_Bsmt_SF), powierzchnia pierwszego piętra (First_Flr_SF), powierzchnia mieszkalna (Gr_Liv_Area), i tak dalej. PCA może być sposobem reprezentowania tych potencjalnie zbędnych zmiennych w mniej wymiarowej przestrzeni cech. Oprócz powierzchni mieszkalnej brutto, predyktory mają przyrostek SF w swoich nazwach (oznaczające stopy kwadratowe), więc krok przepisu PCA mógłby wyglądać tak:\n\nKodstep_pca(matches(\"(SF$)|(Gr_Liv)\"))\n\n\nZauważmy, że wszystkie wspomniane kolumny są mierzone w stopach kwadratowych, a PCA zakłada, że wszystkie predyktory są w tej samej skali. W tym przypadku to prawda, ale często ten krok musi być poprzedzony przez step_normalize().\nIstnieją kroki przepisów dla innych metod ekstrakcji, takie jak: analiza składowych niezależnych (ICA), faktoryzacja macierzy nieujemnej (NNMF), skalowanie wielowymiarowe (MDS), jednolita aproksymacja i projekcja (UMAP) i inne.\n\n6.5.5 Przepisy wierszowe\nDo tej pory wspominaliśmy o krokach, które dotyczyły zmiennych, ale istnieją również takie, które dotyczą wierszy. Na przykład, techniki podpróbkowania (ang. subsampling) dla nierównowagi klas zmieniają proporcje klas w danych przekazywanych do modelu; techniki te często nie poprawiają ogólnej wydajności, ale mogą generować lepiej zachowujące się rozkłady przewidywanych prawdopodobieństw klas. O nich jednak będzie więcej w dalszej części tej książki.\nIstnieją inne funkcje krokowe, które są również oparte na wierszach, jak: step_filter(), step_sample(), step_slice() i step_arrange().\n\n\n\n\n\n\nZagrożenie\n\n\n\nTylko zestaw treningowy powinien być poddany wpływowi tych technik. Zbiór testowy lub inne próbki powinny być pozostawione w niezmienionym stanie, czyli ten krok przepisu nie powinien być do nich stosowany. Z tego powodu, wszystkie kroki podpróbkowania i filtrowania domyślnie ustawiają argument skip na wartość TRUE.\n\n\n\n6.5.6 Przekształcenia ogólne\nPrzekształcenia, które mają swoje korzenie w pakiecie dplyr jak step_mutate też mogą być stosowane w przepisach. Najlepiej używać ich do prostych przekształceń, takich jak obliczanie stosunku dwóch zmiennych, (np. Bedroom_AbvGr / Full_Bath), stosunek sypialni do łazienek dla danych mieszkaniowych ames.\n\n\n\n\n\n\nZagrożenie\n\n\n\nPodczas korzystania z tego kroku należy zachować szczególną ostrożność, aby uniknąć wycieku danych w swoim wstępnym przetwarzaniu. Rozważmy na przykład transformację x = w &gt; mean(w). W przypadku zastosowania go do nowych danych lub danych testowych, ta transformacja użyłaby średniej w z nowych danych, a nie średniej wz danych treningowych.\n\n\nPrzepis może również obsługiwać dane, które nie mają tradycyjnej struktury. Na przykład pakiet textrecipes może zastosować do danych metody przetwarzania języka naturalnego. Kolumna wejściowa jest zwykle ciągiem tekstu, a różne kroki mogą być użyte do tokenizacji danych (np. podzielenia tekstu na osobne słowa), odfiltrowania tokenów i stworzenia nowych cech odpowiednich do modelowania.\n\n6.5.7 Pomijanie kroków\nNa samym początku analiz zbioru ames przekształciliśmy zmienną Sale_Price logarytmicznie. Dlaczego nie użyliśmy zamiast tego kroku\n\nKodstep_log(Sale_Price, base = 10)\n\n\nPonieważ użycie go powodowałoby problemy przy użyciu całego przepisu na nowych danych do predykcji. Często jest bowiem tak, że zmiennej zależnej w tym zbiorze nie ma.\n\n\n\n\n\n\nZagrożenie\n\n\n\nW przypadku prostych przekształceń kolumny (kolumn) wynikowej, zdecydowanie sugerujemy, aby operacje te były wykonywane poza przepisem.\n\n\nRównież wspomniane wcześniej kroki redukujące nierównowagę klasową powinny być stosowane tylko do danych uczących. Wszystkie te kroki, które chcemy wyłączyć z przetwarzania podczas predykcji na nowych zbiorach, powinny mieć włączoną flagę skip = TRUE.\nNa koniec części dotyczącej inżynierii cech nauczymy model.\n\nKodames_rec &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.01, id = \"my_id\") %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;% \n  step_ns(Latitude, Longitude, deg_free = 20)\n\nlm_wflow &lt;- \n  workflow() %&gt;% \n  add_model(lm_model) %&gt;% \n  add_recipe(ames_rec)\n\nlm_fit &lt;- fit(lm_wflow, ames_train)\n\nestimated_recipe &lt;- \n  lm_fit %&gt;% \n  extract_recipe(estimated = TRUE)\n\ntidy(estimated_recipe)\n\n# A tibble: 5 × 6\n  number operation type     trained skip  id            \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;    &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;         \n1      1 step      log      TRUE    FALSE log_p06et     \n2      2 step      other    TRUE    FALSE my_id         \n3      3 step      dummy    TRUE    FALSE dummy_rztdZ   \n4      4 step      interact TRUE    FALSE interact_1T8XZ\n5      5 step      ns       TRUE    FALSE ns_eTRX6      \n\nKod# możemy też wywołać szczegóły konkretnego kroku\ntidy(estimated_recipe, id = \"my_id\")\n\n# A tibble: 21 × 3\n   terms        retained           id   \n   &lt;chr&gt;        &lt;chr&gt;              &lt;chr&gt;\n 1 Neighborhood North_Ames         my_id\n 2 Neighborhood College_Creek      my_id\n 3 Neighborhood Old_Town           my_id\n 4 Neighborhood Edwards            my_id\n 5 Neighborhood Somerset           my_id\n 6 Neighborhood Northridge_Heights my_id\n 7 Neighborhood Gilbert            my_id\n 8 Neighborhood Sawyer             my_id\n 9 Neighborhood Northwest_Ames     my_id\n10 Neighborhood Sawyer_West        my_id\n# ℹ 11 more rows\n\n\n\n6.5.8 Role zmiennych\nW modelach przedstawianych do tej pory dominowały dwie role zmiennych: predyktory (predictor) i zmienna zależna (outcome). W razie potrzeby można jednak przypisać inne role.\nNa przykład, w naszym zestawie danych ames, oryginalne dane zawierały kolumnę z adresem. Może być przydatne zachowanie tej kolumny w danych, aby po dokonaniu przewidywań można było szczegółowo zbadać problematyczne wyniki. Innymi słowy, kolumna może być ważna, nawet jeśli nie jest predyktorem lub wynikiem. Aby to rozwiązać, pomocne mogą być funkcje add_role(), remove_role() i update_role(). Na przykład, dla danych dotyczących cen domów, rola kolumny adresu ulicy może być zmodyfikowana przy użyciu:\n\nKodames_rec %&gt;% update_role(address, new_role = \"street address\")\n\n\nPo tej zmianie kolumna adresu w ramce danych nie będzie już predyktorem, ale “adresem ulicy” zgodnie z przepisem. Każdy ciąg znaków może być użyty jako rola. Również kolumny mogą mieć wiele ról (dodatkowe role są dodawane poprzez add_role()) tak, że mogą być nadane w więcej niż jednym kontekście.\nMoże to być pomocne, gdy dane są próbkowane. Pomaga to utrzymać kolumny, które nie są zaangażowane w dopasowanie modelu w tej samej ramce danych (a nie w zewnętrznym wektorze). Próbkowanie, opisane nieco później, tworzy alternatywne wersje danych głównie poprzez podpróbkowanie wierszy. Gdyby adres ulicy znajdował się w innej kolumnie, wymagane byłoby dodatkowe podpróbkowanie, co mogłoby prowadzić do bardziej skomplikowanego kodu i większego prawdopodobieństwa wystąpienia błędów.\n\n\n\nWreszcie, wszystkie funkcje kroków mają pole role, które może przypisać role do wyników kroku. W wielu przypadkach kolumny, na które wpływa krok, zachowują swoją dotychczasową rolę. Na przykład, wywołania funkcji step_log() w naszym obiekcie ames_rec wpłynęły na kolumnę Gr_Liv_Area. Dla tego kroku, domyślnym zachowaniem jest zachowanie istniejącej roli dla tej kolumny, ponieważ nie jest tworzona żadna nowa kolumna. Jako przykład przeciwny, krok do tworzenia splajnów ustawia domyślnie nowe kolumny na rolę “predyktor”, ponieważ jest to zwykle sposób, w jaki kolumny splajnów są używane w modelu."
  },
  {
    "objectID": "resampling2.html#walidacja-krzyżowa",
    "href": "resampling2.html#walidacja-krzyżowa",
    "title": "\n7  Metody próbkowania\n",
    "section": "\n7.1 Walidacja krzyżowa",
    "text": "7.1 Walidacja krzyżowa\nWalidacja krzyżowa (ang. cross-validation) jest dobrze ugruntowaną metodą próbkowania. Chociaż istnieje wiele odmian, najbardziej powszechną metodą walidacji krzyżowej jest \\(V\\)-krotna walidacja krzyżowa. Dane są losowo dzielone na \\(V\\) zbiorów o mniej więcej równej wielkości (zwanych krotkami lub foldami). Dla ilustracji, \\(V = 3\\) jest pokazane na rysunku -Rysunek 7.2 dla zbioru danych składającego się z 30 punktów zbioru treningowego z losowym przydziałem foldów. Liczba wewnątrz symboli to numer próbki.\n\n\nRysunek 7.2: 3-krotny sprawdzian krzyżowy\n\n\nKolory symboli na Rysunek 7.2 reprezentują ich losowo przypisane foldy.\nW przypadku trzykrotnej walidacji krzyżowej trzy iteracje próbkowania przedstawiono na Rysunek 7.3. Dla każdej iteracji jeden fold jest zatrzymywany do oceny modelu, a pozostałe foldy są używane do uczenia modelu. Proces ten jest kontynuowany dla każdego folda, tak że trzy modele dają trzy zestawy statystyk dopasowania.\n\n\nRysunek 7.3: Zastosowanie foldów w uczeniu i ocenie dopasowania modeli\n\n\nGdy \\(V = 3\\), zbiory analiz stanowią 2/3 zbioru treningowego, a każdy zbiór oceny stanowi odrębną 1/3. Końcowa estymacja resamplingu wydajności uśrednia każdą z \\(V\\) replik.\n\n\n\n\n\n\nUżycie \\(V = 3\\) jest dobrym wyborem do zilustrowania walidacji krzyżowej, ale jest to zły wybór w praktyce, ponieważ jest zbyt mało foldów, aby wygenerować wiarygodne szacunki. W praktyce wartości \\(V\\) to najczęściej 5 lub 10; raczej preferujemy 10-krotną walidację krzyżową jako domyślną, ponieważ jest ona wystarczająco duża, aby uzyskać dobre wyniki w większości sytuacji.\nJakie są skutki zmiany \\(V\\)? Większe wartości powodują, że szacunki z próbkowania mają mały błąd/obciążenie, ale znaczną wariancję. Mniejsze wartości \\(V\\) mają duży błąd, ale niską wariancję. Preferujemy 10-krotne, ponieważ szum jest zmniejszony przez replikacje, ale obciążenie już nie.\n\nKodset.seed(1001)\names_folds &lt;- vfold_cv(ames_train, v = 10)\names_folds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits             id    \n   &lt;list&gt;             &lt;chr&gt; \n 1 &lt;split [2107/235]&gt; Fold01\n 2 &lt;split [2107/235]&gt; Fold02\n 3 &lt;split [2108/234]&gt; Fold03\n 4 &lt;split [2108/234]&gt; Fold04\n 5 &lt;split [2108/234]&gt; Fold05\n 6 &lt;split [2108/234]&gt; Fold06\n 7 &lt;split [2108/234]&gt; Fold07\n 8 &lt;split [2108/234]&gt; Fold08\n 9 &lt;split [2108/234]&gt; Fold09\n10 &lt;split [2108/234]&gt; Fold10\n\nKodlobstr::obj_size(ames_folds)\n\n944.36 kB\n\nKodlobstr::obj_size(ames_train)\n\n849.83 kB\n\n\nKolumna o nazwie splits zawiera informacje o tym, jak podzielone zostaną dane (podobnie jak obiekt używany do tworzenia początkowej partycji trening/test). Chociaż każdy podział ma zagnieżdżoną kopię całego zbioru treningowego, R jest na tyle inteligentny, że nie tworzy kopii danych w pamięci. Metoda print wewnątrz tibble pokazuje częstość występowania każdego z nich: [2107/235] wskazuje, że około dwóch tysięcy próbek znajduje się w zbiorze analitycznym, a 235 w tym konkretnym zbiorze oceniającym."
  },
  {
    "objectID": "resampling2.html#sprawdzian-krzyżowy-z-powtórzeniami",
    "href": "resampling2.html#sprawdzian-krzyżowy-z-powtórzeniami",
    "title": "\n7  Metody próbkowania\n",
    "section": "\n7.2 Sprawdzian krzyżowy z powtórzeniami",
    "text": "7.2 Sprawdzian krzyżowy z powtórzeniami\nNajważniejszą odmianą walidacji krzyżowej jest \\(V\\)-krotna walidacja krzyżowa z powtórzeniami. W zależności od rozmiaru danych i innych cech, ocena modelu uzyskana w wyniku \\(V\\)-krotnej walidacji krzyżowej może być nadmiernie zaszumiona. Podobnie jak w przypadku wielu problemów statystycznych, jednym ze sposobów zmniejszenia szumu jest zebranie większej ilości danych. W przypadku walidacji krzyżowej oznacza to uśrednienie więcej niż \\(V\\) statystyk.\n\n\n\n\n\n\nAby stworzyć \\(R\\) powtórzeń \\(V\\)-krotnej walidacji krzyżowej, ten sam proces generowania foldów jest wykonywany \\(R\\) razy, aby wygenerować \\(R\\) zbiorów złożonych z \\(V\\) podzbiorów. Zamiast uśredniania \\(V\\) statystyk, \\(V\\times R\\) wartości daje ostateczną estymację resamplingu.\nRozważmy dane ames. Średnio, 10-krotna walidacja krzyżowa używa zestawów oceny, które zawierają około 234 obserwacji. Jeśli RMSE jest wybraną statystyką, możemy oznaczyć odchylenie standardowe tego oszacowania jako \\(\\sigma\\). Przy 10-krotnej walidacji krzyżowej błąd standardowy średniej RMSE wynosi \\(\\sigma/\\sqrt{10}\\). Podczas gdy ta wielkość może charakteryzować się jeszcze dużym szumem, powtórzenia zmniejszają błąd standardowy do \\(\\sigma/\\sqrt{10R}\\). Dla 10-krotnej walidacji krzyżowej z \\(R\\) powtórzeniami, wykres na rysunku Rysunek 7.4 pokazuje, jak szybko błąd standardowy maleje wraz z liczbą powtórzeń.\n\n\nRysunek 7.4: Wielkość błędu standardowego estymacji w zależności od liczby powtórzeń walidacji krzyżowych\n\n\nGeneralnie zwiększanie liczby replikacji nie ma dużego wpływu na błąd standardowy estymacji, chyba że bazowa wartość \\(\\sigma\\) jest duża, wówczas faktycznie warto zwiększać liczbę replikacji.\n\nKodvfold_cv(ames_train, v = 10, repeats = 5)\n\n#  10-fold cross-validation repeated 5 times \n# A tibble: 50 × 3\n   splits             id      id2   \n   &lt;list&gt;             &lt;chr&gt;   &lt;chr&gt; \n 1 &lt;split [2107/235]&gt; Repeat1 Fold01\n 2 &lt;split [2107/235]&gt; Repeat1 Fold02\n 3 &lt;split [2108/234]&gt; Repeat1 Fold03\n 4 &lt;split [2108/234]&gt; Repeat1 Fold04\n 5 &lt;split [2108/234]&gt; Repeat1 Fold05\n 6 &lt;split [2108/234]&gt; Repeat1 Fold06\n 7 &lt;split [2108/234]&gt; Repeat1 Fold07\n 8 &lt;split [2108/234]&gt; Repeat1 Fold08\n 9 &lt;split [2108/234]&gt; Repeat1 Fold09\n10 &lt;split [2108/234]&gt; Repeat1 Fold10\n# ℹ 40 more rows"
  },
  {
    "objectID": "resampling2.html#leave-one-out",
    "href": "resampling2.html#leave-one-out",
    "title": "\n7  Metody próbkowania\n",
    "section": "\n7.3 Leave-One-Out",
    "text": "7.3 Leave-One-Out\nJedną z odmian walidacji krzyżowej jest walidacja krzyżowa typu Leave-One-Out (LOO). Jeśli mamy \\(n\\) próbek zbioru treningowego, \\(n\\) modeli jest dopasowywanych przy użyciu \\(n - 1\\) wierszy zbioru treningowego. Każdy model przewiduje pojedynczy wykluczony punkt danych. Na koniec próbkowania \\(n\\) prognoz jest łączonych w celu uzyskania pojedynczej statystyki dopasowania\n\n\n\n\n\n\nMetody LOO są gorsza w porównaniu z prawie każdą inną metodą oceny dopasowania. Dla wszystkich oprócz patologicznie małych próbek, LOO jest obliczeniowo złożony i może nie mieć dobrych właściwości statystycznych. Chociaż pakiet rsample zawiera funkcję loo_cv(), obiekty te nie są mocno zintegrowane z pakietami tidymodels."
  },
  {
    "objectID": "resampling2.html#walidacja-metodą-monte-carlo",
    "href": "resampling2.html#walidacja-metodą-monte-carlo",
    "title": "\n7  Metody próbkowania\n",
    "section": "\n7.4 Walidacja metodą Monte-Carlo",
    "text": "7.4 Walidacja metodą Monte-Carlo\nInnym wariantem \\(V\\)-krotnej walidacji krzyżowej jest walidacja krzyżowa Monte-Carlo (ang. Monte-Carlo Cross-Validation - MCCV) (Xu i Liang 2001). Podobnie jak w sprawdzianie krzyżowym, przydziela ona ustaloną część danych do zbiorów oceny. Różnica między MCCV a zwykłą walidacją krzyżową polega na tym, że w przypadku MCCV ta część danych jest za każdym razem wybierana losowo. Przez to powstają zestawy oceny, które nie wykluczają się wzajemnie.\n\nKodmc_cv(ames_train, prop = 9/10, times = 20)\n\n# Monte Carlo cross-validation (0.9/0.1) with 20 resamples  \n# A tibble: 20 × 2\n   splits             id        \n   &lt;list&gt;             &lt;chr&gt;     \n 1 &lt;split [2107/235]&gt; Resample01\n 2 &lt;split [2107/235]&gt; Resample02\n 3 &lt;split [2107/235]&gt; Resample03\n 4 &lt;split [2107/235]&gt; Resample04\n 5 &lt;split [2107/235]&gt; Resample05\n 6 &lt;split [2107/235]&gt; Resample06\n 7 &lt;split [2107/235]&gt; Resample07\n 8 &lt;split [2107/235]&gt; Resample08\n 9 &lt;split [2107/235]&gt; Resample09\n10 &lt;split [2107/235]&gt; Resample10\n11 &lt;split [2107/235]&gt; Resample11\n12 &lt;split [2107/235]&gt; Resample12\n13 &lt;split [2107/235]&gt; Resample13\n14 &lt;split [2107/235]&gt; Resample14\n15 &lt;split [2107/235]&gt; Resample15\n16 &lt;split [2107/235]&gt; Resample16\n17 &lt;split [2107/235]&gt; Resample17\n18 &lt;split [2107/235]&gt; Resample18\n19 &lt;split [2107/235]&gt; Resample19\n20 &lt;split [2107/235]&gt; Resample20"
  },
  {
    "objectID": "resampling2.html#zbiór-walidacyjny",
    "href": "resampling2.html#zbiór-walidacyjny",
    "title": "\n7  Metody próbkowania\n",
    "section": "\n7.5 Zbiór walidacyjny",
    "text": "7.5 Zbiór walidacyjny\nWe wcześniejszych rozdziałach wspominana była metoda z wykorzystaniem zbioru walidacyjnego. Polega ona na tym, że przy tworzeniu podziału zbioru na uczący i testowy, dodatkowo zbiór uczący dzieli się na właściwy uczący i walidacyjny (patrz Rysunek 7.5). Zbiór walidacyjny jest wykorzystywany do oceny dopasowania modelu, np. w procesie optymalizacji hiperparametrów modelu.\n\n\nRysunek 7.5: Podział zbiorów na uczący, testowy i walidacyjny\n\n\nPodziału można dokonać też od razu dzieląc cały zbiór danych na trzy części (patrz Rysunek 7.6).\n\n\nRysunek 7.6: Podział na trzy zbiory\n\n\nZbiory walidacyjne są często wykorzystywane, gdy pierwotna pula danych jest bardzo duża. W tym przypadku, pojedyncza duża partia danych jest wystarczająca do scharakteryzowania dopasowania modelu bez konieczności wykonywania wielu iteracji próbkowania.\n\nKodset.seed(1002)\nval_set &lt;- validation_split(ames_train, prop = 3/4)\nval_set\n\n# Validation Set Split (0.75/0.25)  \n# A tibble: 1 × 2\n  splits             id        \n  &lt;list&gt;             &lt;chr&gt;     \n1 &lt;split [1756/586]&gt; validation"
  },
  {
    "objectID": "resampling2.html#bootstrapping",
    "href": "resampling2.html#bootstrapping",
    "title": "\n7  Metody próbkowania\n",
    "section": "\n7.6 Bootstrapping",
    "text": "7.6 Bootstrapping\nBootstrap został pierwotnie wynaleziony jako metoda aproksymacji próbkowego rozkładu statystyki, którego własności teoretyczne są nieznane (Davison i Hinkley 1997). Wykorzystanie jej do szacowania dopasowania modelu jest wtórnym zastosowaniem tej metody.\nPróbka bootstrapowa zbioru treningowego to próbka, która ma ten sam rozmiar co zbiór treningowy, ale jest losowana ze zwracaniem. Oznacza to, że niektóre obserwacje zbioru treningowego są wielokrotnie wybierane do zbioru analitycznego. Każdy punkt danych ma 63,2% szans na włączenie do zbioru uczącego przynajmniej raz. Zestaw oceny zawiera wszystkie próbki zestawu treningowego, które nie zostały wybrane do zestawu analitycznego (średnio 36,8% zestawu treningowego). Podczas bootstrappingu zestaw oceny jest często nazywany próbką poza workiem (ang. Out-Of-Bag).\nDla zbioru treningowego składającego się z 30 próbek, schemat trzech próbek bootstrapowych przedstawiono na Rysunek 7.7\n\n\nRysunek 7.7: Trzy próby bootstrapowe\n\n\n\nKodbootstraps(ames_train, times = 5)\n\n# Bootstrap sampling \n# A tibble: 5 × 2\n  splits             id        \n  &lt;list&gt;             &lt;chr&gt;     \n1 &lt;split [2342/858]&gt; Bootstrap1\n2 &lt;split [2342/855]&gt; Bootstrap2\n3 &lt;split [2342/852]&gt; Bootstrap3\n4 &lt;split [2342/851]&gt; Bootstrap4\n5 &lt;split [2342/867]&gt; Bootstrap5\n\n\nPróbki bootstrapowe dają oszacowania dopasowania, które mają bardzo niską wariancję (w przeciwieństwie do walidacji krzyżowej), ale są pesymistyczne w ocenie obciążenia. Oznacza to, że jeśli prawdziwa dokładność modelu wynosi 90%, bootstrap będzie miał tendencję do oszacowania wartości mniejszej niż 90%. Wielkość błędu systematycznego nie może być określona empirycznie z wystarczającą dokładnością. Dodatkowo, wielkość błędu systematycznego zmienia się w zależności od skali dopasowania. Na przykład obciążenie będzie prawdopodobnie inne, gdy dokładność wynosi 90% w porównaniu z 70%.\nBootstrap jest również wykorzystywany wewnątrz wielu modeli. Na przykład, wspomniany wcześniej model lasu losowego zawierał 1000 indywidualnych drzew decyzyjnych. Każde drzewo było produktem innej próbki bootstrapowej zbioru treningowego."
  },
  {
    "objectID": "resampling2.html#kroczące-próbkowanie-źródła",
    "href": "resampling2.html#kroczące-próbkowanie-źródła",
    "title": "\n7  Metody próbkowania\n",
    "section": "\n7.7 Kroczące próbkowanie źródła",
    "text": "7.7 Kroczące próbkowanie źródła\nGdy dane mają istotny składnik czasowy (jak np. szeregi czasowe), metoda próbkowania powinna pozwolić na oszacowanie sezonowych i okresowych trendów w szeregach czasowych. Technika, która losowo próbkuje wartości ze zbioru treningowego, nie pozwoli na oszacowanie tych wzorców.\nKroczące próbkowanie źródła (ang. rolling forecast origin resampling) jest metodą, która emuluje sposób, w jaki dane szeregów czasowych są często partycjonowane w praktyce, estymując parametry modelu na danymych historycznych i oceniając go z najnowszymi danymi (Hyndman i Athanasopoulos 2018). Dla tego typu resamplingu określa się rozmiar zbiorów analiz i ocen. Pierwsza iteracja resamplingu wykorzystuje te rozmiary, zaczynając od początku serii. Druga iteracja wykorzystuje te same rozmiary danych, ale przesuwa się o ustaloną liczbę próbek.\n\n\nRysunek 7.8: Próbkowanie kroczące ze źródła\n\n\nDla zilustrowania, zbiór treningowy składający się z piętnastu próbek został ponownie próbkowany z rozmiarem zbioru analizy wynoszącym osiem próbek i zbioru oceny wynoszącym trzy. W drugiej iteracji odrzucono pierwszą próbkę zbioru uczącego, a oba zbiory danych przesunięto do przodu o jeden. W tej konfiguracji uzyskuje się pięć próbek, jak pokazano na Rysunek 7.8.\nIstnieją dwie różne konfiguracje tej metody:\n\nZestaw analiz może narastać (w przeciwieństwie do utrzymywania tego samego rozmiaru). Po pierwszym początkowym zestawie analitycznym nowe próbki mogą narastać bez odrzucania wcześniejszych danych. W rezultacie oznacza to, że po nauczeniu i ocenie dopasowania modelu na Resample 1, model jest uczony na zbiorze rozszerzonym o obserwację 9, czyli na danych od 1 do 9. Następnie oceniany na obserwacjach od 10 do 12, itd.\nPróbki nie muszą być zwiększane o jeden. Na przykład, w przypadku dużych zestawów danych, blok przyrostowy może wynosić tydzień lub miesiąc zamiast dnia."
  },
  {
    "objectID": "resampling2.html#oszacowanie-dopasowania-z-wykorzystaniem-resamplingu",
    "href": "resampling2.html#oszacowanie-dopasowania-z-wykorzystaniem-resamplingu",
    "title": "\n7  Metody próbkowania\n",
    "section": "\n7.8 Oszacowanie dopasowania z wykorzystaniem resamplingu",
    "text": "7.8 Oszacowanie dopasowania z wykorzystaniem resamplingu\nKażda z metod resamplingu omówionych w tym rozdziale może być wykorzystana do oceny procesu modelowania (w tym przetwarzania wstępnego, dopasowania modelu itp.). Metody te są skuteczne, ponieważ do trenowania modelu i oceny modelu wykorzystywane są różne grupy danych. Przebiega on w następujący sposób:\n\nPodczas resamplingu zbiór analityczny jest używany do wstępnego przetwarzania danych, a następnie przetworzonych danych używa do dopasowania modelu.\nStatystyki przetwarzania wstępnego opracowane przez zbiór analiz są stosowane do zbioru oceny. Prognozy ze zbioru oceny wskazują wydajność modelu na nowych danych.\n\nTa sekwencja powtarza się dla każdej próby. Jeśli istnieje \\(B\\) prób, wówczas mamy \\(B\\) powtórzeń każdej z metryk dopasowania. Ostateczna ocena jest średnią tych \\(B\\) statystyk. Jeśli \\(B = 1\\), jak w przypadku zbioru walidacyjnego, pojedyncze statystyki reprezentują ogólne dopasowanie.\n\n\n\n\nKod# ustawiamy kontrolę resamplingu w ten sposbów aby zapisać predykcje i \n# nauczony przepływ - domyślnie nie są zapisywane\nkeep_pred &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE)\n\nset.seed(1003)\nrf_res &lt;- \n  rf_wflow |&gt;\n  fit_resamples(resamples = ames_folds, control = keep_pred)\nrf_res\n\n# Resampling results\n# 10-fold cross-validation \n# A tibble: 10 × 5\n   splits             id     .metrics         .notes           .predictions\n   &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n 1 &lt;split [2107/235]&gt; Fold01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 2 &lt;split [2107/235]&gt; Fold02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 3 &lt;split [2108/234]&gt; Fold03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 4 &lt;split [2108/234]&gt; Fold04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 5 &lt;split [2108/234]&gt; Fold05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 6 &lt;split [2108/234]&gt; Fold06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 7 &lt;split [2108/234]&gt; Fold07 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 8 &lt;split [2108/234]&gt; Fold08 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 9 &lt;split [2108/234]&gt; Fold09 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n10 &lt;split [2108/234]&gt; Fold10 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n\n\nChociaż te kolumny listy mogą wyglądać zniechęcająco, można je łatwo przekonfigurować za pomocą tidymodels. Na przykład, aby zwrócić metryki wydajności w bardziej użytecznym formacie:\n\nKodcollect_metrics(rf_res)\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   0.0719    10 0.00322 Preprocessor1_Model1\n2 rsq     standard   0.839     10 0.00776 Preprocessor1_Model1\n\n\nAby uzyskać metryki dla każdej próby, należy użyć opcji summarize = FALSE. Natomiast aby uzyskać predykcje z poszczególnych foldów użyjemy:\n\nKodassess_res &lt;- collect_predictions(rf_res)\nassess_res\n\n# A tibble: 2,342 × 5\n   id     .pred  .row Sale_Price .config             \n   &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;               \n 1 Fold01  5.10    10       5.09 Preprocessor1_Model1\n 2 Fold01  5.06    27       5.08 Preprocessor1_Model1\n 3 Fold01  5.11    47       5.10 Preprocessor1_Model1\n 4 Fold01  5.04    52       5.11 Preprocessor1_Model1\n 5 Fold01  5.14    59       5    Preprocessor1_Model1\n 6 Fold01  4.99    63       4.90 Preprocessor1_Model1\n 7 Fold01  5.10    65       5.11 Preprocessor1_Model1\n 8 Fold01  4.90    66       4.77 Preprocessor1_Model1\n 9 Fold01  5.15    67       5.10 Preprocessor1_Model1\n10 Fold01  5.03    68       5.10 Preprocessor1_Model1\n# ℹ 2,332 more rows\n\n\nKolumna .row jest liczbą całkowitą, która odpowiada numerowi wiersza oryginalnego zestawu treningowego, tak aby te wyniki mogły być odpowiednio połączone z oryginalnymi danymi.\n\n\n\n\n\n\nWskazówka\n\n\n\nDla niektórych metod resamplingu, takich jak bootstrap lub walidacja krzyżowa z powtórzeniami, otrzymamy wiele predykcji na wiersz oryginalnego zestawu treningowego. Aby uzyskać jedną statystykę (średnią z predykcji) użyj collect_predictions(object, summarize = TRUE).\n\n\n\nKodassess_res |&gt;\n  ggplot(aes(x = Sale_Price, y = .pred)) + \n  geom_point(alpha = .15) +\n  geom_abline(color = \"red\") + \n  coord_obs_pred() + \n  ylab(\"Predicted\")\n\n\n\nRysunek 7.9: Porównanie predykcji z modelu z obserwowanymi wartościami na podstawie wyników resamplingu\n\n\n\n\nW zbiorze treningowym znajdują się dwa domy o niskiej zaobserwowanej cenie sprzedaży, których cena jest znacznie zawyżona przez model. Które to domy? Dowiedzmy się tego z wyniku assess_res:\n\nKodover_predicted &lt;- \n  assess_res |&gt; \n  mutate(residual = Sale_Price - .pred) |&gt;\n  arrange(desc(abs(residual))) |&gt;\n  slice(1:2)\nover_predicted\n\n# A tibble: 2 × 6\n  id     .pred  .row Sale_Price .config              residual\n  &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;\n1 Fold03  4.95    30       4.11 Preprocessor1_Model1   -0.844\n2 Fold05  4.92   316       4.12 Preprocessor1_Model1   -0.807\n\nKodames_train |&gt;\n  slice(over_predicted$.row) |&gt;\n  select(Gr_Liv_Area, Neighborhood, Year_Built, Bedroom_AbvGr, Full_Bath)\n\n# A tibble: 2 × 5\n  Gr_Liv_Area Neighborhood           Year_Built Bedroom_AbvGr Full_Bath\n        &lt;int&gt; &lt;fct&gt;                       &lt;int&gt;         &lt;int&gt;     &lt;int&gt;\n1         832 Old_Town                     1923             2         1\n2         733 Iowa_DOT_and_Rail_Road       1952             2         1\n\n\nIdentyfikacja takich przykładów ze szczególnie słabym dopasowaniem może pomóc nam śledzić i badać, dlaczego te konkretne przewidywania są tak słabe.\nJeśli chcielibyśmy użyć zbioru walidacyjnego do oceny modelu, użyjemy:\n\nKodval_res &lt;- rf_wflow |&gt; \n  fit_resamples(resamples = val_set)\nval_res\n\n# Resampling results\n# Validation Set Split (0.75/0.25)  \n# A tibble: 1 × 4\n  splits             id         .metrics         .notes          \n  &lt;list&gt;             &lt;chr&gt;      &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [1756/586]&gt; validation &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n\nKodcollect_metrics(val_res)\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   0.0752     1      NA Preprocessor1_Model1\n2 rsq     standard   0.827      1      NA Preprocessor1_Model1\n\n\nJak widać wyniki te są zbliżone do otrzymanych z wykorzystaniem próby testowej."
  },
  {
    "objectID": "resampling2.html#przetwarzanie-równoległe",
    "href": "resampling2.html#przetwarzanie-równoległe",
    "title": "\n7  Metody próbkowania\n",
    "section": "\n7.9 Przetwarzanie równoległe",
    "text": "7.9 Przetwarzanie równoległe\nPakiet tune używa pakietu foreach do przeprowadzenia obliczeń równoległych. Obliczenia te mogą być podzielone pomiędzy rdzenie procesora na tym samym komputerze lub na różnych komputerach, w zależności od wybranej technologii.\nLiczbę dostępnych rdzeni można wykryć za pomocą:\n\nKod# Liczba fizycznych rdzeni\nparallel::detectCores(logical = FALSE)\n\n[1] 8\n\nKod# liczba logicznych rdzeni  \nparallel::detectCores(logical = TRUE)\n\n[1] 8\n\n\nRóżnica między tymi dwoma wartościami jest związana z typem procesora. Na przykład większość procesorów Intela wykorzystuje hyperthreading2, który tworzy dwa wirtualne rdzenie dla każdego fizycznego rdzenia. Dodatkowe zasoby mogą poprawić wydajność, jednak rzadko można je stosować na wszystkich rdzeniach fizycznych, ponieważ pewne zasoby są potrzebne do obsługi hyperthreadingu.2 wielowątkowość współbieżna\nW przypadku fit_resamples() i innych funkcji w pakiecie tune, przetwarzanie równoległe występuje, gdy użytkownik zarejestruje pakiet równoległego przetwarzania. Owe pakiety definiują sposób wykonywania przetwarzania równoległego. W systemach operacyjnych Unix i macOS jedną z metod podziału obliczeń jest rozgałęzienie wątków. Aby to umożliwić, załaduj pakiet doMC i zarejestruj liczbę równoległych rdzeni dla foreach:\n\nKod# tylko w Unix i macOS\nlibrary(doMC)\nregisterDoMC(cores = 4)\n\n\nTo daje instrukcje dla fit_resamples(), aby uruchomić 1/4 obliczeń na każdym z czterech rdzeni. Aby zresetować obliczenia do przetwarzania sekwencyjnego:\n\nKodregisterDoSEQ()\n\n\nAlternatywnie, inne podejście do paralelizacji obliczeń wykorzystuje gniazda sieciowe. Pakiet doParallel umożliwia tę metodę (możliwą do wykorzystania przez wszystkie systemy operacyjne):\n\nKodlibrary(doParallel)\n\ncl &lt;- makePSOCKcluster(4)\nregisterDoParallel(cl)\n\n# fit_resamples()...\n\nstopCluster(cl)\n\n\nJeszcze innym pakietem R, który ułatwia przetwarzanie równoległe, jest pakiet future. Podobnie jak foreach, zapewnia on wykonywanie równoległych obliczeń na wybranej liczbie rdzeni. Pakiet ten jest używany w połączeniu z foreach poprzez pakiet doFuture.\nPrzetwarzanie równoległe z pakietem tune ma tendencję do zapewnienia liniowego przyspieszenia dla pierwszych kilku rdzeni. Oznacza to, że przy dwóch rdzeniach obliczenia są dwukrotnie szybsze. W zależności od typu danych i modelu, wzrost prędkości spada, tak że po włączeniu czterech czy pięciu rdzeni prędkość nie wzrośnie 4- czy 5-krotnie. Użycie większej liczby rdzeni nadal będzie skracać czas potrzebny do wykonania zadania; po prostu ich efektywność spada wraz z włączeniem dodatkowych rdzeni.\nZakończmy ostatnią uwagą na temat obliczeń równoległych. Dla każdej z tych technologii, wymagania dotyczące pamięci wzrastają z każdym dodatkowym rdzeniem. Na przykład, jeśli bieżący zestaw danych zajmuje 2 GB pamięci i używane są trzy rdzenie, całkowite zapotrzebowanie na pamięć wyniesie 8 GB (2 dla każdego procesu roboczego plus oryginał). Użycie zbyt wielu rdzeni może spowodować znaczne spowolnienie obliczeń (i komputera)."
  },
  {
    "objectID": "resampling2.html#przechowywanie-wyników-resamplingu",
    "href": "resampling2.html#przechowywanie-wyników-resamplingu",
    "title": "\n7  Metody próbkowania\n",
    "section": "\n7.10 Przechowywanie wyników resamplingu",
    "text": "7.10 Przechowywanie wyników resamplingu\nModele utworzone podczas próbkowania nie są zapisywane. Modele te są trenowane w celu oceny dopasowania i zazwyczaj nie potrzebujemy ich po obliczeniu miar dopasowania. Jeśli określone podejście do modelowania okaże się najlepszą opcją dla naszego zestawu danych, wtedy najlepszym wyborem jest ponowne dopasowanie modelu do całego zestawu uczącego, aby parametry modelu mogły być oszacowane przy użyciu większej ilości danych.\nPodczas gdy te modele utworzone podczas resamplingu nie są zachowywane, istnieje metoda na zapisanie ich lub części składników. Opcja extract funkcji control_resamples() określa funkcję, która przyjmuje pojedynczy argument; my użyjemy x. Gdy zostanie wykonana, x przekazuje w wyniku dopasowany obiekt przepływu, niezależnie od tego, czy przekazałeś fit_resamples() z przepływem. Przypomnijmy, że pakiet workflows posiada funkcje, które mogą wyciągać różne składniki obiektów (np. model, recepturę itp.).\n\nKodames_rec &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_other(Neighborhood, threshold = 0.01) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) |&gt;\n  step_ns(Latitude, Longitude, deg_free = 20)\n\nlm_wflow &lt;-  \n  workflow() |&gt;\n  add_recipe(ames_rec) |&gt;\n  add_model(linear_reg() |&gt;set_engine(\"lm\")) \n\nlm_fit &lt;- lm_wflow |&gt; \n  fit(data = ames_train)\n\n# wyciągnijmy przepis\nextract_recipe(lm_fit, estimated = TRUE)\n\n# wyciągnijmy model\nget_model &lt;- function(x) {\n  extract_fit_parsnip(x) |&gt; \n    tidy()\n}\n\n# tak działa na pojedynczym modelu\nget_model(lm_fit)\n\n# A tibble: 72 × 5\n   term                             estimate  std.error statistic   p.value\n   &lt;chr&gt;                               &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)                      1.25     0.305          4.09  4.42e-  5\n 2 Gr_Liv_Area                      0.000163 0.00000433    37.6   5.23e-241\n 3 Year_Built                       0.00191  0.000143      13.4   2.23e- 39\n 4 Neighborhood_College_Creek       0.0131   0.0343         0.382 7.03e-  1\n 5 Neighborhood_Old_Town           -0.0559   0.0129        -4.35  1.44e-  5\n 6 Neighborhood_Edwards            -0.0799   0.0282        -2.84  4.60e-  3\n 7 Neighborhood_Somerset            0.0777   0.0198         3.93  8.68e-  5\n 8 Neighborhood_Northridge_Heights  0.156    0.0287         5.43  6.24e-  8\n 9 Neighborhood_Gilbert             0.0322   0.0226         1.43  1.54e-  1\n10 Neighborhood_Sawyer             -0.0911   0.0266        -3.43  6.23e-  4\n# ℹ 62 more rows\n\nKod# a tak na wyniku resamplingu\nctrl &lt;- control_resamples(extract = get_model)\n\nlm_res &lt;- lm_wflow |&gt; \n  fit_resamples(resamples = ames_folds, control = ctrl)\nlm_res\n\n# Resampling results\n# 10-fold cross-validation \n# A tibble: 10 × 5\n   splits             id     .metrics         .notes           .extracts       \n   &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;          \n 1 &lt;split [2107/235]&gt; Fold01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n 2 &lt;split [2107/235]&gt; Fold02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n 3 &lt;split [2108/234]&gt; Fold03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n 4 &lt;split [2108/234]&gt; Fold04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n 5 &lt;split [2108/234]&gt; Fold05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n 6 &lt;split [2108/234]&gt; Fold06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n 7 &lt;split [2108/234]&gt; Fold07 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n 8 &lt;split [2108/234]&gt; Fold08 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n 9 &lt;split [2108/234]&gt; Fold09 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n10 &lt;split [2108/234]&gt; Fold10 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n\n\nTeraz z każdego folda możemy wyciągnąć szczegóły modelu.\n\nKodlm_res$.extracts[[1]][[1]]\n\n[[1]]\n# A tibble: 72 × 5\n   term                             estimate  std.error statistic   p.value\n   &lt;chr&gt;                               &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)                      1.32     0.319          4.13  3.72e-  5\n 2 Gr_Liv_Area                      0.000169 0.00000465    36.3   1.38e-222\n 3 Year_Built                       0.00187  0.000150      12.4   2.85e- 34\n 4 Neighborhood_College_Creek       0.0187   0.0362         0.517 6.06e-  1\n 5 Neighborhood_Old_Town           -0.0521   0.0136        -3.84  1.29e-  4\n 6 Neighborhood_Edwards            -0.0737   0.0297        -2.49  1.30e-  2\n 7 Neighborhood_Somerset            0.0763   0.0213         3.58  3.56e-  4\n 8 Neighborhood_Northridge_Heights  0.151    0.0305         4.96  7.77e-  7\n 9 Neighborhood_Gilbert             0.0287   0.0242         1.19  2.35e-  1\n10 Neighborhood_Sawyer             -0.0861   0.0280        -3.08  2.12e-  3\n# ℹ 62 more rows\n\n\nMoże się to wydawać zawiłą metodą zapisywania wyników modelu. Jednakże, extract jest elastyczna i nie zakłada, że użytkownik będzie zapisywał tylko jedną tibble dla każdej próbki. Na przykład, metoda tidy() może być uruchomiona zarówno na przepisie jak i na modelu. W tym przypadku zwrócona zostanie lista dwóch tibble.\n\nKodall_coef &lt;- map_dfr(lm_res$.extracts, ~ .x[[1]][[1]])\nfilter(all_coef, term == \"Year_Built\")\n\n# A tibble: 10 × 5\n   term       estimate std.error statistic  p.value\n   &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 Year_Built  0.00187  0.000150      12.4 2.85e-34\n 2 Year_Built  0.00190  0.000153      12.5 2.06e-34\n 3 Year_Built  0.00191  0.000144      13.2 1.74e-38\n 4 Year_Built  0.00192  0.000151      12.8 6.61e-36\n 5 Year_Built  0.00188  0.000151      12.4 2.75e-34\n 6 Year_Built  0.00210  0.000152      13.8 9.75e-42\n 7 Year_Built  0.00186  0.000151      12.3 1.23e-33\n 8 Year_Built  0.00200  0.000154      13.0 4.43e-37\n 9 Year_Built  0.00191  0.000153      12.5 1.97e-34\n10 Year_Built  0.00188  0.000154      12.2 3.47e-33\n\n\n\n\n\n\nDavison, A. C., i D. V. Hinkley. 1997. „Bootstrap Methods and their Application”, październik. https://doi.org/10.1017/cbo9780511802843.\n\n\nHyndman, Robin John, i George Athanasopoulos. 2018. Forecasting: Principles and Practice. 2nd wyd. Australia: OTexts.\n\n\nXu, Qing-Song, i Yi-Zeng Liang. 2001. „Monte Carlo Cross Validation”. Chemometrics and Intelligent Laboratory Systems 56 (1): 1–11. https://doi.org/10.1016/s0169-7439(00)00122-2."
  },
  {
    "objectID": "comparison.html#tworzenie-przepływów-pracy-do-porównania-modeli",
    "href": "comparison.html#tworzenie-przepływów-pracy-do-porównania-modeli",
    "title": "\n8  Porównanie modeli\n",
    "section": "\n8.1 Tworzenie przepływów pracy do porównania modeli",
    "text": "8.1 Tworzenie przepływów pracy do porównania modeli\nDo porównania zbudujemy trzy modele regresji liniowej ale z innymi procedurami przygotowania danych.\n\nKodlibrary(tidymodels)\ntidymodels_prefer()\n\nset.seed(1001)\names &lt;- ames |&gt;mutate(Sale_Price = log10(Sale_Price))\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\nbasic_rec &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors())\n\ninteraction_rec &lt;- \n  basic_rec %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) \n\nspline_rec &lt;- \n  interaction_rec %&gt;% \n  step_ns(Latitude, Longitude, deg_free = 50)\n\npreproc &lt;- \n  list(basic = basic_rec, \n       interact = interaction_rec, \n       splines = spline_rec\n  )\n\nlm_models &lt;- workflow_set(preproc, list(lm = linear_reg()), cross = FALSE)\nlm_models\n\n# A workflow set/tibble: 3 × 4\n  wflow_id    info             option    result    \n  &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 basic_lm    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 interact_lm &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 splines_lm  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n\nAby je porównać zastosujemy resampling do każdego modelu stosując funkcję workflow_map pakietu purrr.\n\nKodset.seed(1001)\names_folds &lt;- vfold_cv(ames_train, v = 10)\nkeep_pred &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE)\n\nlm_models &lt;- \n  lm_models %&gt;% \n  workflow_map(\"fit_resamples\", \n               # Options to `workflow_map()`: \n               seed = 1101, verbose = TRUE,\n               # Options to `fit_resamples()`: \n               resamples = ames_folds, \n               control = keep_pred)\n\nlm_models\n\n# A workflow set/tibble: 3 × 4\n  wflow_id    info             option    result   \n  &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n1 basic_lm    &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n2 interact_lm &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n3 splines_lm  &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n\n\nZauważmy, że kolumny option i result są teraz wypełnione. Pierwsza zawiera opcje do fit_resamples(), które zostały podane (dla odtwarzalności), a druga kolumna zawiera wyniki uzyskane przez fit_resamples().\nIstnieje kilka wygodnych funkcji przeznaczonych do zestawów przepływów pracy, w tym collect_metrics() do zestawiania statystyk wydajności. Możemy też filtrować() dowolną konkretną metrykę, która nas interesuje:\n\nKodcollect_metrics(lm_models) %&gt;% \n  filter(.metric == \"rmse\")\n\n# A tibble: 3 × 9\n  wflow_id    .config      preproc model .metric .estimator   mean     n std_err\n  &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 basic_lm    Preprocesso… recipe  line… rmse    standard   0.0796    10 0.00285\n2 interact_lm Preprocesso… recipe  line… rmse    standard   0.0791    10 0.00283\n3 splines_lm  Preprocesso… recipe  line… rmse    standard   0.0773    10 0.00297\n\n\nA co z modelem lasu losowego z poprzedniego rozdziału? Możemy go dodać do zestawu, najpierw konwertując go do własnego zestawu przepływów pracy, a następnie wiążąc wiersze. Wymaga to, aby podczas ponownego próbkowania modelu w funkcji sterującej ustawiona była opcja save_workflow = TRUE.\n\nKodrf_model &lt;- \n  rand_forest(trees = 1000) |&gt; \n  set_engine(\"ranger\") |&gt; \n  set_mode(\"regression\")\n\nrf_wflow &lt;- \n  workflow() |&gt; \n  add_formula(\n    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n      Latitude + Longitude) |&gt; \n  add_model(rf_model) \n\nrf_res &lt;- \n  rf_wflow |&gt;\n  fit_resamples(resamples = ames_folds, control = keep_pred)\n\nfour_models &lt;- \n  as_workflow_set(random_forest = rf_res) %&gt;% \n  bind_rows(lm_models)\n\nfour_models\n\n# A workflow set/tibble: 4 × 4\n  wflow_id      info             option    result   \n  &lt;chr&gt;         &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n1 random_forest &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;rsmp[+]&gt;\n2 basic_lm      &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n3 interact_lm   &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n4 splines_lm    &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n\n\nFunkcja autoplot(), której wyniki przedstawiono na Rysunek 8.1, pokazuje przedziały ufności dla każdego modelu w kolejności od najlepszego do najgorszego. Jeśli chcemy się skupić na konkretnej mierze wybieramy metric = 'rsq', inaczej funkcja dobierze domyślną metrykę.\n\nKodlibrary(ggrepel)\nautoplot(four_models, metric = \"rsq\") +\n  geom_text_repel(aes(label = wflow_id), nudge_x = 1/8, nudge_y = 1/100) +\n  theme(legend.position = \"none\")\n\n\n\nRysunek 8.1: Porównanie jakości dopasowania czterech modeli\n\n\n\n\nZ powyższego porównania widać, że model lasu losowego najlepiej radzi sobie z przewidywaniem wartości wynikowej. Jednocześnie widzimy, że kolejne czynności preprocessingu tylko nieznacznie poprawiają dopasowanie."
  },
  {
    "objectID": "comparison.html#porównanie-próbkowań",
    "href": "comparison.html#porównanie-próbkowań",
    "title": "\n8  Porównanie modeli\n",
    "section": "\n8.2 Porównanie próbkowań",
    "text": "8.2 Porównanie próbkowań\nStwierdzony powyżej niewielki wpływ kolejnych czynności preprocessingu jest określeniem niejasnym. Można to uściślić stosując test statystyczny do porównania analizowanych wielkości.\n\n\n\n\n\n\nZagrożenie\n\n\n\nPrzed dokonaniem porównań pomiędzy modelami, ważne jest, abyśmy omówili korelację wewnątrz-próbkową dla statystyk resamplingu. Każdy model był mierzony z tymi samymi foldami walidacji krzyżowej, a wyniki dla tej samej próby mają tendencję do bycia podobnymi.\n\n\nInnymi słowy, istnieją pewne próbki, dla których wydajność modeli jest niska i inne, w których jest wysoka. W statystyce nazywa się to składnikiem zmienności “próbka do próbki”.\nDla zobrazowania tego problemu zbierzmy poszczególne statystyki resamplingu dla modeli liniowych i lasu losowego. Skupimy się na \\(R^2\\) dla każdego modelu, która mierzy korelację pomiędzy obserwowanymi i przewidywanymi cenami sprzedaży dla każdego domu. Przefiltrujmy wyniki, aby zachować tylko statystyki \\(R^2\\), przekształćmy wyniki i obliczmy jak metryki są ze sobą skorelowane.\n\nKodrsq_indiv_estimates &lt;- \n  collect_metrics(four_models, summarize = FALSE) %&gt;% \n  filter(.metric == \"rsq\") \n\nrsq_wider &lt;- \n  rsq_indiv_estimates %&gt;% \n  select(wflow_id, .estimate, id) %&gt;% \n  pivot_wider(id_cols = \"id\", names_from = \"wflow_id\", values_from = \".estimate\")\n\ncorrr::correlate(rsq_wider %&gt;% select(-id), quiet = TRUE)\n\n# A tibble: 4 × 5\n  term          random_forest basic_lm interact_lm splines_lm\n  &lt;chr&gt;                 &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1 random_forest        NA        0.549       0.537      0.592\n2 basic_lm              0.549   NA           0.995      0.907\n3 interact_lm           0.537    0.995      NA          0.933\n4 splines_lm            0.592    0.907       0.933     NA    \n\n\nKorelacje te są wysokie, co można też zauważyć wykreślając poziomy \\(R^2\\) dla różnych modeli i różnych foldów.\n\nKodrsq_indiv_estimates %&gt;% \n  mutate(wflow_id = reorder(wflow_id, .estimate)) %&gt;% \n  ggplot(aes(x = wflow_id, y = .estimate, group = id, color = id)) + \n  geom_line(alpha = .5, lwd = 1.25) + \n  theme(legend.position = \"none\")\n\n\n\n\nGdyby efektu podobieństwa pomiędzy miarami dla poszczególnych foldów by nie było, wówczas linie nie byłyby równoległe. Można to stwierdzić również na podstawie testów współczynnika korelacji.\n\nKodrsq_wider %&gt;% \n  with( cor.test(basic_lm, splines_lm) ) %&gt;% \n  tidy() %&gt;% \n  select(estimate, starts_with(\"conf\"))\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.907    0.648     0.978\n\n\nJak widać korelacje te nie są przypadkowe. Stąd wniosek, że korelacje pomiędzy wynikami foldów występują. Będziemy to musieli uwzględnić porównując statystyki \\(R^2\\). Ponieważ nie interesuje nas efekt foldów a jedynie różnica pomiędzy statystykami poszczególnych modeli, to porównamy je za pomocą modelu liniowego (ANOVA z powtarzanymi pomiarami).\n\nKodlibrary(rstatix)\nrsq_indiv_estimates |&gt; \n  select(wflow_id, id, .estimate) |&gt; \n  anova_test(dv = .estimate, wid = id, within = wflow_id)\n\nANOVA Table (type III tests)\n\n$ANOVA\n    Effect DFn DFd      F        p p&lt;.05   ges\n1 wflow_id   3  27 14.876 6.54e-06     * 0.225\n\n$`Mauchly's Test for Sphericity`\n    Effect     W        p p&lt;.05\n1 wflow_id 0.011 2.21e-06     *\n\n$`Sphericity Corrections`\n    Effect   GGe      DF[GG] p[GG] p[GG]&lt;.05   HFe      DF[HF]    p[HF]\n1 wflow_id 0.462 1.39, 12.47 0.001         * 0.519 1.56, 14.01 0.000635\n  p[HF]&lt;.05\n1         *\n\n\nGdyby chcieć porównać jedynie model lasu losowego i najgorszego modelu liniowego, można użyć statystyki różnicy.\n\nKodcompare_lm &lt;- \n  rsq_wider %&gt;% \n  mutate(difference = random_forest - basic_lm)\n\nlm(difference ~ 1, data = compare_lm) %&gt;% \n  tidy(conf.int = TRUE) %&gt;% \n  select(estimate, p.value, starts_with(\"conf\"))\n\n# A tibble: 1 × 4\n  estimate p.value conf.low conf.high\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1   0.0396 0.00173   0.0192    0.0600\n\nKod# Alternatively, a paired t-test could also be used: \nrsq_wider %&gt;% \n  with( t.test(random_forest, basic_lm, paired = TRUE) ) %&gt;%\n  tidy() %&gt;% \n  select(estimate, p.value, starts_with(\"conf\"))\n\n# A tibble: 1 × 4\n  estimate p.value conf.low conf.high\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1   0.0396 0.00173   0.0192    0.0600"
  },
  {
    "objectID": "tuning.html#konsekwencje-złego-oszacowania-parametrów",
    "href": "tuning.html#konsekwencje-złego-oszacowania-parametrów",
    "title": "\n9  Optymalizacja modeli\n",
    "section": "\n9.1 Konsekwencje złego oszacowania parametrów",
    "text": "9.1 Konsekwencje złego oszacowania parametrów\nWiele parametrów dostrajania modyfikuje stopień złożoności modelu. Większa złożoność często oznacza większą plastyczność wzorców, które model może naśladować. Na przykład, dodanie stopni swobody w funkcji splajnu zwiększa złożoność predykcji. Choć jest to zaleta, gdy motywacją są leżące u podstaw danych złożone zależności, może to również prowadzić do nadinterpretacji przypadkowych wzorców, które nie powtarzałyby się w nowych danych. Overfitting to sytuacja, w której model zbytnio dostosowuje się do danych treningowych; działa dobrze dla danych użytych do budowy modelu, ale słabo dla nowych danych.\n\n\n\n\nPrzyjrzyjmy się przykładowo modelowi jednowarstwowej sieci neuronowej z jedną warstwą wejściową i jedną warstwą ukrytą z sigmoidalną funkcjach aktywacji. Taka sieć neuronowa to, na dobrą sprawę, po prostu regresja logistyczna2. Jednak wraz ze wzrostem liczby neuronów w warstwie ukrytej rośnie złożoność modelu.2 oczywiście tylko wtedy gdy warstwa ukryta ma jeden neuron, a wejściem do niego jest warstwa z aktywacją liniową\nDopasowaliśmy modele klasyfikacyjne sieci neuronowych do tych samych dwuklasowych danych z poprzedniego przykładu, zmieniając liczbę neuronów w warstwie ukrytej. Używając obszaru pod krzywą ROC jako metryki wydajności, skuteczność modelu na zbiorze treningowym rośnie wraz z dodawaniem kolejnych jednostek ukrytych. Model sieci dokładnie i skrupulatnie uczy się zbioru treningowego. Jeśli ocena modelu jest dokonywana na podstawie wartości ROC zbioru treningowego, to preferuje się wiele neuronów w warstwie ukrytej, tak aby można było prawie całkowicie wyeliminować błędy.\nRozdziały wcześniejsze pokazały, że ocena dopasowania na zbiorze uczącym jest złym podejściem do oceny modelu. W tym przypadku sieć neuronowa bardzo szybko zaczyna nadinterpretować wzorce, które widzi w zbiorze treningowym. Porównaj trzy przykładowe granice klas (opracowane na podstawie zbioru treningowego) nałożone na zbiory treningowe i testowe na Rysunek 9.3.\n\n\nRysunek 9.3: Porównanie sieci neuronowych z różna liczbą neuronów w warstwie ukrytej\n\n\nModel z pojedynczym neuronem w warstwie ukrytej nie dostosowuje się zbyt elastycznie do danych (ponieważ jest ograniczony do bycia liniowym). Model z czterema jednostkami ukrytymi zaczyna wykazywać oznaki przeuczenia z nierealistyczną granicą dla wartości oddalonych od chmury danych. Dla 20 neuronów ukrytych model zaczyna zapamiętywać zbiór treningowy, tworząc małe wyspy wokół pojedynczych obserwacji, aby zminimalizować współczynnik błędu. Wzorce te nie powtarzają się w zbiorze testowym. W przypadku modelu składającego się z 20 jednostek, współczynnik ROC AUC dla zbioru treningowego wynosi 0,944, ale wartość dla zbioru testowego to 0,855."
  },
  {
    "objectID": "tuning.html#dwa-podejścia-do-tuningu",
    "href": "tuning.html#dwa-podejścia-do-tuningu",
    "title": "\n9  Optymalizacja modeli\n",
    "section": "\n9.2 Dwa podejścia do tuningu",
    "text": "9.2 Dwa podejścia do tuningu\nIstnieją dwa sposoby realizacji tuningu modeli:\n\nprzeszukiwanie siatki - gdy wstępnie określamy zestaw wartości parametrów do oceny. Głównymi problemami związanymi z przeszukiwaniem siatki są sposób wykonania siatki i liczba kombinacji parametrów do oceny. Przeszukiwanie siatki jest często oceniane jako nieefektywne, ponieważ liczba punktów siatki wymaganych do pokrycia przestrzeni parametrów może stać się niemożliwa do opanowania. Z jednej strony ten fakt przemawiałby za nie wybieraniem tej metody do poszukiwania optymalnych parametrów, ale jest to najbardziej uzasadniona metoda, gdy proces nie jest zoptymalizowany w kontekście kierunków przeszukiwania.\nprzeszukiwanie iteracyjne lub sekwencyjne - gdy sekwencyjnie odkrywamy nowe kombinacje parametrów na podstawie poprzednich wyników. W niektórych przypadkach do rozpoczęcia procesu optymalizacji wymagany jest wstępny zestaw wyników dla jednej lub więcej kombinacji parametrów.\n\n\n\nRysunek 9.4: Porównanie metod przeszukiwania\n\n\nMożna też stosować rozwiązania hybrydowe, gdzie metoda siatki jest stosowana do wstępnego oszacowania parametrów modelu, a następnie metodami iteracyjnymi korygowane są wspomniane parametry.\nW procesie dostrajania modelu, możemy szacować hiperparametry główne oraz specyficzne dla danego silnika metody.\n\nKodrand_forest(trees = 2000, min_n = 10) %&gt;%                   # &lt;- main arguments\n  set_engine(\"ranger\", regularization.factor = 0.5)         # &lt;- engine-specific\n\n\nAby przekazać informację do przebiegu pracy, że chcemy kalibrować pewne parametry modelu, należy podstawić pod ich wartości funkcję tune().\n\nKodneural_net_spec &lt;- \n  mlp(hidden_units = tune()) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"keras\")\n\n\nChcąc zobaczyć jak model interpretuje takie podstawienie możemy użyć\n\nKodextract_parameter_set_dials(neural_net_spec)\n\nCollection of 1 parameters for tuning\n\n   identifier         type    object\n hidden_units hidden_units nparam[+]\n\n\nWyniki pokazują wartość nparam[+], co wskazuje, że liczba jednostek ukrytych jest parametrem liczbowym. Istnieje opcjonalny argument identyfikacyjny, który kojarzy nazwę z parametrami. Może się to przydać, gdy ten sam rodzaj parametrów jest dostrajany w różnych miejscach. Na przykład, w przypadku danych dotyczących mieszkania w Ames, przepis zakodował zarówno długość jak i szerokość geograficzną za pomocą funkcji spline. Jeśli chcemy dostroić dwie funkcje splajnu, aby potencjalnie miały różne poziomy gładkości, wywołujemy step_ns() dwukrotnie, raz dla każdego predyktora. Aby parametry były identyfikowalne, argument “identyfikator” może przyjąć dowolny ciąg znaków:\n\nKodset.seed(1001)\names &lt;- ames |&gt;mutate(Sale_Price = log10(Sale_Price))\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\names_rec &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train)  %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = tune()) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;% \n  step_ns(Longitude, deg_free = tune(\"longitude df\")) %&gt;% \n  step_ns(Latitude,  deg_free = tune(\"latitude df\"))\n\nrecipes_param &lt;- extract_parameter_set_dials(ames_rec)\nrecipes_param\n\nCollection of 3 parameters for tuning\n\n   identifier      type    object\n    threshold threshold nparam[+]\n longitude df  deg_free nparam[+]\n  latitude df  deg_free nparam[+]\n\n\nGdy receptura i specyfikacja modelu są połączone za pomocą przepływu pracy, oba zestawy parametrów są wyświetlane.\n\nKodwflow_param &lt;- \n  workflow() %&gt;% \n  add_recipe(ames_rec) %&gt;% \n  add_model(neural_net_spec) %&gt;% \n  extract_parameter_set_dials()\nwflow_param\n\nCollection of 4 parameters for tuning\n\n   identifier         type    object\n hidden_units hidden_units nparam[+]\n    threshold    threshold nparam[+]\n longitude df     deg_free nparam[+]\n  latitude df     deg_free nparam[+]\n\n\nW tym przypadku dodanie splajnów do sieci neuronowej jest tylko na potrzeby przykładu, ponieważ sama sieć neuronowa jest w stanie zamodelować dowolnie złożone zależności (w tym splajny) 🙉.\nKażdy argument parametru dostrajania ma odpowiadającą mu funkcję w pakiecie dials. W zdecydowanej większości przypadków funkcja ma taką samą nazwę jak argument parametru:\n\nKodhidden_units()\n\n# Hidden Units (quantitative)\nRange: [1, 10]\n\nKodthreshold()\n\nThreshold (quantitative)\nRange: [0, 1]\n\n\nParametr deg_free jest kontrprzykładem - pojęcie stopni swobody pojawia się w wielu różnych kontekstach. Gdy używamy splajnów, istnieje wyspecjalizowana funkcja dials o nazwie spline_degree(), która jest domyślnie wywoływana dla splajnów:\n\nKodspline_degree()\n\nSpline Degrees of Freedom (quantitative)\nRange: [1, 10]\n\n\nPakiet dials posiada również funkcję wygodną do wyodrębnienia konkretnego obiektu parametru:\n\nKodwflow_param %&gt;% extract_parameter_dials(\"threshold\")\n\nThreshold (quantitative)\nRange: [0, 0.1]\n\n\nWewnątrz zestawu parametrów, zakres parametrów może być również aktualizowany:\n\nKodextract_parameter_set_dials(ames_rec) %&gt;% \n  update(threshold = threshold(c(0.8, 1.0)))\n\nCollection of 3 parameters for tuning\n\n   identifier      type    object\n    threshold threshold nparam[+]\n longitude df  deg_free nparam[+]\n  latitude df  deg_free nparam[+]\n\n\nW niektórych przypadkach łatwo jest mieć rozsądne wartości domyślne dla zakresu możliwych wartości. W innych przypadkach zakres parametrów jest zależny od zbioru danych i nie można go założyć. Podstawowym parametrem dostrajania parametrów dla modeli lasu losowego jest liczba kolumn predyktorów, które są losowo próbkowane dla każdego podziału drzewa, zwykle oznaczana jako mtry(). Bez znajomości liczby predyktorów, ten zakres parametrów nie może być wstępnie skonfigurowany i wymaga finalizacji.\n\nKodrf_spec &lt;- \n  rand_forest(mtry = tune()) %&gt;% \n  set_engine(\"ranger\", regularization.factor = tune(\"regularization\")) %&gt;%\n  set_mode(\"regression\")\n\nrf_param &lt;- extract_parameter_set_dials(rf_spec)\nrf_param\n\nCollection of 2 parameters for tuning\n\n     identifier                  type    object\n           mtry                  mtry nparam[?]\n regularization regularization.factor nparam[+]\n\nModel parameters needing finalization:\n   # Randomly Selected Predictors ('mtry')\n\nSee `?dials::finalize` or `?dials::update.parameters` for more information.\n\n\nKompletne obiekty parametrów mają [+] w podsumowaniu; wartość [?] wskazuje, że brakuje przynajmniej jednego końca możliwego zakresu. Istnieją dwie metody radzenia sobie z tym problemem. Pierwszą jest użycie update(), aby dodać zakres na podstawie tego, co wiesz o wymiarach danych:\n\nKodrf_param %&gt;% \n  update(mtry = mtry(c(1, 70)))\n\nCollection of 2 parameters for tuning\n\n     identifier                  type    object\n           mtry                  mtry nparam[+]\n regularization regularization.factor nparam[+]\n\n\nJednak to podejście może nie działać, jeśli receptura jest dołączona do przepływu pracy, który używa kroków dodających lub odejmujących kolumny. Jeśli te kroki nie są przeznaczone do dostrajania, funkcja finalize() może wykonać recepturę raz, aby uzyskać wymiary:\n\nKodpca_rec &lt;- \n  recipe(Sale_Price ~ ., data = ames_train) %&gt;% \n  # Select the square-footage predictors and extract their PCA components:\n  step_normalize(contains(\"SF\")) %&gt;% \n  # Select the number of components needed to capture 95% of\n  # the variance in the predictors. \n  step_pca(contains(\"SF\"), threshold = .95)\n  \nupdated_param &lt;- \n  workflow() %&gt;% \n  add_model(rf_spec) %&gt;% \n  add_recipe(pca_rec) %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  finalize(ames_train)\nupdated_param\n\nCollection of 2 parameters for tuning\n\n     identifier                  type    object\n           mtry                  mtry nparam[+]\n regularization regularization.factor nparam[+]\n\nKodupdated_param %&gt;% extract_parameter_dials(\"mtry\")\n\n# Randomly Selected Predictors (quantitative)\nRange: [1, 74]\n\n\nGdy receptura jest przygotowana, funkcja finalize() uczy się ustawiać górny zakres mtry na 74 predyktory. Dodatkowo, wyniki extract_parameter_set_dials() będą zawierały parametry specyficzne dla silnika (jeśli takie istnieją). Są one odkrywane w taki sam sposób jak główne argumenty i włączane do zestawu parametrów. Pakiet dials zawiera funkcje parametrów dla wszystkich potencjalnie przestrajalnych parametrów specyficznych dla silnika:\n\nKodrf_param\n\nCollection of 2 parameters for tuning\n\n     identifier                  type    object\n           mtry                  mtry nparam[?]\n regularization regularization.factor nparam[+]\n\nModel parameters needing finalization:\n   # Randomly Selected Predictors ('mtry')\n\nSee `?dials::finalize` or `?dials::update.parameters` for more information.\n\nKodregularization_factor()\n\nGain Penalization (quantitative)\nRange: [0, 1]\n\n\nWreszcie, niektóre parametry dostrajania są związane z transformacjami. Dobrym przykładem jest parametr kary związany z wieloma modelami regresji regularyzowanej. Ten parametr jest nieujemny i często przedstawia się go w skali logarytmicznej. Podstawowy obiekt parametru dials wskazuje, że domyślnie używana jest transformacja:\n\nKodpenalty()\n\nAmount of Regularization (quantitative)\nTransformer: log-10 [1e-100, Inf]\nRange (transformed scale): [-10, 0]\n\n\nJest to ważna informacja, zwłaszcza przy zmianie zakresu. Nowe wartości zakresu muszą być w przekształconych jednostkach:\n\nKod# correct method to have penalty values between 0.1 and 1.0\npenalty(c(-1, 0)) %&gt;% value_sample(1000) %&gt;% summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.1001  0.1805  0.3115  0.3905  0.5582  0.9919 \n\nKod# incorrect:\npenalty(c(0.1, 1.0)) %&gt;% value_sample(1000) %&gt;% summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.260   2.126   3.454   4.132   5.812   9.905 \n\n\nSkala może być zmieniona w razie potrzeby za pomocą argumentu trans.\n\nKodpenalty(trans = NULL, range = 10^c(-10, 0))\n\nAmount of Regularization (quantitative)\nRange: [1e-10, 1]\n\n\n\n\n\n\nFriedman, Jerome H. 2001. „Greedy function approximation: A gradient boosting machine.” The Annals of Statistics 29 (5). https://doi.org/10.1214/aos/1013203451.\n\n\nThomas, Rachel, i David Uminsky. 2020. „The Problem with Metrics is a Fundamental Problem for AI”. https://doi.org/10.48550/ARXIV.2002.08512."
  },
  {
    "objectID": "grid_search.html#siatki-regularne",
    "href": "grid_search.html#siatki-regularne",
    "title": "\n10  Dostrajanie z przeszukiwaniem siatki\n",
    "section": "\n10.1 Siatki regularne",
    "text": "10.1 Siatki regularne\nRegularne siatki są kombinacjami oddzielnych zestawów wartości parametrów. Najpierw użytkownik tworzy odrębny zestaw wartości dla każdego parametru. Liczba możliwych wartości nie musi być taka sama dla każdego parametru. Funkcja tidyr crossing() jest jednym ze sposobów tworzenia siatki regularnej:\n\nKodcrossing(\n  hidden_units = 1:3,\n  penalty = c(0.0, 0.1),\n  epochs = c(100, 200)\n)\n\n# A tibble: 12 × 3\n   hidden_units penalty epochs\n          &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1            1     0      100\n 2            1     0      200\n 3            1     0.1    100\n 4            1     0.1    200\n 5            2     0      100\n 6            2     0      200\n 7            2     0.1    100\n 8            2     0.1    200\n 9            3     0      100\n10            3     0      200\n11            3     0.1    100\n12            3     0.1    200\n\n\nObiekt parametru zna zakresy parametrów. Pakiet dials zawiera zestaw funkcji grid_*(), które przyjmują obiekt parametru jako dane wejściowe, aby wytworzyć różne rodzaje siatek. Na przykład:\n\nKodgrid_regular(mlp_param, levels = 2)\n\n# A tibble: 8 × 3\n  hidden_units      penalty epochs\n         &lt;int&gt;        &lt;dbl&gt;  &lt;int&gt;\n1            1 0.0000000001     10\n2           10 0.0000000001     10\n3            1 1                10\n4           10 1                10\n5            1 0.0000000001   1000\n6           10 0.0000000001   1000\n7            1 1              1000\n8           10 1              1000\n\n\nArgument levels to liczba poziomów na parametr do utworzenia. Może również przyjąć uszczegółowiony wektor wartości:\n\nKodmlp_param %&gt;% \n  grid_regular(levels = c(hidden_units = 3, penalty = 2, epochs = 2))\n\n# A tibble: 12 × 3\n   hidden_units      penalty epochs\n          &lt;int&gt;        &lt;dbl&gt;  &lt;int&gt;\n 1            1 0.0000000001     10\n 2            5 0.0000000001     10\n 3           10 0.0000000001     10\n 4            1 1                10\n 5            5 1                10\n 6           10 1                10\n 7            1 0.0000000001   1000\n 8            5 0.0000000001   1000\n 9           10 0.0000000001   1000\n10            1 1              1000\n11            5 1              1000\n12           10 1              1000\n\n\nIstnieją techniki tworzenia regularnych siatek, które nie wykorzystują wszystkich możliwych wartości każdego zestawu parametrów. Można również wykorzystać te konstrukcje czynnikowe ułamkowe (Booth i in. 1979).\nJedną z zalet stosowania regularnej siatki jest to, że związki i wzorce między dostrajaniem parametrów i metrykami modelu są łatwo zrozumiałe. Czynnikowa natura tych planów pozwala na zbadanie każdego parametru osobno z niewielką współzależnością między parametrami."
  },
  {
    "objectID": "grid_search.html#siatki-nieregularne",
    "href": "grid_search.html#siatki-nieregularne",
    "title": "\n10  Dostrajanie z przeszukiwaniem siatki\n",
    "section": "\n10.2 Siatki nieregularne",
    "text": "10.2 Siatki nieregularne\nIstnieje kilka możliwości tworzenia nieregularnych siatek. Pierwszą z nich jest użycie losowego próbkowania w całym zakresie parametrów. Funkcja grid_random() generuje niezależne rozkłady jednostajne wartości w całym zakresie parametrów. Jeśli parametr ma powiązane przekształcenie (takie jak mamy dla kary), liczby losowe są generowane w przekształconej skali. Utwórzmy siatkę losową dla parametrów z naszej przykładowej sieci neuronowej:\n\nKodset.seed(1301)\nmlp_param %&gt;% \n  grid_random(size = 1000) %&gt;% # 'size' is the number of combinations\n  summary()\n\n  hidden_units       penalty              epochs     \n Min.   : 1.000   Min.   :0.0000000   Min.   : 10.0  \n 1st Qu.: 3.000   1st Qu.:0.0000000   1st Qu.:265.8  \n Median : 5.000   Median :0.0000061   Median :497.0  \n Mean   : 5.381   Mean   :0.0437435   Mean   :509.5  \n 3rd Qu.: 8.000   3rd Qu.:0.0026854   3rd Qu.:761.0  \n Max.   :10.000   Max.   :0.9814405   Max.   :999.0  \n\n\nDla penalty() liczby losowe są jednostajne w skali logarytmicznej ale wartości w siatce są w jednostkach naturalnych.\nProblem z siatkami losowymi polega na tym, że przy małych i średnich siatkach wartości losowe mogą powodować nakładanie się kombinacji parametrów. Ponadto siatka losowa musi pokryć całą przestrzeń parametrów, a prawdopodobieństwo dobrego pokrycia rośnie wraz z liczbą wartości siatki. Nawet dla próbki 20 obserwacji, na rysunku 13.1 widać pewne nakładanie się punktów dla naszego przykładowego perceptronu wielowarstwowego.\n\nKodlibrary(ggforce)\nset.seed(1302)\nmlp_param %&gt;% \n  # The 'original = FALSE' option keeps penalty in log10 units\n  grid_random(size = 20, original = FALSE) %&gt;% \n  ggplot(aes(x = .panel_x, y = .panel_y)) + \n  geom_point() +\n  geom_blank() +\n  facet_matrix(vars(hidden_units, penalty, epochs), layer.diag = 2) + \n  labs(title = \"Random design with 20 candidates\")\n\n\n\nRysunek 10.1: Siatka losowa dla 20 kombinacji parametrów\n\n\n\n\nZnacznie lepszym podejściem jest zastosowanie zestawu planów eksperymentalnych zwanych planami wypełniającymi przestrzeń. Chociaż różne metody projektowania mają nieco inne cele, to generalnie znajdują one konfigurację punktów, które pokrywają przestrzeń parametrów z najmniejszym prawdopodobieństwem wystąpienia nakładających się lub nadmiarowych wartości. Przykładami takich planów są hipersześciany łacińskie (ang. Latin hypercube) (McKay, Beckman, i Conover 1979), plany maksymalnej entropii (ang. maximum entropy) (Shewry i Wynn 1987), plany maksymalnej projekcji (ang. maximum projection) (Joseph, Gul, i Ba 2015)i inne.\n\nKodset.seed(1303)\nmlp_param %&gt;% \n  grid_latin_hypercube(size = 20, original = FALSE) %&gt;% \n  ggplot(aes(x = .panel_x, y = .panel_y)) + \n  geom_point() +\n  geom_blank() +\n  facet_matrix(vars(hidden_units, penalty, epochs), layer.diag = 2) + \n  labs(title = \"Latin Hypercube design with 20 candidates\")\n\n\n\nRysunek 10.2: Siatka oparka na kwadracie łacińskim dla 20 kombinacji parametrów\n\n\n\n\nChociaż nie jest to idealne rozwiązanie, hipersześcian łaciński umieszcza punkty dalej od siebie i pozwala na lepszą eksplorację przestrzeni hiperparametrów.\nKonstrukcje wypełniające przestrzeń mogą być bardzo skuteczne w reprezentowaniu przestrzeni parametrów. Domyślnym wzorem używanym przez pakiet tune jest plan kwadratu łacińskiego. Ma on tendencję do tworzenia siatek, które dobrze pokrywają przestrzeń kandydatów i drastycznie zwiększają szanse na znalezienie dobrych wyników."
  },
  {
    "objectID": "grid_search.html#ocena-hiperparametrów-sieci",
    "href": "grid_search.html#ocena-hiperparametrów-sieci",
    "title": "\n10  Dostrajanie z przeszukiwaniem siatki\n",
    "section": "\n10.3 Ocena hiperparametrów sieci",
    "text": "10.3 Ocena hiperparametrów sieci\nAby wybrać najlepszą kombinację parametrów dostrajania, każdy zestaw kandydatów jest oceniany przy użyciu danych, które nie były używane do szkolenia tego modelu. Metody ponownego próbkowania lub pojedynczy zestaw walidacyjny dobrze sprawdzają się w tym celu.\nPo resamplingu, użytkownik wybiera najbardziej odpowiedni zestaw parametrów. Sensowne może być wybranie empirycznie najlepszej kombinacji parametrów lub ukierunkowanie wyboru na inne aspekty dopasowania modelu, takie jak prostota modelu.\nW tym i następnym rozdziale wykorzystujemy zestaw danych klasyfikacyjnych do demonstracji tuningu modelu. Dane pochodzą od Hill i in. (2007), którzy opracowali zautomatyzowane narzędzie laboratoryjne do mikroskopii w badaniach nad nowotworami. Dane składają się z 56 pomiarów obrazowania na 2019 ludzkich komórkach raka piersi. Predyktory te reprezentują cechy kształtu i intensywności różnych części komórek (np. jądro, granica komórki itp.). Istnieje wysoki stopień korelacji między predyktorami. Na przykład, istnieje kilka różnych predyktorów, które mierzą rozmiar i kształt jądra oraz granicę komórki. Wiele predyktorów ma rozkłady skośne.\nKażda komórka należy do jednej z dwóch klas. Ponieważ jest to część zautomatyzowanego testu laboratoryjnego, skupiliśmy się na zdolności przewidywania, a nie wnioskowania.\n\nKodlibrary(tidymodels)\ndata(cells)\ncells &lt;- cells %&gt;% select(-case)\n\n\nBiorąc pod uwagę wymiary danych, możemy obliczyć metryki wydajności przy użyciu 10-krotnej walidacji krzyżowej:\n\nKodset.seed(1304)\ncell_folds &lt;- vfold_cv(cells)\n\n\nZe względu na wysoki stopień korelacji pomiędzy predyktorami, sensowne jest użycie ekstrakcji cech PCA do usunięcia efektu współliniowości predyktorów. Poniższy przepis zawiera kroki przekształcenia predyktorów w celu zwiększenia symetrii, znormalizowania ich, aby były w tej samej skali, a następnie przeprowadzenia ekstrakcji cech. Liczba komponentów PCA, które mają być zachowane, jest również dostrajana wraz z parametrami modelu. Wiele predyktorów ma rozkłady skośne. Ponieważ PCA opiera się na wariancji, wartości ekstremalne mogą mieć szkodliwy wpływ na te obliczenia. Aby temu zapobiec, dodajmy krok przepisu polegający na oszacowaniu transformacji Yeo-Johnsona dla każdego predyktora. Krok step_YeoJohnson() występuje w recepturze tuż przed wstępną normalizacją poprzez step_normalize(). Następnie, połączmy recepturę transformacji cech z naszą specyfikacją modelu sieci neuronowej mlp_spec.\n\nKodmlp_rec &lt;-\n  recipe(class ~ ., data = cells) %&gt;%\n  step_YeoJohnson(all_numeric_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_pca(all_numeric_predictors(), num_comp = tune()) %&gt;% \n  step_normalize(all_numeric_predictors())\n\nmlp_wflow &lt;- \n  workflow() %&gt;% \n  add_model(mlp_spec) %&gt;% \n  add_recipe(mlp_rec)\n\n\nUtwórzmy obiekt parametrów mlp_param, aby dostosować kilka domyślnych zakresów. Możemy zmienić liczbę epok (50 do 200 epok). Również domyślny zakres dla num_comp() jest bardzo wąski (od jednej do czterech składowych); możemy zwiększyć zakres do 40 składowych i ustawić wartość minimalną na zero:\n\nKodmlp_param &lt;- \n  mlp_wflow %&gt;% \n  extract_parameter_set_dials() %&gt;%\n  update(\n    epochs = epochs(c(50, 200)),\n    num_comp = num_comp(c(0, 40))\n  )\n\n\n\n\n\n\n\n\nWskazówka\n\n\n\nW step_pca(), użycie zerowej liczby komponentów PCA oznazca pominięcie ekstrakcji cech. W ten sposób oryginalne predyktory mogą być bezpośrednio porównywane z wynikami, które zawierają komponenty PCA.\n\n\nFunkcja tune_grid() jest podstawową funkcją do przeprowadzania przeszukiwania siatki. Jej funkcjonalność jest bardzo podobna do fit_resamples(), choć posiada dodatkowe argumenty związane z siatką:\n\n\ngrid - liczba całkowita lub ramka danych. Gdy użyta jest liczba całkowita, funkcja tworzy wypełniający przestrzeń wzór z siatką liczby kandydujących kombinacji parametrów. Jeśli istnieją konkretne kombinacje parametrów, parametr grid jest używany do przekazania ich do funkcji.\n\nparam_info - opcjonalny argument służący do definiowania zakresów parametrów. Argument jest najbardziej przydatny, gdy grid jest liczbą całkowitą.\n\nW przeciwnym razie, interfejs tune_grid() jest taki sam jak fit_resamples(). Pierwszym argumentem jest albo specyfikacja modelu, albo przepływ pracy. Gdy podany jest model, drugim argumentem może być receptura lub wzór. Drugim wymaganym argumentem jest obiekt rsample (taki jak np. cell_folds). Poniższe wywołanie przekazuje również zestaw metryk, tak aby obszar pod krzywą ROC był mierzony podczas ponownego próbkowania.\n\nKodroc_res &lt;- metric_set(roc_auc)\nset.seed(1305)\nmlp_reg_tune &lt;-\n  mlp_wflow %&gt;%\n  tune_grid(\n    cell_folds,\n    grid = mlp_param %&gt;% grid_regular(levels = 3),\n    metrics = roc_res\n  )\nmlp_reg_tune\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits             id     .metrics          .notes          \n   &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n 1 &lt;split [1817/202]&gt; Fold01 &lt;tibble [81 × 8]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [1817/202]&gt; Fold02 &lt;tibble [81 × 8]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [1817/202]&gt; Fold03 &lt;tibble [81 × 8]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [1817/202]&gt; Fold04 &lt;tibble [81 × 8]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [1817/202]&gt; Fold05 &lt;tibble [81 × 8]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [1817/202]&gt; Fold06 &lt;tibble [81 × 8]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [1817/202]&gt; Fold07 &lt;tibble [81 × 8]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [1817/202]&gt; Fold08 &lt;tibble [81 × 8]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [1817/202]&gt; Fold09 &lt;tibble [81 × 8]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [1818/201]&gt; Fold10 &lt;tibble [81 × 8]&gt; &lt;tibble [0 × 3]&gt;\n\n\n\n\n\n\n\nKodautoplot(mlp_reg_tune) + \n  scale_color_viridis_d(direction = -1) + \n  theme(legend.position = \"top\")\n\n\n\nRysunek 10.3: Wyniki tuningu na siatce regularnej\n\n\n\n\nDla tych danych, wielkość kary ma największy wpływ na obszar pod krzywą ROC. Liczba epok nie wydaje się mieć wyraźnego wpływu na wydajność. Zmiana liczby ukrytych jednostek wydaje się mieć największe znaczenie, gdy współczynnik regularyzacji (kara) jest niski (i szkodzi dopasowaniu). Istnieje kilka konfiguracji parametrów, które mają z grubsza podobną wydajność, jak widać przy użyciu funkcji show_best():\n\nKodshow_best(mlp_reg_tune) %&gt;% \n  select(-.estimator)\n\n# A tibble: 5 × 9\n  hidden_units penalty epochs num_comp .metric  mean     n std_err .config      \n         &lt;int&gt;   &lt;dbl&gt;  &lt;int&gt;    &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;        \n1            5       1     50        0 roc_auc 0.897    10 0.00857 Preprocessor…\n2           10       1    125        0 roc_auc 0.895    10 0.00898 Preprocessor…\n3           10       1     50        0 roc_auc 0.894    10 0.00960 Preprocessor…\n4            5       1    200        0 roc_auc 0.894    10 0.00784 Preprocessor…\n5            5       1    125        0 roc_auc 0.892    10 0.00822 Preprocessor…\n\n\nBazując na tych wynikach, sensowne byłoby przeprowadzenie kolejnego przebiegu przeszukiwania siatki z większymi wartościami kary. Aby użyć konstrukcji wypełniającej przestrzeń, można podać argument grid jako liczbę całkowitą lub za pomocą jednej z funkcji grid_*() stworzyć ramkę danych. Aby ocenić ten sam zakres przy użyciu planu kwadratu łacińskiego z 20 zestawami parametrów:\n\nKodset.seed(1306)\nmlp_sfd_tune &lt;-\n  mlp_wflow %&gt;%\n  tune_grid(\n    cell_folds,\n    grid = 20,\n    # Pass in the parameter object to use the appropriate range: \n    param_info = mlp_param,\n    metrics = roc_res\n  )\nmlp_sfd_tune\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits             id     .metrics          .notes          \n   &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n 1 &lt;split [1817/202]&gt; Fold01 &lt;tibble [20 × 8]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [1817/202]&gt; Fold02 &lt;tibble [20 × 8]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [1817/202]&gt; Fold03 &lt;tibble [20 × 8]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [1817/202]&gt; Fold04 &lt;tibble [20 × 8]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [1817/202]&gt; Fold05 &lt;tibble [20 × 8]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [1817/202]&gt; Fold06 &lt;tibble [20 × 8]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [1817/202]&gt; Fold07 &lt;tibble [20 × 8]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [1817/202]&gt; Fold08 &lt;tibble [20 × 8]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [1817/202]&gt; Fold09 &lt;tibble [20 × 8]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [1818/201]&gt; Fold10 &lt;tibble [20 × 8]&gt; &lt;tibble [0 × 3]&gt;\n\n\nTym razem funkcja autoplot przedstawia wyniki efektów brzegowych poszczególnych parametrów. Należy zachować ostrożność podczas badania tego wykresu; ponieważ nie jest używana siatka regularna, wartości pozostałych parametrów dostrajania mogą wpływać na każdy panel.\n\nKodautoplot(mlp_sfd_tune)\n\n\n\nRysunek 10.4: Wynik tuningu z zastosowaniem planu kwadratu łacińskiego\n\n\n\n\nParametr kary wydaje się skutkować lepszą wydajnością przy mniejszych wartościach. Jest to sprzeczne z wynikami z regularnej siatki. Ponieważ każdy punkt w każdym panelu jest współdzielony z pozostałymi trzema parametrami dostrajania, na trendy w jednym panelu mogą wpływać pozostałe. Przy użyciu siatki regularnej każdy punkt w każdym panelu jest uśredniony względem pozostałych parametrów. Z tego powodu efekt każdego parametru jest lepiej izolowany przy użyciu regularnych siatek.\n\nKodshow_best(mlp_sfd_tune) %&gt;% \n  select(-.estimator)\n\n# A tibble: 5 × 9\n  hidden_units       penalty epochs num_comp .metric  mean     n std_err .config\n         &lt;int&gt;         &lt;dbl&gt;  &lt;int&gt;    &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n1            8 0.594             97       22 roc_auc 0.880    10 0.00998 Prepro…\n2            3 0.00000000649    135        8 roc_auc 0.878    10 0.00957 Prepro…\n3            9 0.141            177       11 roc_auc 0.873    10 0.0104  Prepro…\n4            8 0.0000000103      74        9 roc_auc 0.869    10 0.00761 Prepro…\n5            6 0.00581          129       15 roc_auc 0.865    10 0.00659 Prepro…\n\n\nJak to sygnalizowaliśmy wcześniej, dobrym pomysłem jest ocena modeli za pomocą wielu metryk, tak aby uwzględnione zostały różne aspekty dopasowania modelu. Ponadto, często sensowne jest wybranie nieco suboptymalnej kombinacji parametrów, która jest związana z prostszym modelem. W przypadku tego modelu prostota odpowiada większym wartościom kar i/lub mniejszej liczbie neuronów w warstwie ukrytej."
  },
  {
    "objectID": "grid_search.html#finalizowanie-modelu",
    "href": "grid_search.html#finalizowanie-modelu",
    "title": "\n10  Dostrajanie z przeszukiwaniem siatki\n",
    "section": "\n10.4 Finalizowanie modelu",
    "text": "10.4 Finalizowanie modelu\n\n\n\n\nJeśli jeden z zestawów możliwych parametrów modelu znalezionych poprzez show_best() byłby atrakcyjną opcją końcową dla tych danych, moglibyśmy chcieć ocenić jak dobrze radzi sobie na zestawie testowym. Jednakże, wyniki funkcji tune_grid() dostarczają jedynie podłoża do wyboru odpowiednich parametrów. Funkcja ta nie dopasowuje modelu końcowego. Aby dopasować model końcowy, należy określić ostateczny zestaw wartości parametrów. Istnieją dwie metody, aby to zrobić:\n\nręcznie wybrać wartości, które wydają się odpowiednie,\nużyć funkcji select_*().\n\n\nNa przykład select_best() wybierze parametry o numerycznie najlepszych wynikach dopasowania. Wróćmy do naszych zwykłych wyników siatki i zobaczmy, który z nich jest najlepszy:\n\nKodselect_best(mlp_reg_tune, metric = \"roc_auc\")\n\n# A tibble: 1 × 5\n  hidden_units penalty epochs num_comp .config              \n         &lt;int&gt;   &lt;dbl&gt;  &lt;int&gt;    &lt;int&gt; &lt;chr&gt;                \n1            5       1     50        0 Preprocessor1_Model08\n\n\nPatrząc na Rysunek 10.3, widzimy, że model z pojedynczym neuronem w warstwie ukrytej trenowany przez 125 epok na oryginalnych predyktorach z dużą wartością kary ma wydajność konkurencyjną do modelu otrzymanego z zagęszczeniem wartości parametrów, a jest prostszy. Jest to w zasadzie regularyzowana regresja logistyczna! Aby ręcznie określić te parametry, możemy utworzyć tibble z tymi wartościami, a następnie użyć funkcji finalizacji, aby spleść wartości z powrotem do przepływu pracy:\n\nKodlogistic_param &lt;- \n  tibble(\n    num_comp = 0,\n    epochs = 125,\n    hidden_units = 1,\n    penalty = 1\n  )\n\nfinal_mlp_wflow &lt;- \n  mlp_wflow %&gt;% \n  finalize_workflow(logistic_param)\nfinal_mlp_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: mlp()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_YeoJohnson()\n• step_normalize()\n• step_pca()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nSingle Layer Neural Network Model Specification (classification)\n\nMain Arguments:\n  hidden_units = 1\n  penalty = 1\n  epochs = 125\n\nEngine-Specific Arguments:\n  trace = 0\n\nComputational engine: nnet \n\n\nŻadne inne wartości funkcji tune() nie są uwzględniane w sfinalizowanym procesie pracy. Teraz model może być dopasowany do całego zestawu treningowego:\n\nKodfinal_mlp_fit &lt;- \n  final_mlp_wflow %&gt;% \n  fit(cells)\n\n\nObiekt ten może być teraz użyty do przyszłych predykcji na nowych danych.\nJeśli nie użyłeś przepływu pracy, finalizacja modelu i/lub receptury odbywa się za pomocą finalize_model() i finalize_recipe().\nAby automatycznie stworzyć specyfikację modelu do tuningu, wystarczy wykorzystać odpowiednią funkcję pakietu usemodels.\n\nKodset.seed(1001)\names &lt;- ames |&gt;mutate(Sale_Price = log10(Sale_Price))\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\nlibrary(usemodels)\n\nuse_xgboost(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n              Latitude + Longitude, \n            data = ames_train,\n            # Add comments explaining some of the code:\n            verbose = TRUE)\n\nxgboost_recipe &lt;- \n  recipe(formula = Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n    Latitude + Longitude, data = ames_train) %&gt;% \n  step_zv(all_predictors()) \n\nxgboost_spec &lt;- \n  boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(), \n    loss_reduction = tune(), sample_size = tune()) %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"xgboost\") \n\nxgboost_workflow &lt;- \n  workflow() %&gt;% \n  add_recipe(xgboost_recipe) %&gt;% \n  add_model(xgboost_spec) \n\nset.seed(76045)\nxgboost_tune &lt;-\n  tune_grid(xgboost_workflow, resamples = stop(\"add your rsample object\"), grid = stop(\"add number of candidate points\"))\n\n\n\n\n\n\nBooth, Gordon D., George E. P. Box, William G. Hunter, i J. Stuart Hunter. 1979. „Statistics for Experimenters: An Introduction to Design, Data Analysis, and Model Building.” Journal of the American Statistical Association 74 (367): 731. https://doi.org/10.2307/2287009.\n\n\nHill, Andrew A, Peter LaPan, Yizheng Li, i Steve Haney. 2007. „Impact of Image Segmentation on High-Content Screening Data Quality for SK-BR-3 Cells”. BMC Bioinformatics 8 (1). https://doi.org/10.1186/1471-2105-8-340.\n\n\nJoseph, V. Roshan, Evren Gul, i Shan Ba. 2015. „Maximum Projection Designs for Computer Experiments”. Biometrika 102 (2): 371–80. https://doi.org/10.1093/biomet/asv002.\n\n\nMcKay, M. D., R. J. Beckman, i W. J. Conover. 1979. „A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code”. Technometrics 21 (2): 239. https://doi.org/10.2307/1268522.\n\n\nShewry, M. C., i H. P. Wynn. 1987. „Maximum Entropy Sampling”. Journal of Applied Statistics 14 (2): 165–70. https://doi.org/10.1080/02664768700000020."
  },
  {
    "objectID": "parallel.html#dostęp-do-zmiennych-globalnych",
    "href": "parallel.html#dostęp-do-zmiennych-globalnych",
    "title": "\n11  Paralelizacja resamplingu\n",
    "section": "\n11.1 Dostęp do zmiennych globalnych",
    "text": "11.1 Dostęp do zmiennych globalnych\nJeśli definiujemy zmienną, która ma być użyta jako parametr modelu, a następnie przekazujemy ją do funkcji takiej jak linear_reg(), zmienna ta jest zazwyczaj zdefiniowana w środowisku globalnym.\n\nKodlibrary(tidymodels)\ntidymodels_prefer()\n\ncoef_penalty &lt;- 0.1\nspec &lt;- linear_reg(penalty = coef_penalty) %&gt;% set_engine(\"glmnet\")\n\n\nModele utworzone za pomocą pakietu parsnip zapisują argumenty takie jak te jako quosures; są to obiekty śledzące zarówno nazwę obiektu, jak i środowisko, w którym istnieje:\n\nKodspec$args$penalty\n\n&lt;quosure&gt;\nexpr: ^coef_penalty\nenv:  global\n\n\nZauważ, że mamy env: global, ponieważ ta zmienna została utworzona w środowisku globalnym. Specyfikacja modelu zdefiniowana przez spec działa poprawnie, gdy jest uruchamiana w zwykłej sesji użytkownika, ponieważ sesja ta również korzysta ze środowiska globalnego; R może łatwo znaleźć obiekt coef_penalty. Kiedy taki model jest szacowany z równoległymi rdzeniami, może zawieść. W zależności od konkretnej technologii, która jest używana do przetwarzania równoległego, rdzenie mogą nie mieć dostępu do globalnego środowiska. Podczas pisania kodu, który będzie uruchamiany równolegle, dobrym pomysłem jest wstawianie rzeczywistych danych do obiektów, a nie referencji do obiektu. Pakiety rlang i dplyr mogą być w tym bardzo pomocne. Na przykład, operator !! może wstawić pojedynczą wartość do obiektu:\n\nKodspec &lt;- linear_reg(penalty = !!coef_penalty) %&gt;% set_engine(\"glmnet\")\nspec$args$penalty\n\n&lt;quosure&gt;\nexpr: ^0.1\nenv:  empty\n\n\nTeraz wyjście to ^0,1, wskazując, że jest to wartość zamiast odniesienia do obiektu. Kiedy masz wiele zewnętrznych wartości do wstawienia do obiektu, operator !!! może pomóc:\n\nKodmcmc_args &lt;- list(chains = 3, iter = 1000, cores = 3)\n\nlinear_reg() %&gt;% set_engine(\"stan\", !!!mcmc_args)\n\nLinear Regression Model Specification (regression)\n\nEngine-Specific Arguments:\n  chains = 3\n  iter = 1000\n  cores = 3\n\nComputational engine: stan \n\n\nSelektory receptur to kolejne miejsce, w którym możesz chcieć uzyskać dostęp do zmiennych globalnych. Załóżmy, że masz krok receptury, który powinien użyć wszystkich predyktorów w danych komórkowych (cells), które zostały zmierzone za pomocą drugiego kanału optycznego. Możemy utworzyć wektor tych nazw kolumn:\n\nKodlibrary(stringr)\nch_2_vars &lt;- str_subset(names(cells), \"ch_2\")\nch_2_vars\n\n[1] \"avg_inten_ch_2\"   \"total_inten_ch_2\"\n\n\nMoglibyśmy twardo zakodować je w kroku receptury, ale lepiej byłoby odwołać się do nich programowo na wypadek zmiany danych. Mamy dwa sposoby, aby to zrobić to:\n\nKod# Still uses a reference to global data (~_~;)\nrecipe(class ~ ., data = cells) %&gt;% \n  step_spatialsign(all_of(ch_2_vars))\n\n# Inserts the values into the step ヽ(•‿•)ノ\nrecipe(class ~ ., data = cells) %&gt;% \n  step_spatialsign(!!!ch_2_vars)\n\n\nTen ostatni jest lepszy dla przetwarzania równoległego, ponieważ wszystkie potrzebne informacje są osadzone w obiekcie receptury."
  },
  {
    "objectID": "parallel.html#metoda-wyścigów",
    "href": "parallel.html#metoda-wyścigów",
    "title": "\n11  Paralelizacja resamplingu\n",
    "section": "\n11.2 Metoda wyścigów",
    "text": "11.2 Metoda wyścigów\nJednym z problemów z przeszukiwaniem siatki jest to, że wszystkie modele muszą być dopasowane we wszystkich foldach, zanim jakiekolwiek dostrajanie parametrów może być ocenione. Byłoby pomocne, gdyby zamiast tego, w pewnym momencie podczas dostrajania parametrów, można było przeprowadzić analizę przejściową w celu wyeliminowania wszelkich naprawdę bezużytecznych kandydatów na parametry. Byłoby to podobne do analizy daremności w badaniach klinicznych. Jeśli nowy lek działa zbyt słabo (lub dobrze), to potencjalnie nieetyczne jest czekanie na zakończenie badania, by podjąć decyzję.\n\n\n\n\nW uczeniu maszynowym podobną funkcję zapewnia zbiór technik zwanych metodami wyścigowymi (ang. racing method) (Maron i Moore 1993). W tym przypadku proces strojenia ocenia wszystkie modele na początkowym podzbiorze foldów. W oparciu o wartości metryk wydajności, niektóre zestawy parametrów nie są brane pod uwagę w kolejnych etapach dostrajania.\nNa przykład, w procesie tuningu perceptronu wielowarstwowego z siatką regularną, badamy jak wyglądałyby wyniki tylko po pierwszych trzech foldach? Używając technik podobnych do tych z rozdziału o porównywaniu modeli, możemy dopasować model, w którym zmienną wynikową jest obszar pod krzywą ROC, a predyktorem jest wskaźnik dla kombinacji parametrów. Model uwzględnia efekt resample-to-resample i tworzy oszacowania punktowe i przedziałowe dla każdej kombinacji parametrów. Wynikiem modelu są jednostronne 95% przedziały ufności, które mierzą stratę wartości ROC w stosunku do aktualnie najlepiej działających parametrów, jak pokazano na rysunku 13.9.\n\n\nRysunek 11.5: Przykład zastosowania metody wyścigowej\n\n\nKażdy zestaw parametrów, którego przedział ufności zawiera zero, nie daje podstaw by twierdzić, że jego wydajność jest statystycznie różna od najlepszych wyników. Zachowujemy 6 najlepszych ustawień i są one ponownie próbkowane. Pozostałe 14 podmodeli nie jest już rozpatrywane.\n\nMetody wyścigowe mogą być bardziej efektywne niż podstawowe przeszukiwanie siatki, o ile analiza pośrednia jest szybka, a niektóre ustawienia parametrów mają słabą wydajność. Jest to również najbardziej pomocne, gdy model nie ma możliwości wykorzystania predykcji submodelu.\nPakiet finetune zawiera funkcje dla metody wyścigowej. Funkcja tune_race_anova() przeprowadza model ANOVA w celu przetestowania statystycznej istotności różnych konfiguracji modelu. Składnia pozwalająca odtworzyć pokazane wcześniej filtrowanie to:\n\nKodlibrary(finetune)\n\nmlp_spec &lt;- \n  mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %&gt;% \n  set_engine(\"nnet\", trace = 0) %&gt;% \n  set_mode(\"classification\")\n\nmlp_param &lt;- extract_parameter_set_dials(mlp_spec)\n\ndata(cells)\ncells &lt;- cells %&gt;% select(-case)\n\nset.seed(1304)\ncell_folds &lt;- vfold_cv(cells)\n\nmlp_rec &lt;-\n  recipe(class ~ ., data = cells) %&gt;%\n  step_YeoJohnson(all_numeric_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_pca(all_numeric_predictors(), num_comp = tune()) %&gt;% \n  step_normalize(all_numeric_predictors())\n\nmlp_wflow &lt;- \n  workflow() %&gt;% \n  add_model(mlp_spec) %&gt;% \n  add_recipe(mlp_rec)\n\nmlp_param &lt;- \n  mlp_wflow %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  update(\n    epochs = epochs(c(50, 200)),\n    num_comp = num_comp(c(0, 40))\n  )\n\nroc_res &lt;- metric_set(roc_auc)\n\nset.seed(1308)\nmlp_sfd_race &lt;-\n  mlp_wflow %&gt;%\n  tune_race_anova(\n    cell_folds,\n    grid = 20,\n    param_info = mlp_param,\n    metrics = roc_res,\n    control = control_race(verbose_elim = TRUE)\n  )\n\n\nArgumenty są odzwierciedleniem argumentów funkcji tune_grid(). Funkcja control_race() posiada opcje dotyczące procedury eliminacji. Jak pokazano na powyższej animacji, rozważane były dwie kombinacje parametrów po ocenie pełnego zestawu próbek. show_best() zwraca najlepsze modele (uszeregowane według wydajności), ale zwraca tylko konfiguracje, które nigdy nie zostały wyeliminowane:\n\nKodshow_best(mlp_sfd_race, n = 10)\n\n# A tibble: 10 × 10\n   hidden_units  penalty epochs num_comp .metric .estimator  mean     n std_err\n          &lt;int&gt;    &lt;dbl&gt;  &lt;int&gt;    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1            4 1.26e- 3    112        9 roc_auc binary     0.893     4 0.0102 \n 2            8 8.14e- 1    177       15 roc_auc binary     0.890    10 0.00966\n 3            5 1.30e-10     89        5 roc_auc binary     0.889     5 0.0106 \n 4            1 7.08e- 9    106       24 roc_auc binary     0.888     3 0.00379\n 5            3 1.23e- 1     55       36 roc_auc binary     0.888     3 0.00662\n 6            5 4.91e- 7    132       12 roc_auc binary     0.883     4 0.0103 \n 7            2 7.91e- 4    164        7 roc_auc binary     0.883     5 0.0123 \n 8            7 6.53e- 8     69       18 roc_auc binary     0.883     3 0.00932\n 9            2 7.99e- 3    161       32 roc_auc binary     0.882     3 0.0125 \n10            3 4.02e- 2    151       10 roc_auc binary     0.881     7 0.0154 \n# ℹ 1 more variable: .config &lt;chr&gt;\n\n\nIstnieją również inne techniki analizy pośredniej do odrzucania pewnych konfiguracji parametrów.\n\n\n\n\nMaron, Oded, i Andrew Moore. 1993. „Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation”. W Advances in Neural Information Processing Systems, zredagowane przez J. Cowan, G. Tesauro, i J. Alspector. T. 6. Morgan-Kaufmann."
  },
  {
    "objectID": "iterative.html#optymalizacja-bayesowska",
    "href": "iterative.html#optymalizacja-bayesowska",
    "title": "\n12  Przeszukiwanie iteracyjne\n",
    "section": "\n12.1 Optymalizacja bayesowska",
    "text": "12.1 Optymalizacja bayesowska\nTechniki optymalizacji bayesowskiej analizują bieżące wyniki próbkowania i tworzą model predykcyjny, aby zasugerować wartości parametrów dostrajania, które nie zostały jeszcze ocenione. Sugerowana kombinacja parametrów jest następnie ponownie próbkowana. Wyniki te są następnie wykorzystywane w innym modelu predykcyjnym, który rekomenduje więcej wartości kandydatów do testowania, i tak dalej. Proces ten przebiega przez ustaloną liczbę iteracji lub do momentu, gdy nie pojawią się dalsze poprawy. Shahriari i in. (2016) i Frazier (2018) są dobrymi wprowadzeniami do optymalizacji bayesowskiej.\n\n\n\n\nPodczas korzystania z optymalizacji bayesowskiej, podstawowe problemy to sposób tworzenia modelu i wybór parametrów rekomendowanych przez ten model. Najpierw rozważmy technikę najczęściej stosowaną w optymalizacji bayesowskiej, czyli model procesu gaussowskiego.\n\n12.1.1 Model procesu gaussowskiego\nModele procesu gaussowskiego (GP) (Schulz, Speekenbrink, i Krause 2016), to techniki statystyczne, które mają swoją historię w statystyce przestrzennej (pod nazwą metod krigingu). Mogą być wyprowadzone na wiele sposobów, w tym jako model Bayesowski.\nMatematycznie, GP jest zbiorem zmiennych losowych, których wspólny rozkład prawdopodobieństwa jest wielowymiarowy normalny. W kontekście naszych zastosowań jest to zbiór metryk wydajności dla wartości kandydujących parametrów dostrajania. Dla początkowej siatki czterech próbek, realizacje tych czterech zmiennych losowych wynosiły 0.8639, 0.8625, 0.8627 i 0.8659. Zakłada się, że mają rozkład wielowymiarowy normalny. Wejściami definiującymi zmienne niezależne dla modelu GP są odpowiednie wartości dostrajania parametrów (przedstawione w Tabela 12.1).\n\n\n\n\n\n\n\nTabela 12.1:  Wartości startowe w procedurze poszukiwania optymalnych\nparametrów \n  \nROC\n      cost\n      rbf_sigma\n    \n\n\n0.8639\n0.01562\n1e-06\n\n\n0.8625\n2.00000\n1e-06\n\n\n0.8627\n0.01562\n1e-04\n\n\n0.8659\n2.00000\n1e-04\n\n\n\n\n\n\n\nModele procesów gaussowskich są określone przez ich funkcje średniej i kowariancji, choć to ta ostatnia ma większy wpływ na charakter modelu GP. Funkcja kowariancji jest często parametryzowana w kategoriach wartości wejściowych (oznaczanych jako \\(x\\)). Przykładowo, powszechnie stosowaną funkcją kowariancji jest funkcja wykładnicza kwadratowa:\n\\[\n\\operatorname{cov}(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\exp\\left(-\\frac{1}{2}|\\boldsymbol{x}_i - \\boldsymbol{x}_j|^2\\right) + \\sigma^2_{ij}\n\\tag{12.1}\\]\ngdzie \\(\\sigma_{i,j}^2\\) jest wariancją błędu modelu równą zero jeśli \\(i=j\\). Możemy to interpretować jako, że wraz ze wzrostem odległości pomiędzy dwoma kombinacjami parametrów, kowariancja pomiędzy metrykami wydajności rośnie wykładniczo. Z równania wynika również, że zmienność metryki wynikowej jest minimalizowana w punktach, które już zostały zaobserwowane (tzn. gdy \\(|x_i - x_j|^2\\) wynosi zero). Charakter tej funkcji kowariancji pozwala procesowi gaussowskiemu reprezentować wysoce nieliniowe zależności między wydajnością modelu a dostrajaniem parametrów, nawet jeśli istnieje tylko niewielka ilość danych.\nWażną zaletą tego modelu jest to, że ponieważ określony jest pełny model prawdopodobieństwa, przewidywania dla nowych wejść mogą odzwierciedlać cały rozkład wyniku. Innymi słowy, nowe statystyki wydajności mogą być przewidywane zarówno pod względem średniej jak i wariancji.\nNa podstawie początkowej siatki czterech wyników, model GP jest dopasowywany, następnie są obliczane z modelu predykcje dla kandydatów, a piąta kombinacja parametrów dostrajania jest wybierana. Obliczamy szacunkową wydajność dla nowej konfiguracji, GP jest ponownie dopasowywany do pięciu istniejących wyników (i tak dalej).\n\n12.1.2 Funkcja akwizycji\nJak wykorzystać model procesu gaussowskiego po dopasowaniu do aktualnych danych? Naszym celem jest wybranie następnej kombinacji dostrajania parametrów, która najprawdopodobniej da “lepsze wyniki” niż obecna najlepsza. Jednym z podejść do tego jest stworzenie dużego zbioru kandydatów, a następnie wykonanie prognoz średniej i wariancji dla każdego z nich. Korzystając z tych informacji, wybieramy najkorzystniejszą wartość parametru dostrajania.\n\n\n\n\nKlasa funkcji celu, zwanych funkcjami akwizycji, ułatwia kompromis pomiędzy średnią a wariancją. Przypomnijmy, że przewidywana wariancja modeli GP zależy głównie od tego, jak bardzo są one oddalone od istniejących danych. Kompromis pomiędzy przewidywaną średnią i wariancją dla nowych kandydatów jest często postrzegany przez pryzmat eksploracji i eksploatacji:\n\nEksploracja (ang. exploration) - powoduje wybór tych regionów, w których jest mniej obserwowanych modeli kandydujących. W ten sposób nadaje się większą wagę kandydatom o wyższej wariancji i koncentruje się na poszukiwaniu nowych wyników.\nEksploatacja (ang. exploitation) - zasadniczo opiera się istniejących wynikach, w celu odnalezienia najlepszej wartości średniej.\n\nAby zademonstrować, spójrzmy na przykład z jednym parametrem, który ma wartości pomiędzy [0, 1], a metryką wydajności jest \\(R^2\\). Prawdziwa funkcja jest pokazana na Rysunek 12.2 wraz z pięcioma wartościami kandydującymi, które mają istniejące wyniki jako punkty.\n\n\nRysunek 12.2: Hipotetyczny rzeczywisty profil wydajności dla arbitralnie wybranego parametru dostrajania, z pięcioma szacowanymi punktami\n\n\nDla tych danych dopasowanie modelu GP przedstawiono na Rysunek 12.3. Zacieniowany obszar wskazuje średnią \\(\\pm\\) 1 błąd standardowy. Dwie pionowe linie wskazują dwa punkty kandydujące, które są później bardziej szczegółowo badane. Zacieniowany obszar ufności pokazuje funkcję wykładniczej wariancji kwadratowej; staje się ona bardzo duża między punktami i zbiega do zera w istniejących punktach danych.\n\n\nRysunek 12.3: Przykładowy przebieg procesu gaussowskiego z zaznaczoną funkcją średniej i wariancji\n\n\nTa nieliniowa funkcja przechodzi przez każdy obserwowany punkt, ale model nie jest doskonały. Nie ma obserwowanych punktów w pobliżu prawdziwego optimum ustawienia, a w tym regionie dopasowanie mogłoby być znacznie lepsze. Pomimo tego, model GP może skutecznie wskazać nam właściwy kierunek.\nZ punktu widzenia czystej eksploatacji, najlepszym wyborem byłoby wybranie wartości parametru, który ma najlepszą średnią predykcję. W tym przypadku byłaby to wartość 0,106, tuż na prawo od istniejącego najlepszego zaobserwowanego punktu na poziomie 0,09.\nJako sposób na zachęcenie do eksploracji, prostym (ale nie często stosowanym) podejściem jest znalezienie parametru dostrajania związanego z największym przedziałem ufności.\nJedną z najczęściej stosowanych funkcji akwizycji jest oczekiwana poprawa. Pojęcie poprawy wymaga wartości dla bieżących najlepszych wyników (w przeciwieństwie do podejścia opartego na przedziałach ufności). Ponieważ GP może opisać nowy punkt kandydacki za pomocą rozkładu, możemy ważyć fragmenty rozkładu, które wykazują poprawę, używając prawdopodobieństwa wystąpienia poprawy.\nNa przykład, rozważmy dwie wartości parametrów kandydujących 0,10 i 0,25 (wskazane przez pionowe linie na Rysunek 12.3). Używając dopasowanego modelu GP, ich przewidywane \\(R^2\\) są pokazane na rysunku 14.4 wraz z linią odniesienia dla aktualnych najlepszych wyników.\n\n\nRysunek 12.4: Przewidywane rozkłady wydajności dla dwóch próbkowanych wartości parametrów dostrajania\n\n\nRozpatrując tylko średnią \\(R^2\\) lepszym wyborem jest wartość parametru 0,10 (patrz Tabela 12.1). Rekomendacja parametru dostrajania dla 0,25 ma gorsze przewidywanie średnie niż aktualny najlepszy kandydat. Jednakże, ponieważ ma wyższą wariancję, ma większy ogólny obszar prawdopodobieństwa powyżej aktualnego najlepszego. W rezultacie ma większą oczekiwaną poprawę:\n\nKodtab2 &lt;- tibble::tribble(\n  ~Parameter.Value,  ~Mean,  ~Std.Dev, ~Expected.Improvment,\n               0.1, 0.8679, 0.0004317,              0.00019,\n              0.25, 0.8671, 0.0039301,             0.001216\n  )\ngt(tab2)\n\n\n\n\n\n\nTabela 12.2:  Oczekiwana poprawa dla dwóch kandydujących parametrów\ndostrajania. \n  \nParameter.Value\n      Mean\n      Std.Dev\n      Expected.Improvment\n    \n\n\n0.10\n0.8679\n0.0004317\n0.000190\n\n\n0.25\n0.8671\n0.0039301\n0.001216\n\n\n\n\n\n\n\nKiedy oczekiwana poprawa jest obliczana w całym zakresie dostrajania parametrów, zalecany punkt do próbkowania jest znacznie bliższy 0,25 niż 0,10, jak pokazano na rysunku 14.5.\n\n\nRysunek 12.5: Szacowany profil wydajności wygenerowany przez model procesu gaussowskiego (górny panel) oraz oczekiwana poprawa (dolny panel). Pionowa linia wskazuje punkt maksymalnej poprawy.\n\n\nAby zaimplementować wyszukiwanie iteracyjne poprzez optymalizację bayesowską, należy użyć funkcji tune_bayes(). Jej składnia jest bardzo podobna do tune_grid(), z kilkoma dodatkowymi argumentami:\n\n\niter to maksymalna liczba iteracji wyszukiwania.\n\ninitial może być liczbą całkowitą, obiektem utworzonym przy pomocy tune_grid(), albo jedną z funkcji wyścigowych. Użycie liczby całkowitej określa rozmiar konstrukcji wypełniającej przestrzeń, która jest próbkowana przed pierwszym modelem GP.\n\nobjective jest argumentem, dla którego należy użyć funkcji akwizycji. Pakiet tune zawiera funkcje takie jak exp_improve() lub conf_bound().\nArgument param_info, w tym przypadku określa zakres parametrów, jak również wszelkie transformacje.\n\nArgument control funkcji tune_bayes() ustawia się za pomocą control_bayes(). Niektóre istotne argumenty to:\n\n\nno_improve to liczba całkowita, która zatrzyma wyszukiwanie, jeśli ulepszone parametry nie zostaną odkryte w ciągu iteracji no_improve.\n\nuncertain jest również liczbą całkowitą (lub Inf), używaną do ustalenia liczby przejść algorytmu wg reguły eksploatacji bez poprawy, aby następnie wybrać próbę z zakresu z wysoką wariancją, po to żeby dokonać eksploracji.\n\nverbose jest parametrem, który decyduje co będzie się wyświetlało podczas przebiegu algorytmu.\n\nUżyjmy pierwszych wyników SVM jako początkowego podłoża dla modelu GP. Przypomnijmy, że w tym zastosowaniu chcemy zmaksymalizować obszar pod krzywą ROC.\n\nKodctrl &lt;- control_bayes(verbose = TRUE)\n\nset.seed(1403)\nsvm_bo &lt;-\n  svm_wflow %&gt;%\n  tune_bayes(\n    resamples = cell_folds,\n    metrics = roc_res,\n    initial = svm_initial,\n    param_info = svm_param,\n    iter = 25,\n    control = ctrl\n  )\n\n\n\nKodcollect_metrics(svm_bo) |&gt; \n  gt()\n\n\n\n\n\n\nTabela 12.3:  Przebieg optymalizacji baysowskiej \n  \ncost\n      rbf_sigma\n      .metric\n      .estimator\n      mean\n      n\n      std_err\n      .config\n      .iter\n    \n\n\n1.562500e-02\n1.000000e-06\nroc_auc\nbinary\n0.8638724\n10\n0.008637894\nPreprocessor1_Model1\n0\n\n\n2.000000e+00\n1.000000e-06\nroc_auc\nbinary\n0.8625326\n10\n0.008672545\nPreprocessor1_Model2\n0\n\n\n1.562500e-02\n1.000000e-04\nroc_auc\nbinary\n0.8627495\n10\n0.008624554\nPreprocessor1_Model3\n0\n\n\n2.000000e+00\n1.000000e-04\nroc_auc\nbinary\n0.8659439\n10\n0.008545691\nPreprocessor1_Model4\n0\n\n\n8.381539e+00\n5.408960e-07\nroc_auc\nbinary\n0.8625673\n10\n0.008627089\nIter1\n1\n\n\n5.608902e+00\n1.613260e-04\nroc_auc\nbinary\n0.8728182\n10\n0.008236644\nIter2\n2\n\n\n1.600639e+01\n1.983450e-04\nroc_auc\nbinary\n0.8781692\n10\n0.008684509\nIter3\n3\n\n\n2.541191e+01\n2.585081e-04\nroc_auc\nbinary\n0.8832282\n9\n0.009833644\nIter4\n4\n\n\n2.912115e+01\n3.961177e-04\nroc_auc\nbinary\n0.8894476\n10\n0.008674057\nIter5\n5\n\n\n2.282106e+01\n7.094282e-04\nroc_auc\nbinary\n0.8933213\n10\n0.008618012\nIter6\n6\n\n\n3.049977e+01\n1.166976e-03\nroc_auc\nbinary\n0.8978111\n10\n0.008046705\nIter7\n7\n\n\n3.155839e+01\n2.888618e-03\nroc_auc\nbinary\n0.8925884\n9\n0.008601545\nIter8\n8\n\n\n1.312493e-03\n9.584302e-02\nroc_auc\nbinary\n0.8762837\n10\n0.008684225\nIter9\n9\n\n\n2.908667e+01\n1.420931e-03\nroc_auc\nbinary\n0.8979937\n10\n0.007875059\nIter10\n10\n\n\n1.058781e-03\n2.364654e-03\nroc_auc\nbinary\n0.8656167\n10\n0.008754313\nIter11\n11\n\n\n3.154910e+01\n1.210898e-03\nroc_auc\nbinary\n0.8978275\n10\n0.007994954\nIter12\n12\n\n\n3.073241e+01\n1.451115e-03\nroc_auc\nbinary\n0.8982018\n10\n0.007819465\nIter13\n13\n\n\n3.033878e+01\n9.917137e-02\nroc_auc\nbinary\n0.8770458\n10\n0.008412071\nIter14\n14\n\n\n1.129147e-03\n1.026267e-07\nroc_auc\nbinary\n0.8641243\n10\n0.008666403\nIter15\n15\n\n\n3.111979e+01\n1.698500e-03\nroc_auc\nbinary\n0.8986102\n10\n0.007828958\nIter16\n16\n\n\n1.128109e-03\n9.256031e-06\nroc_auc\nbinary\n0.8639301\n10\n0.008622849\nIter17\n17\n\n\n9.925669e-04\n2.237736e-02\nroc_auc\nbinary\n0.8744407\n10\n0.008990285\nIter18\n18\n\n\n3.133523e+01\n1.896086e-03\nroc_auc\nbinary\n0.8984786\n10\n0.007870489\nIter19\n19\n\n\n3.064022e+01\n1.581975e-03\nroc_auc\nbinary\n0.8985440\n10\n0.007808150\nIter20\n20\n\n\n1.480665e+01\n1.821641e-03\nroc_auc\nbinary\n0.8953765\n9\n0.008709148\nIter21\n21\n\n\n3.184368e+01\n1.715154e-03\nroc_auc\nbinary\n0.8987420\n10\n0.007764247\nIter22\n22\n\n\n2.927864e+01\n8.793267e-06\nroc_auc\nbinary\n0.8674531\n10\n0.008316968\nIter23\n23\n\n\n2.878067e+01\n2.244728e-02\nroc_auc\nbinary\n0.8544690\n10\n0.009459848\nIter24\n24\n\n\n1.456602e+01\n1.021120e-07\nroc_auc\nbinary\n0.8623536\n10\n0.008611959\nIter25\n25\n\n\n\n\n\n\n\n\nKodshow_best(svm_bo) |&gt; \n  gt()\n\n\n\n\n\n\nTabela 12.4:  Zestaw optymalnych parametrów na podstawie OB \n  \ncost\n      rbf_sigma\n      .metric\n      .estimator\n      mean\n      n\n      std_err\n      .config\n      .iter\n    \n\n\n31.84368\n0.001715154\nroc_auc\nbinary\n0.8987420\n10\n0.007764247\nIter22\n22\n\n\n31.11979\n0.001698500\nroc_auc\nbinary\n0.8986102\n10\n0.007828958\nIter16\n16\n\n\n30.64022\n0.001581975\nroc_auc\nbinary\n0.8985440\n10\n0.007808150\nIter20\n20\n\n\n31.33523\n0.001896086\nroc_auc\nbinary\n0.8984786\n10\n0.007870489\nIter19\n19\n\n\n30.73241\n0.001451115\nroc_auc\nbinary\n0.8982018\n10\n0.007819465\nIter13\n13\n\n\n\n\n\n\n\n\nKodautoplot(svm_bo, type = \"performance\")\n\n\n\nRysunek 12.6: Jakość dopasowania modelu w poszczególnych iteracjach OB\n\n\n\n\nPoniższa animacja wizualizuje wyniki wyszukiwania. Czarne “x” pokazują wartości początkowe zawarte w svm_initial. Niebieski panel u góry po lewej stronie pokazuje przewidywaną średnią wartość obszaru pod krzywą ROC. Czerwony panel na górze po prawej stronie pokazuje przewidywaną zmienność wartości ROC, podczas gdy dolny wykres wizualizuje oczekiwaną poprawę. W każdym panelu ciemniejsze kolory wskazują mniej atrakcyjne wartości (np. małe wartości średnie, duże zróżnicowanie i małe ulepszenia).\n\nVideo\nRysunek 12.7: Przykład działania optymalizacji bayesowskiej\n\n\nPowierzchnia przewidywanej średniej jest bardzo niedokładna w pierwszych kilku iteracjach wyszukiwania. Pomimo tego, pomaga ona poprowadzić proces w rejon dobrej wydajności. W ciągu pierwszych dziesięciu iteracji, wyszukiwanie jest dokonywane w pobliżu optymalnego miejsca.\nPodczas gdy najlepsza kombinacja dostrajania parametrów znajduje się na granicy przestrzeni parametrów, optymalizacja bayesowska często wybierze nowe punkty poza granicami przestrzeni parametrów.\nPowyższy przykład startował z punktów startowych wybranych nieco arbitralnie ale nieco lepsze wyniki można osiągnąć stosując losowe wypełnienie przestrzeni parametrów."
  },
  {
    "objectID": "iterative.html#symulowane-wyżarzanie",
    "href": "iterative.html#symulowane-wyżarzanie",
    "title": "\n12  Przeszukiwanie iteracyjne\n",
    "section": "\n12.2 Symulowane wyżarzanie",
    "text": "12.2 Symulowane wyżarzanie\nSymulowane wyżarzanie (ang. simulated annealing) (Kirkpatrick, Gelatt, i Vecchi 1983; Laarhoven i Aarts 1987); jest nieliniową procedurą wyszukiwania zainspirowaną procesem stygnięcia metalu. Jest to metoda globalnego wyszukiwania, która może efektywnie poruszać się po wielu różnych obszarach poszukiwań, w tym po funkcjach nieciągłych. W przeciwieństwie do większości procedur optymalizacji opartych na gradiencie, symulowane wyżarzanie może ponownie ocenić poprzednie rozwiązania.\nProces użycia symulowanego wyżarzania rozpoczyna się od wartości początkowej i rozpoczyna kontrolowany losowy spacer przez przestrzeń parametrów. Każda nowa wartość parametru-kandydata jest niewielką perturbacją poprzedniej wartości, która utrzymuje nowy punkt w lokalnym sąsiedztwie.\nPunkt kandydujący jest oceniana przy zastosowaniu resamplingu, aby uzyskać odpowiadającą mu wartość wydajności. Jeśli osiąga ona lepsze wyniki niż poprzednie parametry, jest akceptowana jako nowa najlepsza i proces jest kontynuowany. Jeśli wyniki są gorsze niż poprzednia wartość, procedura wyszukiwania może nadal używać tego parametru do określenia dalszych kroków. Zależy to od dwóch czynników. Po pierwsze, prawdopodobieństwo zatrzymania złego kandydata maleje wraz z pogorszeniem się wyników. Innymi słowy, tylko nieco gorszy wynik od obecnie najlepszego ma większą szansę na akceptację niż ten z dużym spadkiem wydajności. Drugim czynnikiem jest liczba iteracji wyszukiwania. Symulowane wyżarzanie próbuje zaakceptować mniej suboptymalnych wartości w miarę postępu wyszukiwania. Z tych dwóch czynników prawdopodobieństwo akceptacji złego wyniku można sformalizować jako:\n\\[\n\\operatorname{Pr}[\\text{accept suboptimal parameters at iteration } i] = \\exp(c\\times D_i \\times i)\n\\tag{12.2}\\]\ngdzie \\(i\\) jest numerem iteracji, \\(c\\) jest stałą określoną przez użytkownika, \\(D_i\\) jest procentową różnicą pomiędzy starą i nową wartością (gdzie wartości ujemne oznaczają gorsze wyniki). Dla złego wyniku określamy prawdopodobieństwo akceptacji i porównujemy je z liczbą wylosowaną z rozkładu jednostajnego. Jeśli liczba ta jest większa od wartości prawdopodobieństwa, wyszukiwanie odrzuca bieżące parametry i następna iteracja tworzy swoją wartość kandydata w sąsiedztwie poprzedniej wartości. W przeciwnym razie następna iteracja tworzy kolejny zestaw parametrów na podstawie bieżących (suboptymalnych) wartości.\n\n\n\n\n\n\nZagrożenie\n\n\n\nPrawdopodobieństwa akceptacji symulowanego wyżarzania pozwalają na postępowanie w złym kierunku, przynajmniej na krótką metę, z potencjałem znalezienia znacznie lepszego regionu przestrzeni parametrów w dłuższej perspektywie.\n\n\nMapa ciepła na Rysunek 12.8 pokazuje, jak prawdopodobieństwo akceptacji może się zmieniać w zależności od iteracji, wydajności i współczynnika określonego przez użytkownika.\n\n\nRysunek 12.8: Mapa ciepła prawdopodobieństwa akceptacji symulowanego wyżarzania dla różnych wartości współczynnika\n\n\nUżytkownik może dostosować współczynniki \\(c\\), aby znaleźć profil prawdopodobieństwa, który odpowiada jego potrzebom. W finetune::control_sim_anneal(), domyślnym dla tego argumentu cooling_coef jest 0.02. Zmniejszenie tego współczynnika zachęci wyszukiwanie do bycia bardziej wyrozumiałym dla słabych wyników.\nProces ten trwa przez określoną ilość iteracji, ale może zostać zatrzymany, jeśli w ciągu określonej liczby iteracji nie pojawią się globalnie najlepsze wyniki. Bardzo pomocne może być ustawienie progu restartu. Jeśli wystąpi ciąg niepowodzeń, funkcja ta powraca do ostatnich globalnie najlepszych ustawień parametrów i zaczyna od nowa.\n\n\n\n\nNajważniejszym szczegółem jest określenie sposobu perturbacji parametrów dostrajania z iteracji na iterację. W literaturze można znaleźć wiele metod na to. My stosujemy metodę podaną przez Bohachevsky, Johnson, i Stein (1986) zwaną uogólnionym symulowanym wyżarzaniem. Dla ciągłych parametrów dostrajania definiujemy mały promień, aby określić lokalne “sąsiedztwo”. Na przykład załóżmy, że są dwa parametry dostrajania i każdy z nich jest ograniczony przez zero i jeden. Proces symulowanego wyżarzania generuje losowe wartości na otaczającym promieniu i losowo wybiera jedną z nich jako aktualną wartość kandydacką.\nWielkość promienia kontroluje, jak szybko wyszukiwanie bada przestrzeń parametrów. Im większy jest promień tym szybciej przestrzeń parametrów będzie przeszukiwana przez algorytm ale jednocześnie mniej dokładnie.\nDla zilustrowania użyjemy dwóch głównych parametrów dostrajania glmnet:\n\nWielkość kary (penalty). Domyślny zakres tego parametru to od \\(10^{-10}\\) do \\(10^0\\). Najczęściej jest podawany w skali logarytmicznej o podstawie 10.\nProporcja kary lasso (mixture) - wartość pomiędzy 0 i 1 określająca balans pomiędzy karą L1 i L2.\n\nProces rozpoczyna się od wartości początkowych penalty = 0,025 i mixture = 0,050. Używając promienia, który losowo waha się między 0,050 a 0,015, dane są odpowiednio skalowane, losowe wartości są generowane na promieniach wokół punktu początkowego, a następnie jedna jest losowo wybierana jako kandydat. Dla ilustracji przyjmiemy, że wszystkie wartości kandydujące faktycznie poprawiają wydajność modelu. Korzystając z nowej wartości, generowany jest zestaw nowych losowych sąsiadów, wybierany jest jeden, i tak dalej. Rysunek 12.9 przedstawia sześć iteracji sukcesywnie podążających do lewego górnego rogu.\n\n\nRysunek 12.9: Kilka iteracji procesu symulowanego wyżarzania\n\n\nZauważmy, że podczas niektórych iteracji zestawy kandydatów wzdłuż promienia wykluczają punkty poza granicami parametrów. Ponadto, nasza implementacja przesuwa wybór kolejnych konfiguracji dostrajania parametrów od nowych wartości, które są bardzo podobne do poprzednich konfiguracji.\nDla parametrów kategorycznych i całkowitych, każdy z nich jest generowany z określonym wcześniej prawdopodobieństwem. Argument flip w control_sim_anneal() może być użyty do określenia tego prawdopodobieństwa. Dla parametrów całkowitoliczbowych, używana jest najbliższa wartość całkowita.\nPrzeszukiwanie metodą symulowanego wyżarzania może nie być optymalna w przypadku, gdy wiele parametrów jest nienumerycznych lub całkowitoliczbowych o niewielu unikalnych wartościach. W tych przypadkach jest prawdopodobne, że ten sam zestaw kandydatów może być testowany więcej niż raz.\nAby zaimplementować wyszukiwanie iteracyjne poprzez symulowane wyżarzanie, użyj funkcji tune_sim_anneal(). Składnia tej funkcji jest niemal identyczna jak tune_bayes(). Nie ma żadnych opcji dotyczących funkcji akwizycji czy próbkowania niepewności. Funkcja control_sim_anneal() posiada pewne szczegóły, które definiują lokalne sąsiedztwo i harmonogram chłodzenia:\n\n\nno_improve jest liczbą całkowitą, która zatrzyma wyszukiwanie, jeśli w ciągu iteracji określonym przez no_improve nie zostaną odkryte globalnie najlepsze wyniki. Przyjęte suboptymalne lub odrzucone parametry liczą się jako “brak poprawy”.\n\nrestart to liczba iteracji, która musi minąć bez poprawy jakości modelu, po której następuje przejście do najlepszego poprzednio ustalonego zestawu parametrów.\n\nradius to wektor liczbowy w przedziale (0, 1), który określa minimalny i maksymalny promień lokalnego sąsiedztwa wokół punktu początkowego.\n\nflip to wartość prawdopodobieństwa określająca szanse zmiany wartości parametrów kategorycznych lub całkowitych.\n\ncooling_coef jest współczynnikiem \\(c\\) w \\(\\exp(c\\times D_i \\times i)\\), który moduluje jak szybko prawdopodobieństwo akceptacji maleje w trakcie iteracji. Większe wartości cooling_coef zmniejszają prawdopodobieństwo akceptacji suboptymalnego ustawienia parametrów.\n\nDla danych dotyczących segmentacji komórek składnia jest bardzo spójna z poprzednio stosowanymi funkcjami:\n\nKodctrl_sa &lt;- control_sim_anneal(verbose = TRUE, no_improve = 10L)\n\nset.seed(1404)\nsvm_sa &lt;-\n  svm_wflow %&gt;%\n  tune_sim_anneal(\n    resamples = cell_folds,\n    metrics = roc_res,\n    initial = svm_initial,\n    param_info = svm_param,\n    iter = 50,\n    control = ctrl_sa\n  )\n\n\nDane wyjściowe dla poszczególnych iteracji:\n\nKodcollect_metrics(svm_sa) |&gt; \n  gt()\n\n\n\n\n\n\nTabela 12.5:  Podsumowanie symulowanego wyżarzania \n  \ncost\n      rbf_sigma\n      .metric\n      .estimator\n      mean\n      n\n      std_err\n      .config\n      .iter\n    \n\n\n0.0156250\n1.000000e-06\nroc_auc\nbinary\n0.8638724\n10\n0.008637894\ninitial_Preprocessor1_Model1\n0\n\n\n2.0000000\n1.000000e-06\nroc_auc\nbinary\n0.8625326\n10\n0.008672545\ninitial_Preprocessor1_Model2\n0\n\n\n0.0156250\n1.000000e-04\nroc_auc\nbinary\n0.8627495\n10\n0.008624554\ninitial_Preprocessor1_Model3\n0\n\n\n2.0000000\n1.000000e-04\nroc_auc\nbinary\n0.8659439\n10\n0.008545691\ninitial_Preprocessor1_Model4\n0\n\n\n2.7792215\n4.231877e-05\nroc_auc\nbinary\n0.8635104\n10\n0.008642153\nIter1\n1\n\n\n8.9751798\n1.317101e-04\nroc_auc\nbinary\n0.8733437\n10\n0.008401879\nIter2\n2\n\n\n8.0489544\n3.113457e-05\nroc_auc\nbinary\n0.8672404\n10\n0.008380301\nIter3\n3\n\n\n7.4631879\n6.401369e-05\nroc_auc\nbinary\n0.8704169\n10\n0.008147365\nIter4\n4\n\n\n25.4276256\n6.964316e-05\nroc_auc\nbinary\n0.8744436\n10\n0.008459084\nIter5\n5\n\n\n26.0442746\n1.620597e-05\nroc_auc\nbinary\n0.8696570\n10\n0.008110480\nIter6\n6\n\n\n27.8247939\n6.830484e-06\nroc_auc\nbinary\n0.8658910\n10\n0.008447841\nIter7\n7\n\n\n11.5246937\n1.459848e-06\nroc_auc\nbinary\n0.8623315\n10\n0.008657469\nIter8\n8\n\n\n11.5911446\n4.938754e-06\nroc_auc\nbinary\n0.8622997\n10\n0.008647646\nIter9\n9\n\n\n16.0739132\n2.533161e-06\nroc_auc\nbinary\n0.8623219\n10\n0.008658226\nIter10\n10\n\n\n22.9676231\n1.507608e-06\nroc_auc\nbinary\n0.8623751\n10\n0.008670690\nIter11\n11\n\n\n14.6174389\n3.625603e-07\nroc_auc\nbinary\n0.8624605\n10\n0.008645675\nIter12\n12\n\n\n30.2128582\n1.354001e-07\nroc_auc\nbinary\n0.8625366\n10\n0.008612533\nIter13\n13\n\n\n14.1812532\n7.415027e-05\nroc_auc\nbinary\n0.8726092\n10\n0.008351119\nIter14\n14\n\n\n5.1974958\n1.892985e-04\nroc_auc\nbinary\n0.8727956\n10\n0.008301061\nIter15\n15\n\n\n6.2799813\n1.101844e-03\nroc_auc\nbinary\n0.8897887\n10\n0.008859945\nIter16\n16\n\n\n1.7551679\n2.895039e-03\nroc_auc\nbinary\n0.8918849\n10\n0.008922869\nIter17\n17\n\n\n2.7450163\n7.152477e-03\nroc_auc\nbinary\n0.8958891\n9\n0.009100454\nIter18\n18\n\n\n0.6580640\n7.237199e-03\nroc_auc\nbinary\n0.8944414\n10\n0.008826226\nIter19\n19\n\n\n0.2894265\n2.800490e-03\nroc_auc\nbinary\n0.8781751\n10\n0.008519248\nIter20\n20\n\n\n0.8648804\n9.061603e-04\nroc_auc\nbinary\n0.8744488\n10\n0.008274188\nIter21\n21\n\n\n2.6900002\n4.370896e-04\nroc_auc\nbinary\n0.8748006\n10\n0.008467929\nIter22\n22\n\n\n1.9738868\n8.518383e-04\nroc_auc\nbinary\n0.8787410\n10\n0.008594452\nIter23\n23\n\n\n2.8390760\n1.343734e-04\nroc_auc\nbinary\n0.8694466\n10\n0.008255440\nIter24\n24\n\n\n0.6443050\n2.818297e-04\nroc_auc\nbinary\n0.8660129\n10\n0.008642925\nIter25\n25\n\n\n0.4332128\n8.494378e-04\nroc_auc\nbinary\n0.8702872\n10\n0.008407344\nIter26\n26\n\n\n1.5062749\n4.117203e-02\nroc_auc\nbinary\n0.8868902\n10\n0.008384564\nIter27\n27\n\n\n1.4313131\n1.549218e-02\nroc_auc\nbinary\n0.8963429\n10\n0.008367098\nIter28\n28\n\n\n2.7699626\n1.362418e-02\nroc_auc\nbinary\n0.8929540\n10\n0.008489741\nIter29\n29\n\n\n4.9266582\n4.583077e-02\nroc_auc\nbinary\n0.8698810\n10\n0.009097231\nIter30\n30\n\n\n7.7758900\n9.490760e-03\nroc_auc\nbinary\n0.8874485\n10\n0.008767151\nIter31\n31\n\n\n4.8976312\n1.580979e-02\nroc_auc\nbinary\n0.8825035\n10\n0.009000858\nIter32\n32\n\n\n12.4918485\n8.493479e-03\nroc_auc\nbinary\n0.8829449\n10\n0.008897035\nIter33\n33\n\n\n9.4138354\n2.056779e-03\nroc_auc\nbinary\n0.8971685\n10\n0.008342496\nIter34\n34\n\n\n16.7499203\n2.398614e-03\nroc_auc\nbinary\n0.8985179\n10\n0.007916078\nIter35\n35\n\n\n18.8139030\n4.403197e-04\nroc_auc\nbinary\n0.8839150\n9\n0.009020320\nIter36\n36\n\n\n21.9409340\n1.216970e-03\nroc_auc\nbinary\n0.8973481\n10\n0.008280488\nIter37\n37\n\n\n8.7296351\n5.154540e-03\nroc_auc\nbinary\n0.8961246\n10\n0.008263055\nIter38\n38\n\n\n13.5493211\n2.655684e-02\nroc_auc\nbinary\n0.8593977\n10\n0.009612964\nIter39\n39\n\n\n3.4729934\n1.312798e-03\nroc_auc\nbinary\n0.8847509\n9\n0.009126495\nIter40\n40\n\n\n3.1103793\n4.869128e-03\nroc_auc\nbinary\n0.8969694\n10\n0.008168916\nIter41\n41\n\n\n1.2292254\n6.540634e-03\nroc_auc\nbinary\n0.8965152\n10\n0.008550536\nIter42\n42\n\n\n0.5769315\n2.846348e-03\nroc_auc\nbinary\n0.8842725\n10\n0.008540335\nIter43\n43\n\n\n22.9584171\n1.241411e-02\nroc_auc\nbinary\n0.8618903\n10\n0.009740857\nIter44\n44\n\n\n10.6297370\n4.059190e-03\nroc_auc\nbinary\n0.8975137\n10\n0.008203966\nIter45\n45\n\n\n10.6164560\n2.519507e-02\nroc_auc\nbinary\n0.8617737\n10\n0.009517440\nIter46\n46\n\n\n24.5183146\n7.016539e-03\nroc_auc\nbinary\n0.8775665\n10\n0.008934404\nIter47\n47\n\n\n10.2189037\n1.330918e-02\nroc_auc\nbinary\n0.8733744\n10\n0.009258351\nIter48\n48\n\n\n7.1021781\n1.966405e-03\nroc_auc\nbinary\n0.8966379\n10\n0.008539778\nIter49\n49\n\n\n9.2002334\n5.308068e-04\nroc_auc\nbinary\n0.8841932\n10\n0.008839438\nIter50\n50\n\n\n\n\n\n\n\n\nKodshow_best(svm_sa) |&gt; \n  gt()\n\n\n\n\n\n\nTabela 12.6:  Najlepsze kombinacje parametrów na podstawie SA \n  \ncost\n      rbf_sigma\n      .metric\n      .estimator\n      mean\n      n\n      std_err\n      .config\n      .iter\n    \n\n\n16.749920\n0.002398614\nroc_auc\nbinary\n0.8985179\n10\n0.007916078\nIter35\n35\n\n\n10.629737\n0.004059190\nroc_auc\nbinary\n0.8975137\n10\n0.008203966\nIter45\n45\n\n\n21.940934\n0.001216970\nroc_auc\nbinary\n0.8973481\n10\n0.008280488\nIter37\n37\n\n\n9.413835\n0.002056779\nroc_auc\nbinary\n0.8971685\n10\n0.008342496\nIter34\n34\n\n\n3.110379\n0.004869128\nroc_auc\nbinary\n0.8969694\n10\n0.008168916\nIter41\n41\n\n\n\n\n\n\n\nPodobnie jak w przypadku innych funkcji tune_*(), odpowiadająca im funkcja autoplot() tworzy wizualną ocenę wyników. Użycie autoplot(svm_sa, type = \"performance\") pokazuje wydajność w czasie iteracji (Rysunek 12.10), podczas gdy autoplot(svm_sa, type = \"parameters\") przedstawia wydajność w zależności od konkretnych wartości dostrajania parametrów (Rysunek 12.11).\n\nKodautoplot(svm_sa, type = \"performance\")\n\n\n\nRysunek 12.10: Przebieg procesu symulowanego wyżarzania\n\n\n\n\n\nKodautoplot(svm_sa, type = \"parameters\")\n\n\n\nRysunek 12.11: Wydajność modeli w kontekście hiperparametrów modelu\n\n\n\n\nWizualizacja ścieżki wyszukiwania pomaga zrozumieć, gdzie proces wyszukiwania poradził sobie dobrze, a gdzie pobłądził:\n\nVideo\nRysunek 12.12: Wizualizacja przebiegu symulowanego wyżarzania\n\n\n\n\n\n\nBohachevsky, Ihor O., Mark E. Johnson, i Myron L. Stein. 1986. „Generalized Simulated Annealing for Function Optimization”. Technometrics 28 (3): 209–17. https://doi.org/10.1080/00401706.1986.10488128.\n\n\nFrazier, Peter I. 2018. „A Tutorial on Bayesian Optimization”. https://doi.org/10.48550/ARXIV.1807.02811.\n\n\nKirkpatrick, S., C. D. Gelatt, i M. P. Vecchi. 1983. „Optimization by Simulated Annealing”. Science 220 (4598): 671–80. https://doi.org/10.1126/science.220.4598.671.\n\n\nLaarhoven, Peter J. M. van, i Emile H. L. Aarts. 1987. „Simulated annealing”. W, 7–15. Springer Netherlands. https://doi.org/10.1007/978-94-015-7744-1_2.\n\n\nSchulz, Eric, Maarten Speekenbrink, i Andreas Krause. 2016. „A tutorial on Gaussian process regression: Modelling, exploring, and exploiting functions”. http://dx.doi.org/10.1101/095190.\n\n\nShahriari, Bobak, Kevin Swersky, Ziyu Wang, Ryan P. Adams, i Nando de Freitas. 2016. „Taking the Human Out of the Loop: A Review of Bayesian Optimization”. Proceedings of the IEEE 104 (1): 148–75. https://doi.org/10.1109/jproc.2015.2494218."
  },
  {
    "objectID": "example.html#przygotowanie-danych",
    "href": "example.html#przygotowanie-danych",
    "title": "\n13  Poszukiwanie optymalnego modelu\n",
    "section": "\n13.1 Przygotowanie danych",
    "text": "13.1 Przygotowanie danych\n\nKodlibrary(tidymodels)\nlibrary(finetune)\ntidymodels_prefer()\ndata(concrete, package = \"modeldata\")\nglimpse(concrete)\n\nRows: 1,030\nColumns: 9\n$ cement               &lt;dbl&gt; 540.0, 540.0, 332.5, 332.5, 198.6, 266.0, 380.0, …\n$ blast_furnace_slag   &lt;dbl&gt; 0.0, 0.0, 142.5, 142.5, 132.4, 114.0, 95.0, 95.0,…\n$ fly_ash              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ water                &lt;dbl&gt; 162, 162, 228, 228, 192, 228, 228, 228, 228, 228,…\n$ superplasticizer     &lt;dbl&gt; 2.5, 2.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,…\n$ coarse_aggregate     &lt;dbl&gt; 1040.0, 1055.0, 932.0, 932.0, 978.4, 932.0, 932.0…\n$ fine_aggregate       &lt;dbl&gt; 676.0, 676.0, 594.0, 594.0, 825.5, 670.0, 594.0, …\n$ age                  &lt;int&gt; 28, 28, 270, 365, 360, 90, 365, 28, 28, 28, 90, 2…\n$ compressive_strength &lt;dbl&gt; 79.99, 61.89, 40.27, 41.05, 44.30, 47.03, 43.70, …\n\n\nZmienna compressive_strength jest zmienną zależną, przewidywaną w tym zadaniu. W kilku przypadkach w tym zestawie danych, ta sama formuła betonu była testowana wielokrotnie. Wolimy nie uwzględniać tych replikowanych mieszanek jako pojedynczych punktów danych, ponieważ mogą one być rozmieszczone zarówno w zbiorze treningowym, jak i testowym. Może to sztucznie zawyżyć nasze szacunki wydajności.\n\n\n\n\n\nKodconcrete &lt;- \n   concrete %&gt;% \n   group_by(across(-compressive_strength)) %&gt;% \n   summarize(compressive_strength = mean(compressive_strength),\n             .groups = \"drop\")\nnrow(concrete)\n\n[1] 992\n\n\nPodzielmy dane przy użyciu domyślnego stosunku 3:1 treningu do testu i ponownie wypróbujmy zestaw treningowy przy użyciu pięciu powtórzeń 10-krotnej walidacji krzyżowej:\n\nKodset.seed(1501)\nconcrete_split &lt;- initial_split(concrete, strata = compressive_strength)\nconcrete_train &lt;- training(concrete_split)\nconcrete_test  &lt;- testing(concrete_split)\n\nset.seed(1502)\nconcrete_folds &lt;- \n   vfold_cv(concrete_train, strata = compressive_strength, repeats = 5)\n\n\nNiektóre modele (zwłaszcza sieci neuronowe, KNN i SVM) wymagają predyktorów, które zostały wyśrodkowane i przeskalowane, więc niektóre przepływy pracy modelu będą wymagały przepisów z tymi krokami przetwarzania wstępnego. Dla innych modeli, tradycyjne rozwinięcie modelu powierzchni odpowiedzi (tj. interakcje kwadratowe i dwukierunkowe) jest dobrym pomysłem. Dla tych celów tworzymy dwie receptury:\n\nKodnormalized_rec &lt;- \n   recipe(compressive_strength ~ ., data = concrete_train) %&gt;% \n   step_normalize(all_predictors()) \n\npoly_recipe &lt;- \n   normalized_rec %&gt;% \n   step_poly(all_predictors()) %&gt;% \n   step_interact(~ all_predictors():all_predictors())"
  },
  {
    "objectID": "example.html#określenie-modeli",
    "href": "example.html#określenie-modeli",
    "title": "\n13  Poszukiwanie optymalnego modelu\n",
    "section": "\n13.2 Określenie modeli",
    "text": "13.2 Określenie modeli\nTeraz zdefiniujmy model, które chcemy przetestować:\n\nKodlibrary(rules)\nlibrary(baguette)\n\nlinear_reg_spec &lt;- \n   linear_reg(penalty = tune(), mixture = tune()) %&gt;% \n   set_engine(\"glmnet\")\n\nnnet_spec &lt;- \n   mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %&gt;% \n   set_engine(\"nnet\", MaxNWts = 2600) %&gt;% \n   set_mode(\"regression\")\n\nmars_spec &lt;- \n   mars(prod_degree = tune()) %&gt;%  #&lt;- use GCV to choose terms\n   set_engine(\"earth\") %&gt;% \n   set_mode(\"regression\")\n\nsvm_r_spec &lt;- \n   svm_rbf(cost = tune(), rbf_sigma = tune()) %&gt;% \n   set_engine(\"kernlab\") %&gt;% \n   set_mode(\"regression\")\n\nsvm_p_spec &lt;- \n   svm_poly(cost = tune(), degree = tune()) %&gt;% \n   set_engine(\"kernlab\") %&gt;% \n   set_mode(\"regression\")\n\nknn_spec &lt;- \n   nearest_neighbor(neighbors = tune(), dist_power = tune(), weight_func = tune()) %&gt;% \n   set_engine(\"kknn\") %&gt;% \n   set_mode(\"regression\")\n\ncart_spec &lt;- \n   decision_tree(cost_complexity = tune(), min_n = tune()) %&gt;% \n   set_engine(\"rpart\") %&gt;% \n   set_mode(\"regression\")\n\nbag_cart_spec &lt;- \n   bag_tree() %&gt;% \n   set_engine(\"rpart\", times = 50L) %&gt;% \n   set_mode(\"regression\")\n\nrf_spec &lt;- \n   rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %&gt;% \n   set_engine(\"ranger\") %&gt;% \n   set_mode(\"regression\")\n\nxgb_spec &lt;- \n   boost_tree(tree_depth = tune(), learn_rate = tune(), loss_reduction = tune(), \n              min_n = tune(), sample_size = tune(), trees = tune()) %&gt;% \n   set_engine(\"xgboost\") %&gt;% \n   set_mode(\"regression\")\n\ncubist_spec &lt;- \n   cubist_rules(committees = tune(), neighbors = tune()) %&gt;% \n   set_engine(\"Cubist\") \n\n\nAutorzy w Khun i Johnson (2013) określa, że sieć neuronowa powinna mieć do 27 jednostek ukrytych w warstwie. Funkcja extract_parameter_set_dials() wyodrębnia zbiór parametrów, który modyfikujemy, aby miał prawidłowy zakres parametrów:\n\nKodnnet_param &lt;- \n   nnet_spec %&gt;% \n   extract_parameter_set_dials() %&gt;% \n   update(hidden_units = hidden_units(c(1, 27)))\n\n\nW następnym kroku stworzymy zestawy przepływu pracy:\n\nKodnormalized &lt;- \n   workflow_set(\n      preproc = list(normalized = normalized_rec), \n      models = list(SVM_radial = svm_r_spec, SVM_poly = svm_p_spec, \n                    KNN = knn_spec, neural_network = nnet_spec)\n   )\nnormalized\n\n# A workflow set/tibble: 4 × 4\n  wflow_id                  info             option    result    \n  &lt;chr&gt;                     &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 normalized_SVM_radial     &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 normalized_SVM_poly       &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 normalized_KNN            &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 normalized_neural_network &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n\nPonieważ zastosowaliśmy tylko jedna funkcję wstępnej obróbki danych (normalized_rec), to w podsumowaniu występują tylko kombinacje tego preprocesora i modeli. Kolumna wflow_id tworzona jest automatycznie, ale może być modyfikowana poprzez wywołanie mutate(). Kolumna info zawiera tibble z pewnymi identyfikatorami i obiektem przepływu pracy. Przepływ pracy może zostać wyodrębniony:\n\nKodnormalized %&gt;% extract_workflow(id = \"normalized_KNN\")\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-Nearest Neighbor Model Specification (regression)\n\nMain Arguments:\n  neighbors = tune()\n  weight_func = tune()\n  dist_power = tune()\n\nComputational engine: kknn \n\n\nKolumna option to miejsce na dowolne argumenty, których należy użyć, gdy oceniamy przepływ pracy. Na przykład, aby dodać obiekt parametrów sieci neuronowej:\n\n\n\n\n\nKodnormalized &lt;- \n   normalized %&gt;% \n   option_add(param_info = nnet_param, id = \"normalized_neural_network\")\nnormalized\n\n# A workflow set/tibble: 4 × 4\n  wflow_id                  info             option    result    \n  &lt;chr&gt;                     &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 normalized_SVM_radial     &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 normalized_SVM_poly       &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 normalized_KNN            &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 normalized_neural_network &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;\n\n\nKolumna result jest miejscem na wyjście funkcji dostrajania lub resamplingu. Dla innych modeli nieliniowych utwórzmy kolejny zestaw przepływów pracy:\n\nKodmodel_vars &lt;- \n   workflow_variables(outcomes = compressive_strength, \n                      predictors = everything())\n\nno_pre_proc &lt;- \n   workflow_set(\n      preproc = list(simple = model_vars), \n      models = list(MARS = mars_spec, CART = cart_spec, CART_bagged = bag_cart_spec,\n                    RF = rf_spec, boosting = xgb_spec, Cubist = cubist_spec)\n   )\nno_pre_proc\n\n# A workflow set/tibble: 6 × 4\n  wflow_id           info             option    result    \n  &lt;chr&gt;              &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 simple_MARS        &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 simple_CART        &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 simple_CART_bagged &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 simple_RF          &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n5 simple_boosting    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n6 simple_Cubist      &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n\nNa koniec składamy zestaw wykorzystujący warunki nieliniowe i interakcje z odpowiednimi modelami:\n\nKodwith_features &lt;- \n   workflow_set(\n      preproc = list(full_quad = poly_recipe), \n      models = list(linear_reg = linear_reg_spec, KNN = knn_spec)\n   )\n\n\nTe obiekty to tibble z dodatkową klasą workflow_set. Łączenie wierszy nie wpływa na stan zestawów, a wynik jest sam w sobie zestawem przepływów pracy:\n\nKodall_workflows &lt;- \n   bind_rows(no_pre_proc, normalized, with_features) %&gt;% \n   # Make the workflow ID's a little more simple: \n   mutate(wflow_id = gsub(\"(simple_)|(normalized_)\", \"\", wflow_id))\nall_workflows\n\n# A workflow set/tibble: 12 × 4\n   wflow_id             info             option    result    \n   &lt;chr&gt;                &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n 1 MARS                 &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 2 CART                 &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 3 CART_bagged          &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 4 RF                   &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 5 boosting             &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 6 Cubist               &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 7 SVM_radial           &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 8 SVM_poly             &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 9 KNN                  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n10 neural_network       &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;\n11 full_quad_linear_reg &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n12 full_quad_KNN        &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;"
  },
  {
    "objectID": "example.html#tuning-modeli",
    "href": "example.html#tuning-modeli",
    "title": "\n13  Poszukiwanie optymalnego modelu\n",
    "section": "\n13.3 Tuning modeli",
    "text": "13.3 Tuning modeli\nPrawie wszystkie modele ujęte w all_workflows zawierają parametry dostrajania. Aby ocenić ich wydajność, możemy użyć standardowych funkcji strojenia lub resamplingu (np. tune_grid() i tak dalej). Funkcja workflow_map() zastosuje tę samą funkcję do wszystkich przepływów w zestawie; domyślnie jest to tune_grid().\n\n\n\n\nDla tego przykładu, wyszukiwanie w oparciu o siatkę jest stosowane do każdego przepływu pracy, stosując jednocześnie 25 różnych kandydatów na parametry. Istnieje zestaw wspólnych opcji do wykorzystania przy każdym wykonaniu tune_grid(). Na przykład, w poniższym kodzie użyjemy tego samego próbkowania i obiektów kontrolnych dla każdego przepływu pracy, wraz z rozmiarem siatki równym 25. Funkcja workflow_map() posiada dodatkowy argument o nazwie seed, który służy do zapewnienia, że każde wykonanie tune_grid() zużywa tych samych liczb losowych.\n\nKodlibrary(doParallel)\ncl &lt;- makeCluster(4)\nregisterDoParallel(cl)\n\ngrid_ctrl &lt;-\n   control_grid(\n      save_pred = TRUE,\n      parallel_over = \"everything\",\n      save_workflow = TRUE\n   )\n\ngrid_results &lt;-\n   all_workflows %&gt;%\n   workflow_map(\n      seed = 1503,\n      resamples = concrete_folds,\n      grid = 25,\n      control = grid_ctrl\n   )\n\n\nW podsumowaniu widać, że kolumny option i result zostały zaktualizowane:\n\n\n# A workflow set/tibble: 12 × 4\n   wflow_id             info             option    result   \n   &lt;chr&gt;                &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n 1 MARS                 &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n 2 CART                 &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n 3 CART_bagged          &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;rsmp[+]&gt;\n 4 RF                   &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n 5 boosting             &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n 6 Cubist               &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n 7 SVM_radial           &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n 8 SVM_poly             &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n 9 KNN                  &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n10 neural_network       &lt;tibble [1 × 4]&gt; &lt;opts[4]&gt; &lt;tune[+]&gt;\n11 full_quad_linear_reg &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n12 full_quad_KNN        &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n\n\nKolumna option zawiera teraz wszystkie opcje, których użyliśmy w wywołaniu workflow_map(). W kolumnach result, notacje tune[+] i rsmp[+] oznaczają, że obiekt nie miał żadnych problemów w procesie optymalizacji. Wartość taka jak tune[x] pojawia się, gdy wszystkie modele z jakiegoś powodu zawiodły.\nIstnieje kilka wygodnych funkcji do badania wyników, takich jak grid_results. Funkcja rank_results() uporządkuje modele według wybranej metryki wydajności. Domyślnie używa ona pierwszej metryki w zestawie metryk (w tym przypadku RMSE). Przefiltrujmy wyniki, aby analizować tylko na RMSE:\n\nKodgrid_results %&gt;% \n   rank_results() %&gt;% \n   filter(.metric == \"rmse\") %&gt;% \n   select(model, .config, rmse = mean, rank)\n\n# A tibble: 252 × 4\n   model        .config                rmse  rank\n   &lt;chr&gt;        &lt;chr&gt;                 &lt;dbl&gt; &lt;int&gt;\n 1 boost_tree   Preprocessor1_Model04  4.25     1\n 2 boost_tree   Preprocessor1_Model06  4.29     2\n 3 boost_tree   Preprocessor1_Model13  4.31     3\n 4 boost_tree   Preprocessor1_Model14  4.39     4\n 5 boost_tree   Preprocessor1_Model16  4.46     5\n 6 boost_tree   Preprocessor1_Model03  4.47     6\n 7 boost_tree   Preprocessor1_Model15  4.48     7\n 8 boost_tree   Preprocessor1_Model05  4.55     8\n 9 boost_tree   Preprocessor1_Model20  4.71     9\n10 cubist_rules Preprocessor1_Model24  4.71    10\n# ℹ 242 more rows\n\n\nDomyślnie funkcja szereguje wszystkie zestawy kandydatów, dlatego ten sam model może pojawić się wielokrotnie na wyjściu. Opcja select_best może być użyta do uszeregowania modeli przy użyciu najlepszej kombinacji dostrajania parametrów. Metoda autoplot() tworzy wykresy rankingowy; posiada ona również argument select_best. Wykres na Rysunek 13.1 wizualizuje najlepsze wyniki dla każdego modelu i jest generowany za pomocą:\n\nKodautoplot(\n   grid_results,\n   rank_metric = \"rmse\",  # &lt;- how to order models\n   metric = \"rmse\",       # &lt;- which metric to visualize\n   select_best = TRUE     # &lt;- one point per workflow\n) +\n   geom_text(aes(y = mean - 1/2, label = wflow_id), angle = 90, hjust = 1) +\n   lims(y = c(3, 9.5)) +\n   theme(legend.position = \"none\")\n\n\n\nRysunek 13.1: Oszacowany RMSE (i przybliżone przedziały ufności) dla najlepszej konfiguracji modelu w każdym przepływie pracy\n\n\n\n\nW przypadku, gdy chcesz zobaczyć wyniki dostrajania parametrów dla konkretnego modelu, tak jak na Rysunek 13.2, argument id może przyjąć pojedynczą wartość z kolumny wflow_id dla którego modelu ma być wykreślony:\n\nKodautoplot(grid_results, id = \"Cubist\", metric = \"rmse\")\n\n\n\nRysunek 13.2: Wizualizacja RMSE w kontekście konfiguracji hiperparametrów modelu Cubist\n\n\n\n\nFunkcje collect_metrics() i collect_predictions() również pozwalają przejrzenie wyników optymalizacji.\nW powyższym procesie optymalizacji przeuczono 12600 modeli, co zajęło około 2 godzin przy wykorzystaniu 4 rdzeni procesora. Pokazuje to, że zagadnienie tuningu nawet kilku kandydackich modeli zajmuje sporo czasu."
  },
  {
    "objectID": "example.html#efektywna-filtracja-modeli",
    "href": "example.html#efektywna-filtracja-modeli",
    "title": "\n13  Poszukiwanie optymalnego modelu\n",
    "section": "\n13.4 Efektywna filtracja modeli",
    "text": "13.4 Efektywna filtracja modeli\nJedną z metod efektywnego przesiewania dużego zbioru modeli jest zastosowanie podejścia wyścigowego opisanego wcześniej. Mając zestaw przepływów pracy, możemy użyć funkcji workflow_map() do podejścia wyścigowego.\n\n\n\n\n\nKodrace_ctrl &lt;-\n   control_race(\n      save_pred = TRUE,\n      parallel_over = \"everything\",\n      save_workflow = TRUE\n   )\n\nrace_results &lt;-\n   all_workflows %&gt;%\n   workflow_map(\n      \"tune_race_anova\",\n      seed = 1503,\n      resamples = concrete_folds,\n      grid = 25,\n      control = race_ctrl\n   )\n\n\nNowy obiekt wygląda bardzo podobnie, choć elementy kolumny wyników wykazują wartość race[+], co wskazuje na inny typ obiektu:\n\nKodload(\"models/race_results.rda\")\nrace_results\n\n# A workflow set/tibble: 12 × 4\n   wflow_id             info             option    result   \n   &lt;chr&gt;                &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n 1 MARS                 &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n 2 CART                 &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n 3 CART_bagged          &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;rsmp[+]&gt;\n 4 RF                   &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n 5 boosting             &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n 6 Cubist               &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n 7 SVM_radial           &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n 8 SVM_poly             &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n 9 KNN                  &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n10 neural_network       &lt;tibble [1 × 4]&gt; &lt;opts[4]&gt; &lt;race[+]&gt;\n11 full_quad_linear_reg &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n12 full_quad_KNN        &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n\n\n\nKodautoplot(\n   race_results,\n   rank_metric = \"rmse\",  \n   metric = \"rmse\",       \n   select_best = TRUE    \n) +\n   geom_text(aes(y = mean - 1/2, label = wflow_id), angle = 90, hjust = 1) +\n   lims(y = c(3.0, 9.5)) +\n   theme(legend.position = \"none\")\n\n\n\nRysunek 13.3: Oszacowane RMSE (i przybliżone przedziały ufności) dla najlepszej konfiguracji modelu w poszukiwaniu za pomocą metody wyścigowej.\n\n\n\n\nPodejście wyścigowe oszacowało łącznie 1050 modeli, 8,33% z pełnego zestawu 12600 modeli w pełnej siatce. W rezultacie podejście wyścigowe trwało nieco ponad 17 min., więc było 7-krotnie szybsze1.1 Wartości te będą zależały od sprzętu na jakim wykonuje się obliczenia\nNa ile zbliżone wyniki otrzymaliśmy stosując obie metody tuningu?\n\nKodmatched_results &lt;- \n   rank_results(race_results, select_best = TRUE) %&gt;% \n   select(wflow_id, .metric, race = mean, config_race = .config) %&gt;% \n   inner_join(\n      rank_results(grid_results, select_best = TRUE) %&gt;% \n         select(wflow_id, .metric, complete = mean, \n                config_complete = .config, model),\n      by = c(\"wflow_id\", \".metric\"),\n   ) %&gt;%  \n   filter(.metric == \"rmse\")\n\nlibrary(ggrepel)\n\nmatched_results %&gt;% \n   ggplot(aes(x = complete, y = race)) + \n   geom_abline(lty = 3) + \n   geom_point() + \n   geom_text_repel(aes(label = model)) +\n   coord_obs_pred() + \n   labs(x = \"Complete Grid RMSE\", y = \"Racing RMSE\") \n\n\n\n\nPodczas gdy podejście wyścigowe wybrało te same parametry kandydata co kompletna siatka tylko dla 41,67% modeli, metryki wydajności modeli wybranych przez wyścig były prawie równe. Korelacja wartości RMSE wyniosła 0,968, a korelacja rangowa 0,951. Wskazuje to, że w obrębie modelu istniało wiele kombinacji dostrajania parametrów, które dawały niemal identyczne wyniki."
  },
  {
    "objectID": "example.html#finalizacja-modelu",
    "href": "example.html#finalizacja-modelu",
    "title": "\n13  Poszukiwanie optymalnego modelu\n",
    "section": "\n13.5 Finalizacja modelu",
    "text": "13.5 Finalizacja modelu\nPodobnie do tego, co pokazaliśmy w poprzednich rozdziałach, proces wyboru ostatecznego modelu i dopasowania go na zbiorze treningowym jest prosty. Pierwszym krokiem jest wybranie zbioru treningowego do sfinalizowania. Ponieważ model boosted tree działał dobrze, wyodrębnimy go ze zbioru, zaktualizujemy parametry o numerycznie najlepsze ustawienia i dopasujemy do zbioru treningowego. W przypadku gdy mamy wątpliwości dotyczące siatki hiperparametrów dobranych podczas filtrowania modeli, np. że pomija ona ważne kombinacje, możemy zastosować do wybranego modelu metody finetune przedstawione w poprzednim rozdziale.\n\nKodbest_results &lt;- \n   race_results %&gt;% \n   extract_workflow_set_result(\"boosting\") %&gt;% \n   select_best(metric = \"rmse\")\nbest_results\n\n# A tibble: 1 × 7\n  trees min_n tree_depth learn_rate loss_reduction sample_size .config          \n  &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1  1957     8          7     0.0756    0.000000145       0.679 Preprocessor1_Mo…\n\nKodboosting_test_results &lt;- \n   race_results %&gt;% \n   extract_workflow(\"boosting\") %&gt;% \n   finalize_workflow(best_results) %&gt;% \n   last_fit(split = concrete_split)\n\n\nWyniki metryki dla zbioru testowego oraz wizualizację predykcji możemy zobaczyć stosując.\n\nKodcollect_metrics(boosting_test_results)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3.41  Preprocessor1_Model1\n2 rsq     standard       0.954 Preprocessor1_Model1\n\n\n\nKodboosting_test_results %&gt;% \n   collect_predictions() %&gt;% \n   ggplot(aes(x = compressive_strength, y = .pred)) + \n   geom_abline(color = \"gray50\", lty = 2) + \n   geom_point(alpha = 0.5) + \n   coord_obs_pred() + \n   labs(x = \"observed\", y = \"predicted\")\n\n\n\nRysunek 13.4: Porównanie wartości obserwowanych i przewidywanych z modelu\n\n\n\n\n\n\n\n\n\n\n\n\nKhun, M., i K. Johnson. 2013. Applied Predictive Modeling. New York: Springer."
  },
  {
    "objectID": "dimensionality.html#opis-danych",
    "href": "dimensionality.html#opis-danych",
    "title": "\n14  Redukcja wymiarowości\n",
    "section": "\n14.1 Opis danych",
    "text": "14.1 Opis danych\nDo celów ilustracji, jak używać redukcji wymiarowości z przepisami wykorzystamy przykładowy zestaw danych. Koklu i Ozkan (2020) opublikowali zestaw danych dotyczących wizualnych cech suszonej fasoli i opisali metody określania odmian suszonej fasoli na obrazie. Chociaż wymiarowość tych danych nie jest bardzo duża w porównaniu z wieloma problemami modelowania w świecie rzeczywistym, zapewnia ładny przykład roboczy, aby zademonstrować, jak zmniejszyć liczbę cech. W swojej pracy napisali:\n\nPodstawowym celem niniejszej pracy jest dostarczenie metody uzyskiwania jednolitych odmian nasion z produkcji roślinnej, która ma postać populacji, więc nasiona nie są certyfikowane jako jedyna odmiana. W związku z tym opracowano system wizji komputerowej do rozróżniania siedmiu różnych zarejestrowanych odmian suchej fasoli o podobnych cechach w celu uzyskania jednolitej klasyfikacji nasion. Dla modelu klasyfikacji wykonano obrazy 13 611 ziaren 7 różnych zarejestrowanych suchych odmian fasoli za pomocą kamery o wysokiej rozdzielczości.\n\nKażdy obraz zawiera wiele ziaren. Proces określania, które piksele odpowiadają konkretnej fasoli, nazywany jest segmentacją obrazu. Te piksele mogą być analizowane w celu uzyskania cech dla każdej fasoli, takich jak kolor i morfologia (tj. kształt). Cechy te są następnie wykorzystywane do modelowania wyniku (odmiany fasoli), ponieważ różne odmiany fasoli wyglądają inaczej. Dane treningowe pochodzą z zestawu ręcznie oznakowanych obrazów, a ten zestaw danych jest używany do tworzenia modelu predykcyjnego, który może rozróżnić siedem odmian fasoli: Cali, Horoz, Dermason, Seker, Bombay, Barbunya i Sira. Stworzenie skutecznego modelu może pomóc producentom w ilościowym określeniu jednorodności partii fasoli.\n\n\n\n\nIstnieje wiele metod kwantyfikacji kształtów obiektów (Mingqiang, Kidiyo, i Joseph 2008). Wiele z nich dotyczy granic lub regionów interesującego nas obiektu. Przykładowe cechy obejmują:\n\nObszar (lub rozmiar) może być oszacowany przy użyciu liczby pikseli w obiekcie lub rozmiaru wypukłego kadłuba wokół obiektu.\nObwód możemy zmierzyć używając liczby pikseli w granicy, jak również prostokąta ograniczającego (najmniejszy prostokąt zamykający obiekt).\nOś główna określa ilościowo najdłuższą linię łączącą najbardziej skrajne części obiektu. Mała oś jest prostopadła do osi głównej.\nZwartość obiektu możemy mierzyć za pomocą stosunku pola powierzchni obiektu do pola powierzchni koła o tym samym obwodzie. Na przykład symbole \\(\\bullet\\) i \\(\\times\\) mają bardzo różną zwartość.\nIstnieją również różne miary tego, jak bardzo obiekt jest podłużny. Na przykład statystyka ekscentryczności to stosunek osi głównej i małej. Istnieją również powiązane szacunki dla okrągłości i wypukłości.\n\nW danych dotyczących fasoli obliczono 16 cech morfologicznych: powierzchnię, obwód, długość osi głównej, długość osi małej, współczynnik kształtu, ekscentryczność, powierzchnię wypukłą, średnicę równoważną, rozległość, zwartość, krągłość, zwięzłość, współczynnik kształtu 1, współczynnik kształtu 2, współczynnik kształtu 3 i współczynnik kształtu 4.\n\nKodlibrary(tidymodels)\ntidymodels_prefer()\nlibrary(tidyverse)\nlibrary(beans)\n\n\nDla naszych analiz zaczynamy od podziału zbioru za pomocą initial_split(). Pozostałe dane są dzielone na zbiory treningowe i walidacyjne:\n\nKodset.seed(1601)\nbean_split &lt;- initial_split(beans, strata = class, prop = 3/4)\n\nbean_train &lt;- training(bean_split)\nbean_test  &lt;- testing(bean_split)\n\nset.seed(1602)\nbean_val &lt;- validation_split(bean_train, strata = class, prop = 4/5)\nbean_val$splits[[1]]\n\n&lt;Training/Validation/Total&gt;\n&lt;8163/2043/10206&gt;\n\n\nAby wizualnie ocenić, jak dobrze działają różne metody, możemy oszacować metody na zbiorze treningowym (n = 8163 ziaren) i wyświetlić wyniki przy użyciu zbioru walidacyjnego (n = 2043).\nPrzed rozpoczęciem jakiejkolwiek redukcji wymiarowości możemy poświęcić trochę czasu na zbadanie naszych danych. Ponieważ wiemy, że wiele z tych cech kształtu prawdopodobnie mierzy podobne koncepcje, przyjrzyjmy się strukturze korelacyjnej danych na Rysunek 14.1.\n\nKodlibrary(corrplot)\ntmwr_cols &lt;- colorRampPalette(c(\"#91CBD765\", \"#CA225E\"))\nbean_train %&gt;% \n  select(-class) %&gt;% \n  cor() %&gt;% \n  corrplot(col = tmwr_cols(200), tl.col = \"black\", method = \"ellipse\")\n\n\n\nRysunek 14.1: Macierz korelacji\n\n\n\n\nWiele z tych predyktorów jest silnie skorelowanych, jak np. powierzchnia i obwód lub współczynniki kształtu 2 i 3. Chociaż nie poświęcamy temu czasu tutaj, ważne jest również, aby zobaczyć, czy ta struktura korelacji znacząco zmienia się w różnych kategoriach wyników. Może to pomóc w stworzeniu lepszych modeli.\nZacznijmy od podstawowego przepisu wstępnego przetwarzania danych, który często stosujemy przed jakimikolwiek krokami redukcji wymiarowości. Kilka predyktorów to współczynniki, a więc prawdopodobnie będą miały skośne rozkłady. Takie rozkłady mogą siać spustoszenie w obliczeniach wariancji (takich jak te używane w PCA). Pakiet bestNormalize posiada krok, który może wymusić symetryczny rozkład predyktorów. Użyjemy tego, aby złagodzić problem skośnych rozkładów:\n\nKodlibrary(bestNormalize)\nbean_rec &lt;-\n  # Use the training data from the bean_val split object\n  recipe(class ~ ., data = analysis(bean_val$splits[[1]])) %&gt;%\n  step_zv(all_numeric_predictors()) %&gt;%\n  step_orderNorm(all_numeric_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors())\n\n\n\n\n\n\n\n\nOstrzeżenie\n\n\n\nPamiętaj, że podczas wywoływania funkcji recipe() kroki nie są w żaden sposób szacowane ani wykonywane.\n\n\nPrzepis ten zostanie rozszerzony o dodatkowe kroki dla analiz redukcji wymiarowości. Zanim to zrobimy, przejdźmy do tego, jak receptura może być używana poza przepływem pracy.\n\n\n\n\nPrzepływ pracy zawierający recepturę wykorzystuje fit() do estymacji receptury i modelu, a następnie predict() do przetwarzania danych i tworzenia przewidywań modelu. W pakiecie recipes znajdują się analogiczne funkcje, które mogą być użyte do tego samego celu:\n\n\nprep(recipe, training) dopasowuje przepis do zbioru treningowego.\n\nbake(recipe, new_data) stosuje operacje receptury do new_data.\n\nRysunek 16.3 podsumowuje to.\n\n\nRysunek 14.2: Zasada działania poszczególnych czasowników\n\n\n\nKodbean_rec_trained &lt;- prep(bean_rec)\nbean_rec_trained\n\n\nZauważ, że kroki zostały wytrenowane i że selektory nie są już ogólne (tj. all_numeric_predictors()); teraz pokazują rzeczywiste kolumny, które zostały wybrane. Również, prep(bean_rec) nie wymaga argumentu training. Możesz przekazać dowolne dane do tego argumentu, ale pominięcie go oznacza, że użyte zostaną oryginalne dane z wywołania recipe(). W naszym przypadku były to dane ze zbioru treningowego.\nJednym z ważnych argumentów funkcji prep() jest retain. Kiedy retain = TRUE (domyślnie), szacunkowa wersja zbioru treningowego jest przechowywana wewnątrz receptury. Ten zestaw danych został wstępnie przetworzony przy użyciu wszystkich kroków wymienionych w recepturze. Ponieważ funkcja prep() musi wykonywać recepturę w trakcie jej wykonywania, korzystne może być zachowanie tej wersji zbioru treningowego, tak że jeśli ten zbiór danych ma być użyty później, można uniknąć zbędnych obliczeń. Jednakże, jeśli zestaw treningowy jest duży, może być problematyczne przechowywanie tak dużej ilości danych w pamięci. Użyj wówczas retain = FALSE, aby tego uniknąć.\nPo dodaniu nowych kroków do oszacowanej receptury, ponowne zastosowanie prep() oszacuje tylko niewykształcone kroki. Przyda się to, gdy będziemy próbować różnych metod ekstrakcji cech. Inną opcją, która może pomóc zrozumieć, co dzieje się w analizie, jest log_changes:\n\nKodshow_variables &lt;- \n  bean_rec %&gt;% \n  prep(log_changes = TRUE)\n\nstep_zv (zv_6JtxV): same number of columns\n\nstep_orderNorm (orderNorm_4r8al): same number of columns\n\nstep_normalize (normalize_x6oqH): same number of columns\n\n\n\n\n\n\n\n\nWskazówka\n\n\n\nUżywanie bake() z recepturą jest bardzo podobne do używania predict() z modelem; operacje oszacowane na zbiorze treningowym są stosowane do dowolnych danych, jak dane testowe lub nowe dane w czasie predykcji.\n\n\nNa przykład, próbki zbioru walidacyjnego mogą być przetwarzane następująco:\n\nKodbean_validation &lt;- bean_val$splits %&gt;% pluck(1) %&gt;% assessment()\nbean_val_processed &lt;- bake(bean_rec_trained, new_data = bean_validation)\n\n\nRysunek 14.3 przedstawia histogramy predyktora powierzchni przed i po przygotowaniu receptury.\n\nKodlibrary(patchwork)\np1 &lt;- \n  bean_validation %&gt;% \n  ggplot(aes(x = area)) + \n  geom_histogram(bins = 30, color = \"white\", fill = \"blue\", alpha = 1/3) + \n  ggtitle(\"Original validation set data\")\n\np2 &lt;- \n  bean_val_processed %&gt;% \n  ggplot(aes(x = area)) + \n  geom_histogram(bins = 30, color = \"white\", fill = \"red\", alpha = 1/3) + \n  ggtitle(\"Processed validation set data\")\n\np1 + p2\n\n\n\nRysunek 14.3: Rozkłady przed i po transformacji\n\n\n\n\nWarto tutaj zwrócić uwagę na dwa ważne aspekty bake(). Po pierwsze, jak wcześniej wspomniano, użycie prep(recipe, retain = TRUE) zachowuje istniejącą przetworzoną wersję zbioru treningowego w recepturze. Dzięki temu użytkownik może użyć bake(recipe, new_data = NULL), która zwraca ten zestaw danych bez dalszych obliczeń. Na przykład:\n\nKodbake(bean_rec_trained, new_data = NULL) %&gt;% nrow()\n\n[1] 8163\n\nKodbean_val$splits %&gt;% pluck(1) %&gt;% analysis() %&gt;% nrow()\n\n[1] 8163\n\n\nJeśli zestaw treningowy nie jest patologicznie duży, użycie retain może zaoszczędzić dużo czasu obliczeniowego. Po drugie, w wywołaniu można użyć dodatkowych selektorów, aby określić, które kolumny mają zostać zwrócone. Domyślnym selektorem jest everything(), ale można dokonać bardziej szczegółowych wyborów."
  },
  {
    "objectID": "dimensionality.html#ekstrakcja-cech",
    "href": "dimensionality.html#ekstrakcja-cech",
    "title": "\n14  Redukcja wymiarowości\n",
    "section": "\n14.2 Ekstrakcja cech",
    "text": "14.2 Ekstrakcja cech\nPonieważ receptury są podstawową opcją w tidymodels do redukcji wymiarowości, napiszmy funkcję, która oszacuje transformację i wykreśli wynikowe dane w postaci macierzy wykresów za pośrednictwem pakietu ggforce:\n\n\n\n\n\nKodlibrary(ggforce)\nplot_validation_results &lt;- function(recipe, dat = assessment(bean_val$splits[[1]])) {\n  recipe %&gt;%\n    # Estimate any additional steps\n    prep() %&gt;%\n    # Process the data (the validation set by default)\n    bake(new_data = dat) %&gt;%\n    # Create the scatterplot matrix\n    ggplot(aes(x = .panel_x, y = .panel_y, color = class, fill = class)) +\n    geom_point(alpha = 0.4, size = 0.5) +\n    geom_autodensity(alpha = .3) +\n    facet_matrix(vars(-class), layer.diag = 2) + \n    scale_color_brewer(palette = \"Dark2\") + \n    scale_fill_brewer(palette = \"Dark2\")\n}\n\n\nFunkcji tej użyjemy wielokrotnie do wizualizacji wyników po zastosowaniu różnych metod ekstrakcji cech.\n\n14.2.1 PCA\nPCA jest metodą bez nadzoru, która wykorzystuje liniowe kombinacje predyktorów do określenia nowych cech. Cechy te starają się uwzględnić jak najwięcej zmienności w oryginalnych danych. Dodajemy step_pca() do oryginalnego przepisu i wykorzystujemy naszą funkcję do wizualizacji wyników na zbiorze walidacyjnym na Rysunek 14.4 za pomocą:\n\nKodbean_rec_trained %&gt;%\n  step_pca(all_numeric_predictors(), num_comp = 4) %&gt;%\n  plot_validation_results() + \n  ggtitle(\"Principal Component Analysis\")\n\n\n\nRysunek 14.4: Wynik działania PCA dla pierwszych czterech składowych głównych\n\n\n\n\nWidzimy, że pierwsze dwie składowe PC1 i PC2, zwłaszcza gdy są używane razem, skutecznie rozróżniają lub oddzielają klasy. To może nas skłonić do oczekiwania, że ogólny problem klasyfikacji tych ziaren nie będzie szczególnie trudny.\nPrzypomnijmy, że PCA jest metodą bez nadzoru. Dla tych danych okazuje się, że składowe PCA, które wyjaśniają największe różnice w predyktorach, są również predyktorami klas. Jakie cechy wpływają na wydajność? Pakiet learntidymodels posiada funkcje, które mogą pomóc w wizualizacji najważniejszych cech dla każdego komponentu. Będziemy potrzebowali przygotowanego przepisu; krok PCA jest dodany w poniższym kodzie wraz z wywołaniem prep():\n\nKodlibrary(learntidymodels)\nbean_rec_trained %&gt;%\n  step_pca(all_numeric_predictors(), num_comp = 4) %&gt;% \n  prep() %&gt;% \n  plot_top_loadings(component_number &lt;= 4, n = 5) + \n  scale_fill_brewer(palette = \"Paired\") +\n  ggtitle(\"Principal Component Analysis\")\n\n\n\nRysunek 14.5: Ładunki predyktorów w transformacji PCA\n\n\n\n\nNajwyższe ładunki są w większości związane z grupą skorelowanych predyktorów pokazanych w lewej górnej części poprzedniego wykresu korelacji: obwód, powierzchnia, długość osi głównej i powierzchnia wypukła. Wszystkie one są związane z wielkością fasoli. Miary wydłużenia wydają się dominować w drugim komponencie PCA.\n\n14.2.2 PLS\nPartial Least Squares jest nadzorowaną wersją PCA. Próbuje znaleźć składowe, które jednocześnie maksymalizują zmienność predyktorów, a jednocześnie maksymalizują związek między tymi składowymi a wynikiem. Rysunek 14.6 przedstawia wyniki tej nieco zmodyfikowanej wersji kodu PCA:\n\nKodbean_rec_trained %&gt;%\n  step_pls(all_numeric_predictors(), outcome = \"class\", num_comp = 4) %&gt;%\n  plot_validation_results() + \n  ggtitle(\"Partial Least Squares\")\n\n\n\nRysunek 14.6: Oceny składowych PLS dla zbioru walidacyjnego fasoli, pokolorowane według klas\n\n\n\n\nPierwsze dwie składowe PLS wykreślone na Rysunek 14.6 są niemal identyczne z pierwszymi dwiema składowymi PCA! Pozostałe składowe różnią się nieco od PCA. Rysunek 14.7 wizualizuje ładunki, czyli najwyższe cechy dla każdej składowej.\n\nKodbean_rec_trained %&gt;%\n  step_pls(all_numeric_predictors(), outcome = \"class\", num_comp = 4) %&gt;%\n  prep() %&gt;% \n  plot_top_loadings(component_number &lt;= 4, n = 5, type = \"pls\") + \n  scale_fill_brewer(palette = \"Paired\") +\n  ggtitle(\"Partial Least Squares\")\n\n\n\nRysunek 14.7: Ładunki predyktorów w transformacji PLS\n\n\n\n\n\n14.2.3 ICA\nMetoda składowych niezależnych różni się nieco od PCA tym, że znajduje składowe, które są tak statystycznie niezależne od siebie, jak to tylko możliwe (w przeciwieństwie do bycia nieskorelowanym). Można o tym myśleć jako o rozdzielaniu informacji zamiast kompresji informacji jak w przypadku PCA. Użyjmy funkcji step_ica() do wykonania dekompozycji (patrz rysunku 16.9):\n\nKodbean_rec_trained %&gt;%\n  step_ica(all_numeric_predictors(), num_comp = 4) %&gt;%\n  plot_validation_results() + \n  ggtitle(\"Independent Component Analysis\")\n\n\n\nRysunek 14.8: Oceny komponentów ICA dla zbioru walidacyjnego fasoli, pokolorowane według klas\n\n\n\n\nAnaliza wyników nie wskazuje na wyraźne różnice między klasami w pierwszych kilku komponentach ICA. Te niezależne (lub tak niezależne, jak to możliwe) komponenty nie separują typów fasoli.\n\n14.2.4 UMAP\nUniform manifold approximation and projection jest podobna do innej znanej metody t-SNE służącej do nieliniowej redukcji wymiarów. W oryginalnej przestrzeni wielowymiarowej UMAP wykorzystuje opartą na odległości metodę najbliższych sąsiadów, aby znaleźć lokalne obszary danych, w których punkty z dużym prawdopodobieństwem są powiązane. Relacje między punktami danych są zapisywane jako model grafu skierowanego, gdzie większość punktów nie jest połączona. Stąd UMAP tłumaczy punkty w grafie na przestrzeń o zmniejszonym wymiarze. Aby to zrobić, algorytm posiada proces optymalizacji, który wykorzystuje entropię krzyżową do mapowania punktów danych do mniejszego zestawu cech, tak aby graf był dobrze przybliżony.\nPakiet embed zawiera funkcję krokową dla tej metody, zwizualizowaną na Rysunek 14.9.\n\nKodlibrary(embed)\n\n\n\nKodbean_rec_trained %&gt;%\n  step_umap(all_numeric_predictors(), num_comp = 4) %&gt;%\n  plot_validation_results() +\n  ggtitle(\"UMAP\")\n\n\n\nRysunek 14.9: Oceny komponentów UMAP dla zestawu walidacyjnego fasoli, pokolorowane według klas\n\n\n\n\nWidać wyraźne przestrzenie pomiędzy skupieniami, choć te dalej mogą być heterogeniczne.\nIstnieje też odmiana UMAP nadzorowana, która zamiast używać podobieństwa do określenia skupień w wielowymiarowej przestrzeni, używa etykiet.\n\nKodbean_rec_trained %&gt;%\n  step_umap(all_numeric_predictors(), outcome = \"class\", num_comp = 4) %&gt;%\n  plot_validation_results() +\n  ggtitle(\"UMAP (supervised)\")\n\n\n\nRysunek 14.10: Oceny komponentów UMAP (werjsa nadzorowana) dla zestawu walidacyjnego fasoli, pokolorowane według klas\n\n\n\n\nUMAP jest wydajną metodą redukcji przestrzeni cech. Jednak może być bardzo wrażliwa na dostrajanie parametrów (np. liczba sąsiadów). Z tego powodu pomocne byłoby eksperymentowanie z kilkoma parametrami, aby ocenić, jak wiarygodne są wyniki dla tych danych."
  },
  {
    "objectID": "dimensionality.html#modelowanie-ze-wstępną-ekstrakcją-cech",
    "href": "dimensionality.html#modelowanie-ze-wstępną-ekstrakcją-cech",
    "title": "\n14  Redukcja wymiarowości\n",
    "section": "\n14.3 Modelowanie ze wstępną ekstrakcją cech",
    "text": "14.3 Modelowanie ze wstępną ekstrakcją cech\nZarówno metoda PLS jak i UMAP są warte zbadania w połączeniu z różnymi modelami. Zbadajmy wiele różnych modeli z tymi technikami redukcji wymiarowości (wraz z brakiem jakiejkolwiek transformacji): jednowarstwowa sieć neuronowa, bagged trees, elastyczna analiza dyskryminacyjna (FDA), naiwny Bayes i regularyzowana analiza dyskryminacyjna (RDA).\n\n\n\n\nStworzymy serię specyfikacji modeli, a następnie użyjemy zestawu przepływów pracy do dostrojenia modeli w poniższym kodzie. Zauważ, że parametry modelu są dostrajane w połączeniu z parametrami receptury (np. rozmiar zredukowanego wymiaru, parametry UMAP).\n\nKodlibrary(baguette)\nlibrary(discrim)\n\nmlp_spec &lt;-\n  mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %&gt;%\n  set_engine('nnet') %&gt;%\n  set_mode('classification')\n\nbagging_spec &lt;-\n  bag_tree() %&gt;%\n  set_engine('rpart') %&gt;%\n  set_mode('classification')\n\nfda_spec &lt;-\n  discrim_flexible(\n    prod_degree = tune()\n  ) %&gt;%\n  set_engine('earth')\n\nrda_spec &lt;-\n  discrim_regularized(frac_common_cov = tune(), frac_identity = tune()) %&gt;%\n  set_engine('klaR')\n\nbayes_spec &lt;-\n  naive_Bayes() %&gt;%\n  set_engine('klaR')\n\n\nPotrzebujemy również receptur dla metod redukcji wymiarowości, które będziemy testować. Zacznijmy od bazowego przepisu bean_rec, a następnie rozszerzmy go o różne kroki redukcji wymiarowości:\n\nKodbean_rec &lt;-\n  recipe(class ~ ., data = bean_train) %&gt;%\n  step_zv(all_numeric_predictors()) %&gt;%\n  step_orderNorm(all_numeric_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\npls_rec &lt;- \n  bean_rec %&gt;% \n  step_pls(all_numeric_predictors(), outcome = \"class\", num_comp = tune())\n\numap_rec &lt;-\n  bean_rec %&gt;%\n  step_umap(\n    all_numeric_predictors(),\n    outcome = \"class\",\n    num_comp = tune(),\n    neighbors = tune(),\n    min_dist = tune()\n  )\n\n\nPo raz kolejny pakiet workflowsets bierze reguły wstępnego przetwarzani oraz modele i krzyżuje je. Opcja kontrolna parallel_over jest ustawiona tak, aby przetwarzanie równoległe mogło działać jednocześnie w różnych kombinacjach dostrajania parametrów. Funkcja workflow_map() stosuje przeszukiwanie siatki w celu optymalizacji parametrów modelu/przetwarzania wstępnego (jeśli istnieją) w 10 kombinacjach parametrów. Obszar pod krzywą ROC jest szacowany na zbiorze walidacyjnym.\n\nKodctrl &lt;- control_grid(parallel_over = \"resamples\")\n\nbean_res &lt;- \n  workflow_set(\n    preproc = list(basic = class ~., pls = pls_rec, umap = umap_rec), \n    models = list(bayes = bayes_spec, fda = fda_spec,\n                  rda = rda_spec, bag = bagging_spec,\n                  mlp = mlp_spec)\n  ) %&gt;% \n  workflow_map(\n    verbose = TRUE,\n    seed = 1603,\n    resamples = bean_val,\n    grid = 10,\n    metrics = metric_set(roc_auc),\n    control = ctrl\n  )\n\n\n\nKodload(\"models/bean_res.rda\")\nautoplot(\nbean_res,\nrank_metric = \"roc_auc\",  # &lt;- how to order models\nmetric = \"roc_auc\",       # &lt;- which metric to visualize\nselect_best = TRUE     # &lt;- one point per workflow\n) +\ngeom_text(aes(y = mean - 0.02, label = wflow_id), angle = 90, hjust = 0.1) +\nlims(y = c(0.9, 1))\n\n\n\nRysunek 14.11: Obszar pod krzywą ROC ze zbioru walidacyjnego\n\n\n\n\nMożemy uszeregować modele według ich oszacowań obszaru pod krzywą ROC w zbiorze walidacyjnym:\n\nKodrankings &lt;- \n  rank_results(bean_res, select_best = TRUE) %&gt;% \n  mutate(method = map_chr(wflow_id, ~ str_split(.x, \"_\", simplify = TRUE)[1])) \n\ntidymodels_prefer()\nfilter(rankings, rank &lt;= 5) %&gt;% dplyr::select(rank, mean, model, method)\n\n# A tibble: 5 × 4\n   rank  mean model               method\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt; \n1     1 0.995 mlp                 basic \n2     2 0.995 discrim_regularized pls   \n3     3 0.994 naive_Bayes         pls   \n4     4 0.994 mlp                 pls   \n5     5 0.994 discrim_flexible    basic \n\n\nZ tych wyników jasno wynika, że większość modeli daje bardzo dobre wyniki; niewiele jest tu złych wyborów. Dla demonstracji użyjemy modelu RDA z cechami PLS jako modelu końcowego.\n\nKodrda_res &lt;- \n  bean_res %&gt;% \n  extract_workflow(\"pls_rda\") %&gt;% \n  finalize_workflow(\n    bean_res %&gt;% \n      extract_workflow_set_result(\"pls_rda\") %&gt;% \n      select_best(metric = \"roc_auc\")\n  ) %&gt;% \n  last_fit(split = bean_split, metrics = metric_set(roc_auc))\n\nrda_wflow_fit &lt;- extract_workflow(rda_res)\n\ncollect_metrics(rda_res)\n\n# A tibble: 1 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 roc_auc hand_till      0.995 Preprocessor1_Model1\n\n\n\n\n\n\nKoklu, Murat, i Ilker Ali Ozkan. 2020. „Multiclass Classification of Dry Beans Using Computer Vision and Machine Learning Techniques”. Computers and Electronics in Agriculture 174 (lipiec): 105507. https://doi.org/10.1016/j.compag.2020.105507.\n\n\nMingqiang, Yang, Kpalma Kidiyo, i Ronsin Joseph. 2008. „A Survey of Shape Feature Extraction Techniques”. W. InTech. https://doi.org/10.5772/6237."
  },
  {
    "objectID": "imbalance.html#próbkowanie-danych",
    "href": "imbalance.html#próbkowanie-danych",
    "title": "\n15  Nierównowaga klas w zadaniu klasyfikacyjnym\n",
    "section": "\n15.1 Próbkowanie danych",
    "text": "15.1 Próbkowanie danych\nJednym ze sposobów na złagodzenie tego problemu jest podpróbkowanie danych. Istnieje wiele sposobów, aby to zrobić, ale najprostszym jest próbkowanie w dół (undersample) danych klasy większościowej, aż wystąpi ona z taką samą częstotliwością jak klasa mniejszościowa. Choć może się to wydawać sprzeczne z intuicją, wyrzucenie dużego procentu danych może być skuteczne w tworzeniu użytecznego modelu, który potrafi rozpoznać zarówno klasy większościowe, jak i mniejszościowe. W niektórych przypadkach oznacza to nawet, że ogólna wydajność modelu jest lepsza (np. poprawiony obszar pod krzywą ROC). Podpróbkowanie prawie zawsze daje modele, które są lepiej skalibrowane, co oznacza, że rozkłady prawdopodobieństwa klas są lepiej zachowane. W rezultacie, domyślne odcięcie 50% daje znacznie większe prawdopodobieństwo uzyskania lepszych wartości czułości i specyficzności niż w innym przypadku.\n\n\n\n\nIstnieją również techniki oversampling, które sprowadzają klasy mniejszościowe do liczebności takiej samej jak klasa większościowa (lub jej części) poprzez odpowiednie próbkowanie istniejących obserwacji lub też (jak to jest w przypadku metody SMOTE) tworzy się syntetyczne obserwacje podobne do już istniejących w klasie mniejszościowej. W pakiecie themis można znaleźć różne techniki próbkowania w górę: step_upsample(), step_smote(), step_bsmote(method = 1), step_bsmote(method = 2), step_adasyn(), step_rose() oraz kilka technik próbkowania w dół: step_downsample(), step_nearmiss() i step_tomek().\nZbadajmy działanie próbkowania używając themis::step_rose() w przepisie dla symulowanych danych. Wykorzystuje ona metodę ROSE (ang. Random Over Sampling Examples) z Menardi i Torelli (2012). Jest to przykład strategii oversampling.\nW zakresie przepływu pracy:\n\nNiezwykle ważne jest, aby subsampling występował wewnątrz resamplingu. W przeciwnym razie proces resamplingu może dać słabe oszacowania wydajności modelu.\nProces próbkowania powinien być stosowany tylko do zbioru analiz. Zestaw analiz powinien odzwierciedlać częstość zdarzeń widzianych “w naturze” i z tego powodu argument skip w step_downsample() i innych krokach receptury próbkowania ma domyślnie wartość TRUE.\n\nOto prosta recepta implementująca oversampling:\n\nKodlibrary(themis)\nimbal_rec &lt;- \n  recipe(Class ~ ., data = imbal_data) %&gt;%\n  step_rose(Class, seed = 1234)\n\n\nJako modelu użyjmy modelu kwadratowej analizy dyskryminacyjnej (QDA). Z poziomu pakietu discrim, model ten można określić za pomocą:\n\nKodlibrary(discrim)\nqda_mod &lt;- \n  discrim_regularized(frac_common_cov = 0, frac_identity = 0) %&gt;% \n  set_engine(\"klaR\")\n\n\nAby utrzymać te obiekty związane ze sobą, połączymy je w ramach przepływu pracy:\n\nKodqda_rose_wflw &lt;- \n  workflow() %&gt;% \n  add_model(qda_mod) %&gt;% \n  add_recipe(imbal_rec)\n\nqda_rose_wflw\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: discrim_regularized()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_rose()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRegularized Discriminant Model Specification (classification)\n\nMain Arguments:\n  frac_common_cov = 0\n  frac_identity = 0\n\nComputational engine: klaR \n\n\nDo oceny jakości dopasowania modelu zastosujemy 10-krotny sprawdzian krzyżowy z powtórzeniami:\n\nKodset.seed(5732)\ncv_folds &lt;- vfold_cv(imbal_data, strata = \"Class\", repeats = 5)\n\n\nAby zmierzyć wydajność modelu, użyjmy dwóch metryk:\n\nObszar pod krzywą ROC;\nWskaźnik J (statystyka Youdena J) określony jako czułość + specyficzność - 1. Wartości bliskie jeden są najlepsze.\n\nJeśli model jest źle skalibrowany, wartość krzywej ROC może nie wykazywać zmniejszonej wydajności. Jednak wskaźnik J byłby niższy dla modeli z patologicznymi rozkładami prawdopodobieństw klas. Do obliczenia tych metryk zostanie użyty pakiet yardstick.\n\nKodcls_metrics &lt;- metric_set(roc_auc, j_index)\n\n\n\nKodset.seed(2180)\nqda_rose_res &lt;- fit_resamples(\n  qda_rose_wflw, \n  resamples = cv_folds, \n  metrics = cls_metrics\n)\n\ncollect_metrics(qda_rose_res)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 j_index binary     0.768    50 0.0214  Preprocessor1_Model1\n2 roc_auc binary     0.951    50 0.00509 Preprocessor1_Model1\n\n\nJak wyglądają wyniki bez użycia ROSE? Możemy stworzyć kolejny przepływ pracy i dopasować model QDA dla tych samych foldów:\n\nKodqda_wflw &lt;- \n  workflow() %&gt;% \n  add_model(qda_mod) %&gt;% \n  add_formula(Class ~ .)\n\nset.seed(2180)\nqda_only_res &lt;- fit_resamples(qda_wflw, resamples = cv_folds, metrics = cls_metrics)\n\ncollect_metrics(qda_only_res)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 j_index binary     0.250    50 0.0288  Preprocessor1_Model1\n2 roc_auc binary     0.953    50 0.00479 Preprocessor1_Model1\n\n\nWygląda na to, że próbkowanie metodą ROSE bardzo pomogło, zwłaszcza w przypadku indeksu J. Metody próbkowania nierównowagi klasowej mają tendencję do znacznej poprawy metryk opartych na twardych przewidywaniach klasowych (tj. przewidywaniach kategorycznych), ponieważ domyślne odcięcie ma tendencję do lepszej równowagi pomiędzy czułością i specyficznością.\nWykreślmy metryki dla każdej próbki, aby zobaczyć, jak zmieniły się poszczególne wyniki.\n\nKodno_sampling &lt;- \n  qda_only_res %&gt;% \n  collect_metrics(summarize = FALSE) %&gt;% \n  dplyr::select(-.estimator) %&gt;% \n  mutate(sampling = \"no_sampling\")\n\nwith_sampling &lt;- \n  qda_rose_res %&gt;% \n  collect_metrics(summarize = FALSE) %&gt;% \n  dplyr::select(-.estimator) %&gt;% \n  mutate(sampling = \"rose\")\n\nbind_rows(no_sampling, with_sampling) %&gt;% \n  mutate(label = paste(id2, id)) %&gt;%  \n  ggplot(aes(x = sampling, y = .estimate, group = label)) + \n  geom_line(alpha = .4) + \n  facet_wrap(~ .metric, scales = \"free_y\")\n\n\n\nRysunek 15.1: Porównanie dopasowania przed i po ROSE dla obu metryk\n\n\n\n\nJak widać na podstawie Rysunek 15.1 szczególnie w kontekście miar, które wykorzystują twardy podział (czyli zdefiniowany przez parametr odcięcia) nastąpiła znaczna poprawa."
  },
  {
    "objectID": "imbalance.html#zagrożenia",
    "href": "imbalance.html#zagrożenia",
    "title": "\n15  Nierównowaga klas w zadaniu klasyfikacyjnym\n",
    "section": "\n15.2 Zagrożenia",
    "text": "15.2 Zagrożenia\nPierwszą komplikacją związaną z próbkowaniem jest połączenie jej z przetwarzaniem wstępnym. Czy próbkowanie powinno mieć miejsce przed czy po przetwarzaniu wstępnym? Na przykład, jeśli zmniejszamy próbkę danych i używamy PCA do ekstrakcji cech, czy ładunki powinny być oszacowane z całego zbioru treningowego? Estymacja ta byłaby potencjalnie lepsza, ponieważ wykorzystywany byłby cały zbiór treningowy, ale może się zdarzyć, że podpróbka uchwyci niewielką część przestrzeni PCA. Nie ma żadnej oczywistej odpowiedzi ale zaleca się stosować próbkowanie przed procedurą wstępnego przetwarzania.\nInne zagrożenia to:\n\nSłabo reprezentowane kategorie w zmiennych czynnikowych (predyktorach) mogą przekształcić się w predyktory o zerowej wariancji lub mogą być całkowicie wyrywane z modelu.\nJeśli używasz grid_search() do określenia siatki wyszukiwania, może się zdarzyć, że dane, które są używane do określenia siatki po próbkowaniu, nie wypełniają pełnych zakresów zmienności hiperparametrów. W większości przypadków nie ma to znaczenia, ale czasami może doprowadzić do uzyskania nieoptymalnej siatki.\nW przypadku niektórych modeli, które wymagają więcej próbek niż parametrów, zmniejszenie rozmiaru próbki może uniemożliwić dopasowanie modelu.\n\n\n\n\n\nMenardi, Giovanna, i Nicola Torelli. 2012. „Training and Assessing Classification Rules with Imbalanced Data”. Data Mining and Knowledge Discovery 28 (1): 92–122. https://doi.org/10.1007/s10618-012-0295-5."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "3.4 Evaluating Forecast Accuracy |\nForecasting: Principles and\nPractice (2nd Ed). n.d.\n\n\nBellon-Maurel, Véronique, Elvira Fernandez-Ahumada, Bernard Palagos,\nJean-Michel Roger, and Alex McBratney. 2010. “Critical Review of\nChemometric Indicators Commonly Used for Assessing the Quality of the\nPrediction of Soil Attributes by NIR Spectroscopy.”\nTrAC Trends in Analytical Chemistry 29 (9): 1073–81. https://doi.org/10.1016/j.trac.2010.05.006.\n\n\nBohachevsky, Ihor O., Mark E. Johnson, and Myron L. Stein. 1986.\n“Generalized Simulated Annealing for Function\nOptimization.” Technometrics 28 (3): 209–17. https://doi.org/10.1080/00401706.1986.10488128.\n\n\nBolstad, Benjamin Milo. 2004. Low-Level Analysis of\nHigh-density Oligonucleotide Array Data:\nBackground, Normalization and\nSummarization. University of California,\nBerkeley.\n\n\nBooth, Gordon D., George E. P. Box, William G. Hunter, and J. Stuart\nHunter. 1979. “Statistics for Experimenters: An Introduction to\nDesign, Data Analysis, and Model Building.” Journal of the\nAmerican Statistical Association 74 (367): 731. https://doi.org/10.2307/2287009.\n\n\nCouch, Simon P., Andrew P. Bray, Chester Ismay, Evgeni Chasnovski,\nBenjamin S. Baumer, and Mine Çetinkaya-Rundel. 2021.\n“Infer: An r\nPackage for Tidyverse-Friendly Statistical Inference” 6: 3661. https://doi.org/10.21105/joss.03661.\n\n\nCraig-Schapiro, Rebecca, Max Kuhn, Chengjie Xiong, Eve H. Pickering,\nJingxia Liu, Thomas P. Misko, Richard J. Perrin, et al. 2011.\n“Multiplexed Immunoassay Panel Identifies Novel CSF Biomarkers for\nAlzheimer’s Disease Diagnosis and Prognosis.” Edited by Ashley I.\nBush. PLoS ONE 6 (4): e18850. https://doi.org/10.1371/journal.pone.0018850.\n\n\nDavison, A. C., and D. V. Hinkley. 1997. “Bootstrap Methods and\nTheir Application,” October. https://doi.org/10.1017/cbo9780511802843.\n\n\nFranses, Philip Hans. 2016. “A Note on the Mean Absolute\nScaled Error.” International Journal of\nForecasting 32 (1): 20–22. https://doi.org/10.1016/j.ijforecast.2015.03.008.\n\n\nFrazier, Peter I. 2018. “A Tutorial on Bayesian\nOptimization.” https://doi.org/10.48550/ARXIV.1807.02811.\n\n\nFriedman, Jerome H. 2001. “Greedy Function Approximation: A\nGradient Boosting Machine.” The Annals of Statistics 29\n(5). https://doi.org/10.1214/aos/1013203451.\n\n\nGentleman, Robert, Vincent Carey, Wolfgang Huber, Sandrine Dudoit, and\nRafael Irizarry. 2005. Bioinformatics and Computational\nBiology Solutions Using R and Bioconductor.\nSpringer.\n\n\nHill, Andrew A, Peter LaPan, Yizheng Li, and Steve Haney. 2007.\n“Impact of Image Segmentation on High-Content Screening Data\nQuality for SK-BR-3 Cells.” BMC Bioinformatics 8 (1). https://doi.org/10.1186/1471-2105-8-340.\n\n\nHosmer, David W., and Stanley Lemeshow. 2000. Applied Logistic\nRegression. John Wiley & Sons, Inc. https://doi.org/10.1002/0471722146.\n\n\nHyndman, Rob J., and Anne B. Koehler. 2006. “Another Look at\nMeasures of Forecast Accuracy.” International Journal of\nForecasting 22 (4): 679–88. https://doi.org/10.1016/j.ijforecast.2006.03.001.\n\n\nHyndman, Robin John, and George Athanasopoulos. 2018. Forecasting:\nPrinciples and Practice. 2nd ed. Australia: OTexts.\n\n\nJohnson, Dan, Phoebe Eckart, Noah Alsamadisi, Hilary Noble, Celia\nMartin, and Rachel Spicer. 2018. “Polar Auxin Transport Is\nImplicated in Vessel Differentiation and Spatial Patterning During\nSecondary Growth in Populus.” American Journal\nof Botany 105 (2): 186–96. https://doi.org/10.1002/ajb2.1035.\n\n\nJoseph, V. Roshan. 2022. “Optimal Ratio for Data\nSplitting.” Statistical Analysis and Data Mining: The ASA\nData Science Journal 15 (4): 531–38. https://doi.org/10.1002/sam.11583.\n\n\nJoseph, V. Roshan, Evren Gul, and Shan Ba. 2015. “Maximum\nProjection Designs for Computer Experiments.” Biometrika\n102 (2): 371–80. https://doi.org/10.1093/biomet/asv002.\n\n\nKhun, M., and K. Johnson. 2013. Applied Predictive\nModeling. New York: Springer.\n\n\nKim, Jungsu, Jacob M. Basak, and David M. Holtzman. 2009. “The\nRole of Apolipoprotein E in Alzheimer’s Disease.” Neuron\n63 (3): 287–303. https://doi.org/10.1016/j.neuron.2009.06.026.\n\n\nKirkpatrick, S., C. D. Gelatt, and M. P. Vecchi. 1983.\n“Optimization by Simulated Annealing.” Science 220\n(4598): 671–80. https://doi.org/10.1126/science.220.4598.671.\n\n\nKoklu, Murat, and Ilker Ali Ozkan. 2020. “Multiclass\nClassification of Dry Beans Using Computer Vision and Machine Learning\nTechniques.” Computers and Electronics in Agriculture\n174 (July): 105507. https://doi.org/10.1016/j.compag.2020.105507.\n\n\nKuhn, Max, and Kjell Johnson. 2019. Feature Engineering and\nSelection. Chapman; Hall/CRC. https://doi.org/10.1201/9781315108230.\n\n\n———. 2021. Feature Engineering and\nSelection: A Practical Approach for\nPredictive Models. Taylor & Francis\nGroup.\n\n\nKuhn, Max, and Hadley Wickham. 2020. “Tidymodels: A Collection of\nPackages for Modeling and Machine Learning Using Tidyverse\nPrinciples.” https://www.tidymodels.org.\n\n\nKvalseth, Tarald O. 1985. “Cautionary Note about\nR2.” The American Statistician 39 (4):\n279–85. https://doi.org/10.2307/2683704.\n\n\nLaarhoven, Peter J. M. van, and Emile H. L. Aarts. 1987.\n“Simulated Annealing.” In, 7–15. Springer Netherlands. https://doi.org/10.1007/978-94-015-7744-1_2.\n\n\nMaron, Oded, and Andrew Moore. 1993. “Hoeffding Races:\nAccelerating Model Selection Search for Classification and\nFunction Approximation.” In Advances in Neural Information\nProcessing Systems, edited by J. Cowan, G. Tesauro, and J.\nAlspector. Vol. 6. Morgan-Kaufmann.\n\n\nMcKay, M. D., R. J. Beckman, and W. J. Conover. 1979. “A\nComparison of Three Methods for Selecting Values of Input Variables in\nthe Analysis of Output from a Computer Code.”\nTechnometrics 21 (2): 239. https://doi.org/10.2307/1268522.\n\n\nMenardi, Giovanna, and Nicola Torelli. 2012. “Training and\nAssessing Classification Rules with Imbalanced Data.” Data\nMining and Knowledge Discovery 28 (1): 92–122. https://doi.org/10.1007/s10618-012-0295-5.\n\n\nMingqiang, Yang, Kpalma Kidiyo, and Ronsin Joseph. 2008. “A Survey\nof Shape Feature Extraction Techniques.” In. InTech. https://doi.org/10.5772/6237.\n\n\nSchulz, Eric, Maarten Speekenbrink, and Andreas Krause. 2016. “A\nTutorial on Gaussian Process Regression: Modelling, Exploring, and\nExploiting Functions.” http://dx.doi.org/10.1101/095190.\n\n\nShahriari, Bobak, Kevin Swersky, Ziyu Wang, Ryan P. Adams, and Nando de\nFreitas. 2016. “Taking the Human Out of the Loop: A Review of\nBayesian Optimization.” Proceedings of the IEEE 104 (1):\n148–75. https://doi.org/10.1109/jproc.2015.2494218.\n\n\nShewry, M. C., and H. P. Wynn. 1987. “Maximum Entropy\nSampling.” Journal of Applied Statistics 14 (2): 165–70.\nhttps://doi.org/10.1080/02664768700000020.\n\n\nThomas, Rachel, and David Uminsky. 2020. “The Problem with Metrics\nIs a Fundamental Problem for AI.” https://doi.org/10.48550/ARXIV.2002.08512.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the Tidyverse.” Journal of Open Source\nSoftware 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nXu, Qing-Song, and Yi-Zeng Liang. 2001. “Monte Carlo Cross\nValidation.” Chemometrics and Intelligent Laboratory\nSystems 56 (1): 1–11. https://doi.org/10.1016/s0169-7439(00)00122-2.\n\n\nYeh, I.-Cheng. 2006. “Analysis of Strength of\nConcrete Using Design of Experiments and\nNeural Networks.” Journal of Materials in Civil\nEngineering 18 (4): 597–604. https://doi.org/10.1061/(ASCE)0899-1561(2006)18:4(597)."
  }
]