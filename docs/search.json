[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Metody walidacji modeli statystycznych",
    "section": "",
    "text": "Wprowadzenie\nNiniejsza książka została przygotowana z myślą o studentach kierunków Inżynierii i Analizy Danych oraz Matematyka. Ma ona posłużyć w nauce szeroko rozumianej walidacji modeli statystycznych. Pojęcie zostanie dosyć szeroko potraktowane, dlatego w książce nawiązuję do metod próbkowania (ang. resampling), stosowanych do oceny jakości modelu oraz w procesie tuningowaniu modelu, jak również do miar stosowanych do oceny dopasowania modeli. Ponieważ książka ta ma stanowić przegląd metod używanych w ocenie jak dobrze model przewiduje nieznane wartości, to zostanie również uzupełniona o analizę wrażliwości pozwalającą na określenie jakie poszczególne czynniki (predyktory) i w jaki sposób wpływają na predykcję.\nW statystyce walidacja modelu jest zadaniem polegającym na ocenie, czy wybrany model statystyczny jest odpowiedni, czy nie. Często we wnioskowaniu statystycznym wnioski z modeli, które wydają się pasować do danych, mogą być błędne, co powoduje, że badacze nie poznają rzeczywistej wartości swojego modelu. Aby temu zapobiec, stosuje się walidację modelu, aby sprawdzić, czy model statystyczny posiada tzw. zdolność generalizacji. Owa zdolność polega na możliwości poprawnego przewidywania wartości wyjściowej nie tylko dla obserwacji ze zbioru uczącego, ale również z testowego, bez znaczącej straty jakości wspomnianej predykcji. Tego tematu nie należy mylić z blisko związanym zadaniem wyboru modelu, czyli procesem rozróżniania wielu modeli kandydujących: walidacja modelu nie dotyczy tak bardzo konceptualnej konstrukcji modeli, lecz testuje jedynie spójność pomiędzy wybranym modelem a jego deklarowanymi wynikami. Nadrzędnym celem walidacji modelu jest chęć poprawienia jego właściwości.\n\n\n\n\n\nWalidacja modelu występuje w wielu odmianach, a konkretna metoda walidacji modelu stosowana przez badacza jest często uwarunkowana jego projektem badawczym. Oznacza to, że nie ma jednej uniwersalnej metody walidacji modelu. Na przykład, jeśli badacz pracuje z bardzo ograniczonym zestawem danych, ale ma silne założenia wstępne dotyczące danych, może rozważyć walidację dopasowania swojego modelu poprzez zastosowanie metod bayesowskich i testowanie dopasowania modelu przy użyciu różnych rozkładów wstępnych. Jeśli jednak badacz ma dużo danych i testuje wiele zagnieżdżonych modeli, warunki te mogą sprzyjać walidacji krzyżowej i ewentualnie testowi “leave one out”. Są to dwa abstrakcyjne przykłady i każda rzeczywista walidacja modelu będzie musiała rozważyć znacznie więcej zawiłości niż tu opisano, ale przykład ten ilustruje, że metody walidacji modelu zawsze będą miały charakter poszlakowy. Ogólnie rzecz biorąc, modele mogą być walidowane przy użyciu istniejących danych lub przy użyciu nowych danych, a obie metody są omówione w kolejnych rozdziałach.\nNiezbędnym narzędziem w ocenie jakości dopasowania modeli będą metryki, czy inne narzędzia stosowane w ocenie poprawności modelu jak wykresy diagnostyczne, czy macierze klasyfikacji (ang. confusion matrix). W zależności od realizowanego zadania inne metryki mogą być wykorzystywane. I tak w zadaniach klasyfikacyjnych, w zależności od liczby klas zmiennej wyjściowej, będziemy stosowali inne miary do oceny modelu. Również w modelach regresyjnych istnieje cała gama miar pomagających ocenić dopasowanie. Jeszcze inne miary służą do ewaluacji metod nienadzorowanego uczenia maszynowego. Istnieje bowiem wiele technik określenia stopnia jednorodności powstałych skupień, a jednocześnie odpowiedzi napytanie czy wybrana została właściwa liczba grup."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Ekosystem",
    "section": "",
    "text": "Książka zawiera szereg przykładów ilustrujących zasadę działania poszczególnych technik walidacji modeli. Ponieważ od wielu lat używam języka R do budowania modeli uczenia maszynowego, to również w tej książce znajdą się implementacje metod ewaluacji modeli wykonane w R. Spośród trzech dominujących ekosystemów w języku R do budowania modeli uczenia maszynowego, czyli caret1,mlr32 i tidymodels3 postanowiłem postawić na ten ostatn i. Stanowi on przeniesienie filozofii “tidy data” realizowanej w pakiecie tidyverse4 na modele ML. tidymodels jest meta-pakietem zawierającym w sob ie:1 biblioteka napisana przez Maxa Kuhna, która niestety nie dostaje już wsparcia - https://topepo.github.io/caret/2 jedna biblioteka z całego ekosystemu przygotowanego głównie przez niemieckich programistów Lars Kotthoff, Raphael Sonabend, Michel Lang, Bernd Bischl. Jej największą zaletą jest fakt, iż wszelkie operacje na danych są dokonywane w formacie data.table uznawanym za najszybszy - https://mlr3book.mlr-org.com3 https://www.tidymodels.org4 pakiet został stworzony przez Julie Silge i Maxa Kuhna, a ich motto podane podczas prezentacji biblioteki brzmiało Whenever possible, the software should be able to protect users from committing mistakes. Software should make it easy for users to do the right thing\n\n\nbroom - przekształcanie obiektów statystycznych w tibble;\ndials - narzędzia do tuningowania parametrów modelu;\ndplyr - narzędzia do manipulacji danymi;\nggplot2 - narzędzia do wizualizacji;\ninfer - narzędzia do wnioskowania statystycznego;\nmodeldata - pakiet przykładowych danych do budowania modeli;\nparsnip - narzędzia do tworzenia i manipulowania funkcjami powszechnie używanymi podczas modelowania;\npurrr - zestaw narzędzi do programowania funkcyjnego;\nrecipes - narzędzie do tworzenia modeli;\nrsample - narzędzie do resamplingu;\ntibble - narzędzie do operacji na ramkach danych;\ntidyr - narzędzie do oczyszczania o transformacji danych;\ntune - narzędzie do tuningu hiperparametrów modelu;\nworkflows - narzędzia do zarządzania procesem uczenia modeli;\nworkflowsets - narzędzie do tworzenia zestawów workflow;\nyardstick - narzędzia do oceny dopasowania modelu.\n\nOprócz wspomnianych pakietów w ramach tidymodels występują także: applicable, baguette, butcher, discrim, embed, hardhat, corrr, rules, text recipes, tidypredict, modeldb i tidyposterior.\n\n\n\nWybór padł na tidymodels ponieważ filozofia tworzenia i walidacji modeli przypominająca pieczenie ciasta bardzo mi przypadła do gustu."
  },
  {
    "objectID": "modeling.html#typy-modeli",
    "href": "modeling.html#typy-modeli",
    "title": "\n2  Modelowanie statystyczne\n",
    "section": "\n2.1 Typy modeli",
    "text": "2.1 Typy modeli\nZanim przejdziemy dalej, opiszmy taksonomię rodzajów modeli, pogrupowanych według przeznaczenia. Taksonomia ta informuje zarówno o tym, jak model jest używany, jak i o wielu aspektach tego, jak model może być tworzony lub oceniany. Chociaż lista ta nie jest wyczerpująca, większość modeli należy do przynajmniej jednej z tych kategorii:\n\n2.1.1 Modele opisowe\nCelem modelu opisowego jest opis lub zilustrowanie pewnych cech danych. Analiza może nie mieć innego celu niż wizualne podkreślenie jakiegoś trendu lub artefaktu (lub defektu) w danych.\nNa przykład, pomiary RNA na dużą skalę są możliwe od pewnego czasu przy użyciu mikromacierzy. Wczesne metody laboratoryjne umieszczały próbkę biologiczną na małym mikrochipie. Bardzo małe miejsca na chipie mogą mierzyć sygnał oparty na bogactwie specyficznej sekwencji RNA. Chip zawierałby tysiące (lub więcej) wyników, z których każdy byłby kwantyfikacją RNA związanego z procesem biologicznym. Jednakże na chipie mogłyby wystąpić problemy z jakością, które mogłyby prowadzić do słabych wyników. Na przykład, odcisk palca przypadkowo pozostawiony na części chipa mógłby spowodować niedokładne pomiary podczas skanowania.\n\n\nRysunek 2.1: Ocena jakości odczytów z mikroczipa\n\n\nWczesną metodą oceny takich zagadnień były modele na poziomie sondy, czyli PLM (Bolstad 2004). Tworzono model statystyczny, który uwzględniał pewne różnice w danych, takie jak chip, sekwencja RNA, typ sekwencji i tak dalej. Jeśli w danych występowałyby inne, nieznane czynniki, to efekty te zostałyby uchwycone w resztach modelu. Gdy reszty zostały wykreślone według ich lokalizacji na chipie, dobrej jakości chip nie wykazywałby żadnych wzorców. W przypadku wystąpienia problemu, pewien rodzaj wzorca przestrzennego byłby dostrzegalny. Często typ wzorca sugerowałby problem (np. odcisk palca) oraz możliwe rozwiązanie (wytarcie chipa i ponowne skanowanie, powtórzenie próbki, itp.) Rysunek 2.1 pokazuje zastosowanie tej metody dla dwóch mikromacierzy (Gentleman i in. 2005). Obrazy pokazują dwie różne wartości kolorystyczne; obszary, które są ciemniejsze to miejsca, gdzie intensywność sygnału była większa niż oczekuje model, podczas gdy jaśniejszy kolor pokazuje wartości niższe niż oczekiwane. Lewy panel pokazuje w miarę losowy wzór, podczas gdy prawy panel wykazuje niepożądany artefakt w środku chipa.\n\n2.1.2 Modele do wnioskowania\nCelem modelu inferencyjnego jest podjęcie decyzji dotyczącej pytania badawczego lub sprawdzenie określonej hipotezy, podobnie jak w przypadku testów statystycznych. Model inferencyjny zaczyna się od wcześniej zdefiniowanego przypuszczenia lub pomysłu na temat populacji i tworzy wniosek statystyczny, taki jak szacunek przedziału lub odrzucenie hipotezy.\nNa przykład, celem badania klinicznego może być potwierdzenie, że nowa terapia pozwala wydłużyć życie w porównaniu z istniejącą terapią lub brakiem leczenia. Jeśli kliniczny punkt końcowy dotyczyłby przeżycia pacjenta, hipoteza zerowa mogłaby brzmieć, że nowa terapia ma równą lub niższą medianę czasu przeżycia, a hipoteza alternatywna, że nowa terapia ma wyższą medianę czasu przeżycia. Jeśli ta próba byłaby oceniana przy użyciu tradycyjnego testowania istotności hipotezy zerowej poprzez modelowanie, testowanie istotności dałoby wartość \\(p\\) przy użyciu jakiejś wcześniej zdefiniowanej metodologii opartej na zestawie założeń. Małe wartości dla wartości \\(p\\) w wynikach modelu wskazywałyby na istnienie przesłanek, że nowa terapia pomaga pacjentom żyć dłużej. Duże wartości \\(p\\) w wynikach modelu wskazywałyby, że nie udało się wykazać takiej różnicy; ten brak przesłanek mógłby wynikać z wielu powodów, w tym z tego, że terapia nie działa.\nJakie są ważne aspekty tego typu analizy? Techniki modelowania inferencyjnego zazwyczaj dają pewien rodzaj danych wyjściowych o charakterze probabilistycznym, takich jak wartość \\(p\\), przedział ufności lub prawdopodobieństwo a posteriori. Zatem, aby obliczyć taką wielkość, należy przyjąć formalne założenia probabilistyczne dotyczące danych i procesów, które je wygenerowały. Jakość wyników modelowania statystycznego w dużym stopniu zależy od tych wcześniej określonych założeń, jak również od tego, w jakim stopniu obserwowane dane wydają się z nimi zgadzać. Najbardziej krytycznymi czynnikami są tutaj założenia teoretyczne: “Jeśli moje obserwacje były niezależne, a reszty mają rozkład X, to statystyka testowa Y może być użyta do uzyskania wartości \\(p\\). W przeciwnym razie wynikowa wartość \\(p\\) może być niewłaściwa.”\nJednym z aspektów analiz inferencyjnych jest to, że istnieje tendencja do opóźnionego sprzężenia zwrotnego w zrozumieniu, jak dobrze dane odpowiadają założeniom modelu. W naszym przykładzie badania klinicznego, jeśli znaczenie statystyczne (i kliniczne) wskazuje, że nowa terapia powinna być dostępna do stosowania przez pacjentów, mogą minąć lata zanim zostanie ona zastosowana w terenie i zostanie wygenerowana wystarczająca ilość danych do niezależnej oceny, czy pierwotna analiza statystyczna doprowadziła do podjęcia właściwej decyzji.\n\n2.1.3 Modele predykcyjne\nCzasami modelowania używamy w celu uzyskania jak najdokładniejszej prognozy dla nowych danych. W tym przypadku głównym celem jest, aby przewidywane wartości (ang. prediction) miały najwyższą możliwą zgodność z prawdziwą wartością (ang. observed).\nProstym przykładem może być przewidywanie przez sprzedającego książki, ile egzemplarzy danej książki powinno być wysłanych do jego sklepu w następnym miesiącu. Nadmierna prognoza powoduje marnowanie miejsca i pieniędzy z powodu nadmiaru książek. Jeśli przewidywanie jest mniejsze niż powinno, następuje utrata potencjału i mniejszy zysk.\nCelem tego typu modeli jest raczej estymacja niż wnioskowanie. Na przykład nabywca zwykle nie jest zainteresowany pytaniem typu “Czy w przyszłym miesiącu sprzedam więcej niż 100 egzemplarzy książki X?”, ale raczej “Ile egzemplarzy książki X klienci kupią w przyszłym miesiącu?”. Również, w zależności od kontekstu, może nie być zainteresowania tym, dlaczego przewidywana wartość wynosi X. Innymi słowy, bardziej interesuje go sama wartość niż ocena formalnej hipotezy związanej z danymi. Prognoza może również zawierać miary niepewności. W przypadku nabywcy książek podanie błędu prognozy może być pomocne przy podejmowaniu decyzji, ile książek należy kupić. Może też służyć jako metryka pozwalająca ocenić, jak dobrze zadziałała metoda predykcji.\nJakie są najważniejsze czynniki wpływające na modele predykcyjne? Istnieje wiele różnych sposobów, w jaki można stworzyć model predykcyjny, dlatego w ocenie wpływu poszczególnych czynników kluczowej jest to jak model został opracowany.\nModel mechanistyczny może być wyprowadzony przy użyciu podstawowych zasad w celu uzyskania równania modelowego, które zależy od pewnych założeń. Na przykład przy przewidywaniu ilości leku, która znajduje się w organizmie danej osoby w określonym czasie, przyjmuje się pewne formalne założenia dotyczące sposobu podawania, wchłaniania, metabolizowania i eliminacji leku. Na tej podstawie można wykorzystać układ równań różniczkowych do wyprowadzenia określonego równania modelowego. Dane są wykorzystywane do oszacowania nieznanych parametrów tego równania, tak aby można było wygenerować prognozy. Podobnie jak modele inferencyjne, mechanistyczne modele predykcyjne w dużym stopniu zależą od założeń, które definiują ich równania modelowe. Jednakże, w przeciwieństwie do modeli inferencyjnych, łatwo jest formułować oparte na danych stwierdzenia dotyczące tego, jak dobrze model działa, na podstawie tego, jak dobrze przewiduje istniejące dane. W tym przypadku pętla sprzężenia zwrotnego dla osoby zajmującej się modelowaniem jest znacznie szybsza niż w przypadku testowania hipotez.\nModele empiryczne są tworzone przy bardziej niejasnych założeniach. Modele te należą zwykle do kategorii uczenia maszynowego. Dobrym przykładem jest model K-najbliższych sąsiadów (KNN). Biorąc pod uwagę zestaw danych referencyjnych, nowa obserwacja jest przewidywana przy użyciu wartości K najbardziej podobnych danych w zestawie referencyjnym. Na przykład, jeśli kupujący książkę potrzebuje prognozy dla nowej książki, a dodatkowo posiada dane historyczne o istniejących książkach, wówczas model 5-najbliższych sąsiadów może posłużyć do estymacji liczby nowych książek do zakupu na podstawie liczby sprzedaży pięciu książek, które są najbardziej podobne do nowej książki (dla pewnej definicji “podobnej”). Model ten jest zdefiniowany jedynie przez samą funkcję predykcji (średnia z pięciu podobnych książek). Nie przyjmuje się żadnych teoretycznych lub probabilistycznych założeń dotyczących sprzedaży lub zmiennych, które są używane do określenia podobieństwa pomiędzy książkami. W rzeczywistości podstawową metodą oceny adekwatności modelu jest ocena jego precyzji przy użyciu istniejących danych. Jeśli model jest dobrym wyborem, predykcje powinny być zbliżone do wartości rzeczywistych."
  },
  {
    "objectID": "modeling.html#związki-pomiędzy-typami-modeli",
    "href": "modeling.html#związki-pomiędzy-typami-modeli",
    "title": "\n2  Modelowanie statystyczne\n",
    "section": "\n2.2 Związki pomiędzy typami modeli",
    "text": "2.2 Związki pomiędzy typami modeli\nZwykły model regresji może należeć do którejś z tych trzech klas modeli, w zależności od sposobu jego wykorzystania:\n\nmodel regresji liniowej może być użyty do opisania trendów w danych;\nmodel analizy wariancji (ANOVA) jest specjalnym rodzajem modelu liniowego, który może być użyty do wnioskowania o prawdziwości hipotezy;\nmodel regresji liniowej wykorzystywany jako model predykcyjny.\n\nIstnieje dodatkowy związek między typami modeli, ponieważ konstrukcje, których celem był opis zjawiska lub wnioskowanie o nim, nie są zwykle wykorzystywane do predykcji, to nie należy całkowicie ignorować ich zdolności predykcyjnych. W przypadku pierwszych dwóch typów modeli, badacz skupia się głównie na wyselekcjonowaniu statystycznie istotnych zmiennych w modelu oraz spełnieniu szeregu założeń pozwalających na bezpieczne wnioskowanie. Takie podejście może być niebezpieczne, gdy istotność statystyczna jest używana jako jedyna miara jakości modelu. Jest możliwe, że ten statystycznie zoptymalizowany model ma słabą dokładność wyrażoną pewną miarą dopasowania. Wiemy więc, że model może nie być używany do przewidywania, ale jak bardzo należy ufać wnioskom z modelu, który ma istotne wartości \\(p\\), ale fatalną dokładność?\n\n\n\n\n\n\n\n\n\nWażne\n\n\n\nJeśli model nie jest dobrze dopasowany do danych, wnioski uzyskane na jego podstawie mogą być wysoce wątpliwe. Innymi słowy, istotność statystyczna może nie być wystarczającym dowodem na to, że model jest właściwy.\n\n\nIstnieje również podział samych modeli uczenia maszynowego. Po pierwsze, wiele modeli można skategoryzować jako nadzorowane lub nienadzorowane. Modele nienadzorowane to takie, które uczą się wzorców, skupisk lub innych cech danych, ale nie mają zmiennej wynikowej (nauczyciela). Analiza głównych składowych (PCA), analiza skupień czy autoenkodery są przykładami modeli nienadzorowanych; są one używane do zrozumienia relacji pomiędzy zmiennymi lub zestawami zmiennych bez wyraźnego związku pomiędzy predyktorami i wynikiem. Modele nadzorowane to takie, które mają zmienną wynikową. Regresja liniowa, sieci neuronowe i wiele innych metodologii należą do tej kategorii.\nW ramach modeli nadzorowanych można wyróżnić dwie główne podkategorie:\n\nregresyjne - przewidujące zmienną wynikową będącą zmienną o charakterze ilościowym;\nklasyfikacyjne - przewidujące zmienną wynikową będącą zmienną o charakterze jakościowym.\n\nRóżne zmienne modelu mogą pełnić różne role, zwłaszcza w nadzorowanym uczeniu maszynowym. Zmienna zależna lub objaśniana (ang. outcome) to wartość przewidywana w modelach nadzorowanych. Zmienne niezależne, które są podłożem do tworzenia przewidywań wyniku, są również określane jako predyktory, cechy lub kowarianty (w zależności od kontekstu)."
  },
  {
    "objectID": "modeling.html#proces-tworzenie-modelu",
    "href": "modeling.html#proces-tworzenie-modelu",
    "title": "\n2  Modelowanie statystyczne\n",
    "section": "\n2.3 Proces tworzenie modelu",
    "text": "2.3 Proces tworzenie modelu\nPo pierwsze, należy pamiętać o chronicznie niedocenianym procesie czyszczenia danych. Bez względu na okoliczności, należy przeanalizować dane pod kątem tego, czy są one odpowiednie do celów projektu i czy są właściwe. Te kroki mogą z łatwością zająć więcej czasu niż cała reszta procesu analizy danych (w zależności od okoliczności).\nCzyszczenie danych może również pokrywać się z drugą fazą eksploracji danych, często określaną jako eksploracyjna analiza danych (ang. exploratory data analysis - EDA). EDA wydobywa na światło dzienne to, jak różne zmienne są ze sobą powiązane, ich rozkłady, typowe zakresy zmienności i inne atrybuty. Dobrym pytaniem, które należy zadać w tej fazie, jest “Jak dotarłem do tych danych?”. To pytanie może pomóc zrozumieć, w jaki sposób dane, o których mowa, były próbkowane lub filtrowane i czy te operacje były właściwe. Na przykład podczas łączenia tabel bazy danych może dojść do nieudanego złączenia, które może przypadkowo wyeliminować jedną lub więcej subpopulacji.\nWreszcie, przed rozpoczęciem procesu analizy danych, powinny istnieć jasne oczekiwania co do celu modelu i sposobu oceny jego wydajności. Należy zidentyfikować przynajmniej jedną metrykę wydajności z realistycznymi celami dotyczącymi tego, co można osiągnąć. Typowe metryki statystyczne, to dokładność klasyfikacji (ang. accuracy), odsetek poprawnie i niepoprawnie zaklasyfikowanych sukcesów (przez sukces rozumiemy wyróżnioną klasę), pierwiastek błędu średniokwadratowego i tak dalej. Należy rozważyć względne korzyści i wady tych metryk. Ważne jest również, aby metryka była zgodna z szerszymi celami analizy danych.\n\n\nTypowy przebieg budowy modelu\n\n\nProces badania danych może nie być prosty. (Wickham i in. 2019) przedstawili doskonałą ilustrację ogólnego procesu analizy danych . Import danych i czyszczenie/porządkowanie są pokazane jako początkowe kroki. Kiedy rozpoczynają się kroki analityczne dla zrozumienia relacji panujących pomiędzy predyktorami i/lub zmienną wynikową, nie możemy wstępnie określić, ile czasu mogą zająć. Cykl transformacji, modelowania i wizualizacji często wymaga wielu iteracji.\nW ramach czynności zaznaczonych na szarym polu możemy wyróżnić:\n\neksploracyjna analiza danych - to kombinacja pewnych obliczeń statystycznych i wizualizacji, w celu odpowiedzi na podstawowe pytania i postawienia kolejnych. Przykładowo jeśli na wykresie histogramu lub gęstości zmiennej wynikowej w zadaniu regresyjnym zauważymy wyraźną dwumodalność, to może ona świadczyć, że badana zbiorowość nie jest homogeniczna w kontekście analizowanej zmiennej, a co w konsekwencji może skłonić nas do oddzielnego modelowania zjawisk w każdej z podpopulacji.\ninżynieria cech (ang. feature engineering) - zespół czynności mający na celu transformację i selekcję cech w procesie budowania modelu.\ntuning modeli - zespół czynności mający na celu optymalizację hiperparametrów modeli, poprzez wybór różnych ich konfiguracji oraz porównanie efektów uczenia.\nocena dopasowania modeli - ocena jakości otrzymanych modeli na podstawie miar oraz wykresów diagnostycznych.\n\n\n\nPrzykładowy przebieg budowy modelu\n\n\nPrzykładowo w pracy Kuhn i Johnson (2021) autorzy badając natężenie ruchu kolei publicznej w Chicago, przeprowadzili następujące rozumowanie podczas budowy modelu (oryginalna pisownia):\n\nKoddt <- tibble::tribble(\n                                                                                                                           ~Thoughts,             ~Activity,\n                                                             \"The daily ridership values between stations are extremely correlated.\",                 \"EDA\",\n                                                                                \"Weekday and weekend ridership look very different.\",                 \"EDA\",\n                                                           \"One day in the summer of 2010 has an abnormally large number of riders.\",                 \"EDA\",\n                                                                             \"Which stations had the lowest daily ridership values?\",                 \"EDA\",\n                                                                    \"Dates should at least be encoded as day-of-the-week, and year.\", \"Feature Engineering\",\n                                \"Maybe PCA could be used on the correlated predictors to make it easier for the models to use them.\", \"Feature Engineering\",\n                                                     \"Hourly weather records should probably be summarized into daily measurements.\", \"Feature Engineering\",\n                                      \"Let’s start with simple linear regression, K-nearest neighbors, and a boosted decision tree.\",       \"Model Fitting\",\n                                                                                                \"How many neighbors should be used?\",        \"Model Tuning\",\n                                                                         \"Should we run a lot of boosting iterations or just a few?\",        \"Model Tuning\",\n                                                                           \"How many neighbors seemed to be optimal for these data?\",        \"Model Tuning\",\n                                                                            \"Which models have the lowest root mean squared errors?\",    \"Model Evaluation\",\n                                                                                                 \"Which days were poorly predicted?\",                 \"EDA\",\n  \"Variable importance scores indicate that the weather information is not predictive. We’ll drop them from the next set of models.\",    \"Model Evaluation\",\n                                                     \"It seems like we should focus on a lot of boosting iterations for that model.\",    \"Model Evaluation\",\n                                            \"We need to encode holiday features to improve predictions on (and around) those dates.\", \"Feature Engineering\",\n                                                                                               \"Let’s drop KNN from the model list.\",    \"Model Evaluation\"\n  )\ndt |> \n  gt::gt()\n\n\n\n\n\n\nThoughts\n      Activity\n    \n\n\nThe daily ridership values between stations are extremely correlated.\nEDA\n\n\nWeekday and weekend ridership look very different.\nEDA\n\n\nOne day in the summer of 2010 has an abnormally large number of riders.\nEDA\n\n\nWhich stations had the lowest daily ridership values?\nEDA\n\n\nDates should at least be encoded as day-of-the-week, and year.\nFeature Engineering\n\n\nMaybe PCA could be used on the correlated predictors to make it easier for the models to use them.\nFeature Engineering\n\n\nHourly weather records should probably be summarized into daily measurements.\nFeature Engineering\n\n\nLet’s start with simple linear regression, K-nearest neighbors, and a boosted decision tree.\nModel Fitting\n\n\nHow many neighbors should be used?\nModel Tuning\n\n\nShould we run a lot of boosting iterations or just a few?\nModel Tuning\n\n\nHow many neighbors seemed to be optimal for these data?\nModel Tuning\n\n\nWhich models have the lowest root mean squared errors?\nModel Evaluation\n\n\nWhich days were poorly predicted?\nEDA\n\n\nVariable importance scores indicate that the weather information is not predictive. We’ll drop them from the next set of models.\nModel Evaluation\n\n\nIt seems like we should focus on a lot of boosting iterations for that model.\nModel Evaluation\n\n\nWe need to encode holiday features to improve predictions on (and around) those dates.\nFeature Engineering\n\n\nLet’s drop KNN from the model list.\nModel Evaluation\n\n\n\n\n\n\n\n\n\n\nBolstad, Benjamin Milo. 2004. Low-Level Analysis of High-Density Oligonucleotide Array Data: Background, Normalization and Summarization. University of California, Berkeley.\n\n\nGentleman, Robert, Vincent Carey, Wolfgang Huber, Sandrine Dudoit, i Rafael Irizarry. 2005. Bioinformatics and Computational Biology Solutions Using R and Bioconductor. Springer.\n\n\nKuhn, Max, i Kjell Johnson. 2021. Feature Engineering and Selection: A Practical Approach for Predictive Models. Taylor & Francis Group.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, i in. 2019. „Welcome to the Tidyverse”. Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bolstad, Benjamin Milo. 2004. Low-Level Analysis of\nHigh-density Oligonucleotide Array Data:\nBackground, Normalization and\nSummarization. University of California,\nBerkeley.\n\n\nGentleman, Robert, Vincent Carey, Wolfgang Huber, Sandrine Dudoit, and\nRafael Irizarry. 2005. Bioinformatics and Computational\nBiology Solutions Using R and Bioconductor.\nSpringer.\n\n\nKuhn, Max, and Kjell Johnson. 2021. Feature Engineering\nand Selection: A Practical Approach for\nPredictive Models. Taylor & Francis\nGroup.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the Tidyverse.” Journal of Open Source\nSoftware 4 (43): 1686. https://doi.org/10.21105/joss.01686."
  }
]