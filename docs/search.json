[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Metody walidacji modeli statystycznych",
    "section": "",
    "text": "Wprowadzenie\nNiniejsza książka została przygotowana z myślą o studentach kierunków Inżynierii i Analizy Danych oraz Matematyka. Ma ona posłużyć w nauce szeroko rozumianej walidacji modeli statystycznych. Pojęcie zostanie dosyć szeroko potraktowane, dlatego w książce nawiązuję do metod próbkowania (ang. resampling), stosowanych do oceny jakości modelu oraz w procesie tuningowaniu modelu, jak również do miar stosowanych do oceny dopasowania modeli. Ponieważ książka ta ma stanowić przegląd metod używanych w ocenie jak dobrze model przewiduje nieznane wartości, to zostanie również uzupełniona o analizę wrażliwości pozwalającą na określenie jakie poszczególne czynniki (predyktory) i w jaki sposób wpływają na predykcję.\nW statystyce walidacja modelu jest zadaniem polegającym na ocenie, czy wybrany model statystyczny jest odpowiedni, czy nie. Często we wnioskowaniu statystycznym wnioski z modeli, które wydają się pasować do danych, mogą być błędne, co powoduje, że badacze nie poznają rzeczywistej wartości swojego modelu. Aby temu zapobiec, stosuje się walidację modelu, aby sprawdzić, czy model statystyczny posiada tzw. zdolność generalizacji. Owa zdolność polega na możliwości poprawnego przewidywania wartości wyjściowej nie tylko dla obserwacji ze zbioru uczącego, ale również z testowego, bez znaczącej straty jakości wspomnianej predykcji. Tego tematu nie należy mylić z blisko związanym zadaniem wyboru modelu, czyli procesem rozróżniania wielu modeli kandydujących: walidacja modelu nie dotyczy tak bardzo konceptualnej konstrukcji modeli, lecz testuje jedynie spójność pomiędzy wybranym modelem a jego deklarowanymi wynikami. Nadrzędnym celem walidacji modelu jest chęć poprawienia jego właściwości.\n\n\n\n\n\nWalidacja modelu występuje w wielu odmianach, a konkretna metoda walidacji modelu stosowana przez badacza jest często uwarunkowana jego projektem badawczym. Oznacza to, że nie ma jednej uniwersalnej metody walidacji modelu. Na przykład, jeśli badacz pracuje z bardzo ograniczonym zestawem danych, ale ma silne założenia wstępne dotyczące danych, może rozważyć walidację dopasowania swojego modelu poprzez zastosowanie metod bayesowskich i testowanie dopasowania modelu przy użyciu różnych rozkładów wstępnych. Jeśli jednak badacz ma dużo danych i testuje wiele zagnieżdżonych modeli, warunki te mogą sprzyjać walidacji krzyżowej i ewentualnie testowi “leave one out”. Są to dwa abstrakcyjne przykłady i każda rzeczywista walidacja modelu będzie musiała rozważyć znacznie więcej zawiłości niż tu opisano, ale przykład ten ilustruje, że metody walidacji modelu zawsze będą miały charakter poszlakowy. Ogólnie rzecz biorąc, modele mogą być walidowane przy użyciu istniejących danych lub przy użyciu nowych danych, a obie metody są omówione w kolejnych rozdziałach.\nNiezbędnym narzędziem w ocenie jakości dopasowania modeli będą metryki, czy inne narzędzia stosowane w ocenie poprawności modelu jak wykresy diagnostyczne, czy macierze klasyfikacji (ang. confusion matrix). W zależności od realizowanego zadania inne metryki mogą być wykorzystywane. I tak w zadaniach klasyfikacyjnych, w zależności od liczby klas zmiennej wyjściowej, będziemy stosowali inne miary do oceny modelu. Również w modelach regresyjnych istnieje cała gama miar pomagających ocenić dopasowanie. Jeszcze inne miary służą do ewaluacji metod nienadzorowanego uczenia maszynowego. Istnieje bowiem wiele technik określenia stopnia jednorodności powstałych skupień, a jednocześnie odpowiedzi napytanie czy wybrana została właściwa liczba grup."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Ekosystem",
    "section": "",
    "text": "Książka zawiera szereg przykładów ilustrujących zasadę działania poszczególnych technik walidacji modeli. Ponieważ od wielu lat używam języka R do budowania modeli uczenia maszynowego, to również w tej książce znajdą się implementacje metod ewaluacji modeli wykonane w R. Spośród trzech dominujących ekosystemów w języku R do budowania modeli uczenia maszynowego, czyli caret1,mlr32 i tidymodels3 postanowiłem postawić na ten ostatn i. Stanowi on przeniesienie filozofii “tidy data” realizowanej w pakiecie tidyverse4 na modele ML. tidymodels jest meta-pakietem zawierającym w sob ie:1 biblioteka napisana przez Maxa Kuhna, która niestety nie dostaje już wsparcia - https://topepo.github.io/caret/2 jedna biblioteka z całego ekosystemu przygotowanego głównie przez niemieckich programistów Lars Kotthoff, Raphael Sonabend, Michel Lang, Bernd Bischl. Jej największą zaletą jest fakt, iż wszelkie operacje na danych są dokonywane w formacie data.table uznawanym za najszybszy - https://mlr3book.mlr-org.com3 https://www.tidymodels.org4 pakiet został stworzony przez Julie Silge i Maxa Kuhna, a ich motto podane podczas prezentacji biblioteki brzmiało Whenever possible, the software should be able to protect users from committing mistakes. Software should make it easy for users to do the right thing\n\n\nbroom - przekształcanie obiektów statystycznych w tibble;\ndials - narzędzia do tuningowania parametrów modelu;\ndplyr - narzędzia do manipulacji danymi;\nggplot2 - narzędzia do wizualizacji;\ninfer - narzędzia do wnioskowania statystycznego;\nmodeldata - pakiet przykładowych danych do budowania modeli;\nparsnip - narzędzia do tworzenia i manipulowania funkcjami powszechnie używanymi podczas modelowania;\npurrr - zestaw narzędzi do programowania funkcyjnego;\nrecipes - narzędzie do tworzenia modeli;\nrsample - narzędzie do resamplingu;\ntibble - narzędzie do operacji na ramkach danych;\ntidyr - narzędzie do oczyszczania o transformacji danych;\ntune - narzędzie do tuningu hiperparametrów modelu;\nworkflows - narzędzia do zarządzania procesem uczenia modeli;\nworkflowsets - narzędzie do tworzenia zestawów workflow;\nyardstick - narzędzia do oceny dopasowania modelu.\n\nOprócz wspomnianych pakietów w ramach tidymodels występują także: applicable, baguette, butcher, discrim, embed, hardhat, corrr, rules, text recipes, tidypredict, modeldb i tidyposterior.\n\n\n\nWybór padł na tidymodels ponieważ filozofia tworzenia i walidacji modeli przypominająca pieczenie ciasta bardzo mi przypadła do gustu."
  },
  {
    "objectID": "modeling.html#typy-modeli",
    "href": "modeling.html#typy-modeli",
    "title": "\n2  Modelowanie statystyczne\n",
    "section": "\n2.1 Typy modeli",
    "text": "2.1 Typy modeli\nZanim przejdziemy dalej, opiszmy taksonomię rodzajów modeli, pogrupowanych według przeznaczenia. Taksonomia ta informuje zarówno o tym, jak model jest używany, jak i o wielu aspektach tego, jak model może być tworzony lub oceniany. Chociaż lista ta nie jest wyczerpująca, większość modeli należy do przynajmniej jednej z tych kategorii:\n\n2.1.1 Modele opisowe\nCelem modelu opisowego jest opis lub zilustrowanie pewnych cech danych. Analiza może nie mieć innego celu niż wizualne podkreślenie jakiegoś trendu lub artefaktu (lub defektu) w danych.\nNa przykład, pomiary RNA na dużą skalę są możliwe od pewnego czasu przy użyciu mikromacierzy. Wczesne metody laboratoryjne umieszczały próbkę biologiczną na małym mikrochipie. Bardzo małe miejsca na chipie mogą mierzyć sygnał oparty na bogactwie specyficznej sekwencji RNA. Chip zawierałby tysiące (lub więcej) wyników, z których każdy byłby kwantyfikacją RNA związanego z procesem biologicznym. Jednakże na chipie mogłyby wystąpić problemy z jakością, które mogłyby prowadzić do słabych wyników. Na przykład, odcisk palca przypadkowo pozostawiony na części chipa mógłby spowodować niedokładne pomiary podczas skanowania.\n\n\nRysunek 2.1: Ocena jakości odczytów z mikroczipa\n\n\nWczesną metodą oceny takich zagadnień były modele na poziomie sondy, czyli PLM (Bolstad 2004). Tworzono model statystyczny, który uwzględniał pewne różnice w danych, takie jak chip, sekwencja RNA, typ sekwencji i tak dalej. Jeśli w danych występowałyby inne, nieznane czynniki, to efekty te zostałyby uchwycone w resztach modelu. Gdy reszty zostały wykreślone według ich lokalizacji na chipie, dobrej jakości chip nie wykazywałby żadnych wzorców. W przypadku wystąpienia problemu, pewien rodzaj wzorca przestrzennego byłby dostrzegalny. Często typ wzorca sugerowałby problem (np. odcisk palca) oraz możliwe rozwiązanie (wytarcie chipa i ponowne skanowanie, powtórzenie próbki, itp.) Rysunek 2.1 pokazuje zastosowanie tej metody dla dwóch mikromacierzy (Gentleman i in. 2005). Obrazy pokazują dwie różne wartości kolorystyczne; obszary, które są ciemniejsze to miejsca, gdzie intensywność sygnału była większa niż oczekuje model, podczas gdy jaśniejszy kolor pokazuje wartości niższe niż oczekiwane. Lewy panel pokazuje w miarę losowy wzór, podczas gdy prawy panel wykazuje niepożądany artefakt w środku chipa.\n\n2.1.2 Modele do wnioskowania\nCelem modelu inferencyjnego jest podjęcie decyzji dotyczącej pytania badawczego lub sprawdzenie określonej hipotezy, podobnie jak w przypadku testów statystycznych. Model inferencyjny zaczyna się od wcześniej zdefiniowanego przypuszczenia lub pomysłu na temat populacji i tworzy wniosek statystyczny, taki jak szacunek przedziału lub odrzucenie hipotezy.\nNa przykład, celem badania klinicznego może być potwierdzenie, że nowa terapia pozwala wydłużyć życie w porównaniu z istniejącą terapią lub brakiem leczenia. Jeśli kliniczny punkt końcowy dotyczyłby przeżycia pacjenta, hipoteza zerowa mogłaby brzmieć, że nowa terapia ma równą lub niższą medianę czasu przeżycia, a hipoteza alternatywna, że nowa terapia ma wyższą medianę czasu przeżycia. Jeśli ta próba byłaby oceniana przy użyciu tradycyjnego testowania istotności hipotezy zerowej poprzez modelowanie, testowanie istotności dałoby wartość \\(p\\) przy użyciu jakiejś wcześniej zdefiniowanej metodologii opartej na zestawie założeń. Małe wartości dla wartości \\(p\\) w wynikach modelu wskazywałyby na istnienie przesłanek, że nowa terapia pomaga pacjentom żyć dłużej. Duże wartości \\(p\\) w wynikach modelu wskazywałyby, że nie udało się wykazać takiej różnicy; ten brak przesłanek mógłby wynikać z wielu powodów, w tym z tego, że terapia nie działa.\nJakie są ważne aspekty tego typu analizy? Techniki modelowania inferencyjnego zazwyczaj dają pewien rodzaj danych wyjściowych o charakterze probabilistycznym, takich jak wartość \\(p\\), przedział ufności lub prawdopodobieństwo a posteriori. Zatem, aby obliczyć taką wielkość, należy przyjąć formalne założenia probabilistyczne dotyczące danych i procesów, które je wygenerowały. Jakość wyników modelowania statystycznego w dużym stopniu zależy od tych wcześniej określonych założeń, jak również od tego, w jakim stopniu obserwowane dane wydają się z nimi zgadzać. Najbardziej krytycznymi czynnikami są tutaj założenia teoretyczne: “Jeśli moje obserwacje były niezależne, a reszty mają rozkład X, to statystyka testowa Y może być użyta do uzyskania wartości \\(p\\). W przeciwnym razie wynikowa wartość \\(p\\) może być niewłaściwa.”\nJednym z aspektów analiz inferencyjnych jest to, że istnieje tendencja do opóźnionego sprzężenia zwrotnego w zrozumieniu, jak dobrze dane odpowiadają założeniom modelu. W naszym przykładzie badania klinicznego, jeśli znaczenie statystyczne (i kliniczne) wskazuje, że nowa terapia powinna być dostępna do stosowania przez pacjentów, mogą minąć lata zanim zostanie ona zastosowana w terenie i zostanie wygenerowana wystarczająca ilość danych do niezależnej oceny, czy pierwotna analiza statystyczna doprowadziła do podjęcia właściwej decyzji.\n\n2.1.3 Modele predykcyjne\nCzasami modelowania używamy w celu uzyskania jak najdokładniejszej prognozy dla nowych danych. W tym przypadku głównym celem jest, aby przewidywane wartości (ang. prediction) miały najwyższą możliwą zgodność z prawdziwą wartością (ang. observed).\nProstym przykładem może być przewidywanie przez sprzedającego książki, ile egzemplarzy danej książki powinno być wysłanych do jego sklepu w następnym miesiącu. Nadmierna prognoza powoduje marnowanie miejsca i pieniędzy z powodu nadmiaru książek. Jeśli przewidywanie jest mniejsze niż powinno, następuje utrata potencjału i mniejszy zysk.\nCelem tego typu modeli jest raczej estymacja niż wnioskowanie. Na przykład nabywca zwykle nie jest zainteresowany pytaniem typu “Czy w przyszłym miesiącu sprzedam więcej niż 100 egzemplarzy książki X?”, ale raczej “Ile egzemplarzy książki X klienci kupią w przyszłym miesiącu?”. Również, w zależności od kontekstu, może nie być zainteresowania tym, dlaczego przewidywana wartość wynosi X. Innymi słowy, bardziej interesuje go sama wartość niż ocena formalnej hipotezy związanej z danymi. Prognoza może również zawierać miary niepewności. W przypadku nabywcy książek podanie błędu prognozy może być pomocne przy podejmowaniu decyzji, ile książek należy kupić. Może też służyć jako metryka pozwalająca ocenić, jak dobrze zadziałała metoda predykcji.\nJakie są najważniejsze czynniki wpływające na modele predykcyjne? Istnieje wiele różnych sposobów, w jaki można stworzyć model predykcyjny, dlatego w ocenie wpływu poszczególnych czynników kluczowej jest to jak model został opracowany.\nModel mechanistyczny może być wyprowadzony przy użyciu podstawowych zasad w celu uzyskania równania modelowego, które zależy od pewnych założeń. Na przykład przy przewidywaniu ilości leku, która znajduje się w organizmie danej osoby w określonym czasie, przyjmuje się pewne formalne założenia dotyczące sposobu podawania, wchłaniania, metabolizowania i eliminacji leku. Na tej podstawie można wykorzystać układ równań różniczkowych do wyprowadzenia określonego równania modelowego. Dane są wykorzystywane do oszacowania nieznanych parametrów tego równania, tak aby można było wygenerować prognozy. Podobnie jak modele inferencyjne, mechanistyczne modele predykcyjne w dużym stopniu zależą od założeń, które definiują ich równania modelowe. Jednakże, w przeciwieństwie do modeli inferencyjnych, łatwo jest formułować oparte na danych stwierdzenia dotyczące tego, jak dobrze model działa, na podstawie tego, jak dobrze przewiduje istniejące dane. W tym przypadku pętla sprzężenia zwrotnego dla osoby zajmującej się modelowaniem jest znacznie szybsza niż w przypadku testowania hipotez.\nModele empiryczne są tworzone przy bardziej niejasnych założeniach. Modele te należą zwykle do kategorii uczenia maszynowego. Dobrym przykładem jest model K-najbliższych sąsiadów (KNN). Biorąc pod uwagę zestaw danych referencyjnych, nowa obserwacja jest przewidywana przy użyciu wartości K najbardziej podobnych danych w zestawie referencyjnym. Na przykład, jeśli kupujący książkę potrzebuje prognozy dla nowej książki, a dodatkowo posiada dane historyczne o istniejących książkach, wówczas model 5-najbliższych sąsiadów może posłużyć do estymacji liczby nowych książek do zakupu na podstawie liczby sprzedaży pięciu książek, które są najbardziej podobne do nowej książki (dla pewnej definicji “podobnej”). Model ten jest zdefiniowany jedynie przez samą funkcję predykcji (średnia z pięciu podobnych książek). Nie przyjmuje się żadnych teoretycznych lub probabilistycznych założeń dotyczących sprzedaży lub zmiennych, które są używane do określenia podobieństwa pomiędzy książkami. W rzeczywistości podstawową metodą oceny adekwatności modelu jest ocena jego precyzji przy użyciu istniejących danych. Jeśli model jest dobrym wyborem, predykcje powinny być zbliżone do wartości rzeczywistych."
  },
  {
    "objectID": "modeling.html#związki-pomiędzy-typami-modeli",
    "href": "modeling.html#związki-pomiędzy-typami-modeli",
    "title": "\n2  Modelowanie statystyczne\n",
    "section": "\n2.2 Związki pomiędzy typami modeli",
    "text": "2.2 Związki pomiędzy typami modeli\nZwykły model regresji może należeć do którejś z tych trzech klas modeli, w zależności od sposobu jego wykorzystania:\n\nmodel regresji liniowej może być użyty do opisania trendów w danych;\nmodel analizy wariancji (ANOVA) jest specjalnym rodzajem modelu liniowego, który może być użyty do wnioskowania o prawdziwości hipotezy;\nmodel regresji liniowej wykorzystywany jako model predykcyjny.\n\nIstnieje dodatkowy związek między typami modeli, ponieważ konstrukcje, których celem był opis zjawiska lub wnioskowanie o nim, nie są zwykle wykorzystywane do predykcji, to nie należy całkowicie ignorować ich zdolności predykcyjnych. W przypadku pierwszych dwóch typów modeli, badacz skupia się głównie na wyselekcjonowaniu statystycznie istotnych zmiennych w modelu oraz spełnieniu szeregu założeń pozwalających na bezpieczne wnioskowanie. Takie podejście może być niebezpieczne, gdy istotność statystyczna jest używana jako jedyna miara jakości modelu. Jest możliwe, że ten statystycznie zoptymalizowany model ma słabą dokładność wyrażoną pewną miarą dopasowania. Wiemy więc, że model może nie być używany do przewidywania, ale jak bardzo należy ufać wnioskom z modelu, który ma istotne wartości \\(p\\), ale fatalną dokładność?\n\n\n\n\n\n\n\n\n\nWażne\n\n\n\nJeśli model nie jest dobrze dopasowany do danych, wnioski uzyskane na jego podstawie mogą być wysoce wątpliwe. Innymi słowy, istotność statystyczna może nie być wystarczającym dowodem na to, że model jest właściwy.\n\n\nIstnieje również podział samych modeli uczenia maszynowego. Po pierwsze, wiele modeli można skategoryzować jako nadzorowane lub nienadzorowane. Modele nienadzorowane to takie, które uczą się wzorców, skupisk lub innych cech danych, ale nie mają zmiennej wynikowej (nauczyciela). Analiza głównych składowych (PCA), analiza skupień czy autoenkodery są przykładami modeli nienadzorowanych; są one używane do zrozumienia relacji pomiędzy zmiennymi lub zestawami zmiennych bez wyraźnego związku pomiędzy predyktorami i wynikiem. Modele nadzorowane to takie, które mają zmienną wynikową. Regresja liniowa, sieci neuronowe i wiele innych metodologii należą do tej kategorii.\nW ramach modeli nadzorowanych można wyróżnić dwie główne podkategorie:\n\nregresyjne - przewidujące zmienną wynikową będącą zmienną o charakterze ilościowym;\nklasyfikacyjne - przewidujące zmienną wynikową będącą zmienną o charakterze jakościowym.\n\nRóżne zmienne modelu mogą pełnić różne role, zwłaszcza w nadzorowanym uczeniu maszynowym. Zmienna zależna lub objaśniana (ang. outcome) to wartość przewidywana w modelach nadzorowanych. Zmienne niezależne, które są podłożem do tworzenia przewidywań wyniku, są również określane jako predyktory, cechy lub kowarianty (w zależności od kontekstu)."
  },
  {
    "objectID": "modeling.html#proces-tworzenie-modelu",
    "href": "modeling.html#proces-tworzenie-modelu",
    "title": "\n2  Modelowanie statystyczne\n",
    "section": "\n2.3 Proces tworzenie modelu",
    "text": "2.3 Proces tworzenie modelu\nPo pierwsze, należy pamiętać o chronicznie niedocenianym procesie czyszczenia danych. Bez względu na okoliczności, należy przeanalizować dane pod kątem tego, czy są one odpowiednie do celów projektu i czy są właściwe. Te kroki mogą z łatwością zająć więcej czasu niż cała reszta procesu analizy danych (w zależności od okoliczności).\nCzyszczenie danych może również pokrywać się z drugą fazą eksploracji danych, często określaną jako eksploracyjna analiza danych (ang. exploratory data analysis - EDA). EDA wydobywa na światło dzienne to, jak różne zmienne są ze sobą powiązane, ich rozkłady, typowe zakresy zmienności i inne atrybuty. Dobrym pytaniem, które należy zadać w tej fazie, jest “Jak dotarłem do tych danych?”. To pytanie może pomóc zrozumieć, w jaki sposób dane, o których mowa, były próbkowane lub filtrowane i czy te operacje były właściwe. Na przykład podczas łączenia tabel bazy danych może dojść do nieudanego złączenia, które może przypadkowo wyeliminować jedną lub więcej subpopulacji.\nWreszcie, przed rozpoczęciem procesu analizy danych, powinny istnieć jasne oczekiwania co do celu modelu i sposobu oceny jego wydajności. Należy zidentyfikować przynajmniej jedną metrykę wydajności z realistycznymi celami dotyczącymi tego, co można osiągnąć. Typowe metryki statystyczne, to dokładność klasyfikacji (ang. accuracy), odsetek poprawnie i niepoprawnie zaklasyfikowanych sukcesów (przez sukces rozumiemy wyróżnioną klasę), pierwiastek błędu średniokwadratowego i tak dalej. Należy rozważyć względne korzyści i wady tych metryk. Ważne jest również, aby metryka była zgodna z szerszymi celami analizy danych.\n\n\nTypowy przebieg budowy modelu\n\n\nProces badania danych może nie być prosty. (Wickham i in. 2019) przedstawili doskonałą ilustrację ogólnego procesu analizy danych . Import danych i czyszczenie/porządkowanie są pokazane jako początkowe kroki. Kiedy rozpoczynają się kroki analityczne dla zrozumienia relacji panujących pomiędzy predyktorami i/lub zmienną wynikową, nie możemy wstępnie określić, ile czasu mogą zająć. Cykl transformacji, modelowania i wizualizacji często wymaga wielu iteracji.\nW ramach czynności zaznaczonych na szarym polu możemy wyróżnić:\n\neksploracyjna analiza danych - to kombinacja pewnych obliczeń statystycznych i wizualizacji, w celu odpowiedzi na podstawowe pytania i postawienia kolejnych. Przykładowo jeśli na wykresie histogramu lub gęstości zmiennej wynikowej w zadaniu regresyjnym zauważymy wyraźną dwumodalność, to może ona świadczyć, że badana zbiorowość nie jest homogeniczna w kontekście analizowanej zmiennej, a co w konsekwencji może skłonić nas do oddzielnego modelowania zjawisk w każdej z podpopulacji.\ninżynieria cech (ang. feature engineering) - zespół czynności mający na celu transformację i selekcję cech w procesie budowania modelu.\ntuning modeli - zespół czynności mający na celu optymalizację hiperparametrów modeli, poprzez wybór różnych ich konfiguracji oraz porównanie efektów uczenia.\nocena dopasowania modeli - ocena jakości otrzymanych modeli na podstawie miar oraz wykresów diagnostycznych.\n\n\n\nPrzykładowy przebieg budowy modelu\n\n\nPrzykładowo w pracy Kuhn i Johnson (2021) autorzy badając natężenie ruchu kolei publicznej w Chicago, przeprowadzili następujące rozumowanie podczas budowy modelu (oryginalna pisownia):\n\nKoddt <- tibble::tribble(\n                                                                                                                           ~Thoughts,             ~Activity,\n                                                             \"The daily ridership values between stations are extremely correlated.\",                 \"EDA\",\n                                                                                \"Weekday and weekend ridership look very different.\",                 \"EDA\",\n                                                           \"One day in the summer of 2010 has an abnormally large number of riders.\",                 \"EDA\",\n                                                                             \"Which stations had the lowest daily ridership values?\",                 \"EDA\",\n                                                                    \"Dates should at least be encoded as day-of-the-week, and year.\", \"Feature Engineering\",\n                                \"Maybe PCA could be used on the correlated predictors to make it easier for the models to use them.\", \"Feature Engineering\",\n                                                     \"Hourly weather records should probably be summarized into daily measurements.\", \"Feature Engineering\",\n                                      \"Let’s start with simple linear regression, K-nearest neighbors, and a boosted decision tree.\",       \"Model Fitting\",\n                                                                                                \"How many neighbors should be used?\",        \"Model Tuning\",\n                                                                         \"Should we run a lot of boosting iterations or just a few?\",        \"Model Tuning\",\n                                                                           \"How many neighbors seemed to be optimal for these data?\",        \"Model Tuning\",\n                                                                            \"Which models have the lowest root mean squared errors?\",    \"Model Evaluation\",\n                                                                                                 \"Which days were poorly predicted?\",                 \"EDA\",\n  \"Variable importance scores indicate that the weather information is not predictive. We’ll drop them from the next set of models.\",    \"Model Evaluation\",\n                                                     \"It seems like we should focus on a lot of boosting iterations for that model.\",    \"Model Evaluation\",\n                                            \"We need to encode holiday features to improve predictions on (and around) those dates.\", \"Feature Engineering\",\n                                                                                               \"Let’s drop KNN from the model list.\",    \"Model Evaluation\"\n  )\ndt |> \n  gt::gt()\n\n\n\n\n\n\nThoughts\nActivity\n\n\n\nThe daily ridership values between stations are extremely correlated.\nEDA\n\n\nWeekday and weekend ridership look very different.\nEDA\n\n\nOne day in the summer of 2010 has an abnormally large number of riders.\nEDA\n\n\nWhich stations had the lowest daily ridership values?\nEDA\n\n\nDates should at least be encoded as day-of-the-week, and year.\nFeature Engineering\n\n\nMaybe PCA could be used on the correlated predictors to make it easier for the models to use them.\nFeature Engineering\n\n\nHourly weather records should probably be summarized into daily measurements.\nFeature Engineering\n\n\nLet’s start with simple linear regression, K-nearest neighbors, and a boosted decision tree.\nModel Fitting\n\n\nHow many neighbors should be used?\nModel Tuning\n\n\nShould we run a lot of boosting iterations or just a few?\nModel Tuning\n\n\nHow many neighbors seemed to be optimal for these data?\nModel Tuning\n\n\nWhich models have the lowest root mean squared errors?\nModel Evaluation\n\n\nWhich days were poorly predicted?\nEDA\n\n\nVariable importance scores indicate that the weather information is not predictive. We’ll drop them from the next set of models.\nModel Evaluation\n\n\nIt seems like we should focus on a lot of boosting iterations for that model.\nModel Evaluation\n\n\nWe need to encode holiday features to improve predictions on (and around) those dates.\nFeature Engineering\n\n\nLet’s drop KNN from the model list.\nModel Evaluation\n\n\n\n\n\n\n\n\n\n\nBolstad, Benjamin Milo. 2004. Low-Level Analysis of High-Density Oligonucleotide Array Data: Background, Normalization and Summarization. University of California, Berkeley.\n\n\nGentleman, Robert, Vincent Carey, Wolfgang Huber, Sandrine Dudoit, i Rafael Irizarry. 2005. Bioinformatics and Computational Biology Solutions Using R and Bioconductor. Springer.\n\n\nKuhn, Max, i Kjell Johnson. 2021. Feature Engineering and Selection: A Practical Approach for Predictive Models. Taylor & Francis Group.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, i in. 2019. „Welcome to the Tidyverse”. Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686."
  },
  {
    "objectID": "infer.html",
    "href": "infer.html",
    "title": "\n3  Modele inferencyjne\n",
    "section": "",
    "text": "Jak to zostało wspomniane w poprzednim rozdziale modele inferencyjne służą do wyciągania wniosków na podstawie modelu. W większości przypadków dotyczy to przedziałów ufności pewnych charakterystyk, czy weryfikacji hipotez. Pod pojęciem modeli inferencyjnych będziemy rozumieli wszelkie modele stosowane w procedurze wnioskowania.\nW tym rozdziale zostanie przedstawiona procedura weryfikacyjna mająca na celu ocenę jakości przedstawionych rozwiązań w klasycznym podejściu do przedziałów ufności i weryfikacji hipotez. Największa trudnością w szacowaniu parametrów rozkładu za pomocą przedziałów ufności oraz w weryfikacji hipotez jest konieczność spełnienia założeń stosowalności tych metod. Bardzo często badacz nie posiada wystarczającej wiedzy o konsekwencji naruszenia tych założeń, a czasem nawet o ich istnieniu. Nawet wówczas, gdy badacz jest świadom konieczności spełnienia założeń w estymacji przedziałowej i weryfikacji hipotez, wymagania te mogą się okazać trudne do wypełnienia. W wielu przypadkach podczas weryfikacji hipotez za pomocą testu t-Studenta, weryfikując hipotezę o normalności rozkładu badanej cechy pojawiają się pewne wątpliwości. Po pierwsze, czy wybrany test mogę stosować do weryfikacji hipotezy o normalności w przypadku tak mało licznej próby lub tak licznej próby. Wiemy bowiem, że często stosowany test Shapiro-Wilka do weryfikacji hipotezy o zgodności populacji z rozkładem normalnym, może zbyt często odrzucać hipotezę o zgodności z rozkładem jeśli test jest wykonywany na dużej próbie1. Z drugiej strony dla prób o małej liczności test w większości nie odrzuca hipotezy o normalności, a to dlatego, że nie sposób jej odrzucić np. na podstawie 5 obserwacji. Podniesiony problem normalności rozkładu badanej cechy nie jest jedynym z jakim badacz może się spotkać chcąc spełnić wszystkie założenia modelu2 . Założenia o równości wariancji badanej cechy pomiędzy grupami, czy brak nadmiarowości3, to kolejne przykłady problemów z jakimi może spotkać się badac z.1 moc tego testu rośnie bardzo szybko wraz ze wzrostem liczebności próby2 mam na myśli zarówno modele przedziałów ufności, jaki i modele statystyczne do testowania hipotez3 w przypadku badania efektu za pomocą modelu liniowego\nKonieczność spełnienia wymienionych w stosowanej metodzie wnioskowania założeń jest konieczna, ponieważ w przeciwnym przypadku nie możemy być pewni czy wyniki zastosowanej metody są trafne4. Konsekwencją niespełnienia warunków początkowych metody, nie możemy być pewni czy rozkład statystyki testowej jest taki jak głosi metoda. I choć istnieją prace, które wyraźnie wskazują na odporność pewnych metod statystycznych na niespełnienie założeń, to nie zwalniają nas z weryfikacji tychże założeń, ponieważ w przypadku niektórych z nich nie znamy konsekwencji ich naruszenia.4 czy wniosek wyciągnięty na podstawie modelu jest właściwy\nW przypadku wspomnianych wyżej wątpliwości co do stosowalności poszczególnych metod weryfikacyjnych należy poszukać rozwiązań, które uprawdopodobnią wyniki uzyskane metodami klasycznymi. Powszechnie polecane w takiej sytuacji są rozwiązania opierające się na próbkowaniu (ang. resampling), wśród których najbardziej znane, to bootstrap i metody permutacyjne.\n\n\n\n\nNiezależnie od stawianej hipotezy, badacz zadaje sobie ten sam rodzaj pytania podczas wnioskowania statystycznego: czy efekt/różnica w obserwowanych danych jest rzeczywista, czy wynika z przypadku? Aby odpowiedzieć na to pytanie, analityk zakłada, że efekt w obserwowanych danych był spowodowany przypadkiem i nazywa to założenie hipotezą zerową5. Analityk następnie oblicza statystykę testową z danych, która opisuje obserwowany efekt. Może użyć tej statystyki testowej do obliczenia wartości \\(p\\) poprzez zestawienie jej z rozkładem wynikającym z hipotezy zerowej. Jeśli to prawdopodobieństwo jest poniżej jakiegoś wcześniej zdefiniowanego poziomu istotności \\(\\alpha\\), to analityk powinien odrzucić hipotezę zerową.5 W rzeczywistości, może nie wierzyć, że hipoteza zerowa jest prawdziwa - hipoteza zerowa jest w opozycji do hipotezy alternatywnej, która zakłada, że efekt obecny w obserwowanych danych jest rzeczywiście spowodowany faktem, że “coś się dzieje”\nPoniżej przedstawione zostaną przykłady zastosowania obu metod we wnioskowaniu. Można te zadania realizować na różne sposoby, my natomiast wykorzystamy bibliotekę infer (Couch i in. 2021) ekosystemu tidymodels (Kuhn i Wickham 2020).\n\nPrzykład 3.1 W tym przykładzie przetestujemy hipotezę o równości średniej z wartością teoretyczną. Dane weźmiemy ze zbioru gss biblioteki infer zawierającego podzbiór wyników spisu powszechnego przeprowadzonego w 1972 r. w USA.\n\nKodlibrary(tidymodels)\nlibrary(nord) # palettes\nglimpse(gss)\n\nRows: 500\nColumns: 11\n$ year    <dbl> 2014, 1994, 1998, 1996, 1994, 1996, 1990, 2016, 2000, 1998, 20…\n$ age     <dbl> 36, 34, 24, 42, 31, 32, 48, 36, 30, 33, 21, 30, 38, 49, 25, 56…\n$ sex     <fct> male, female, male, male, male, female, female, female, female…\n$ college <fct> degree, no degree, degree, no degree, degree, no degree, no de…\n$ partyid <fct> ind, rep, ind, ind, rep, rep, dem, ind, rep, dem, dem, ind, de…\n$ hompop  <dbl> 3, 4, 1, 4, 2, 4, 2, 1, 5, 2, 4, 3, 4, 4, 2, 2, 3, 2, 1, 2, 5,…\n$ hours   <dbl> 50, 31, 40, 40, 40, 53, 32, 20, 40, 40, 23, 52, 38, 72, 48, 40…\n$ income  <ord> $25000 or more, $20000 - 24999, $25000 or more, $25000 or more…\n$ class   <fct> middle class, working class, working class, working class, mid…\n$ finrela <fct> below average, below average, below average, above average, ab…\n$ weight  <dbl> 0.8960034, 1.0825000, 0.5501000, 1.0864000, 1.0825000, 1.08640…\n\n\nPrzetestujmy hipotezę, że średnia wieku wynosi 40 lat. Zacznijmy od sprawdzenia jak wygląda rozkład badanej cechy.\n\nKodgss |> \n  ggplot(aes(age))+\n  geom_histogram(color = \"white\", bins = 15)\n\n\n\nRysunek 3.1: Histogram wieku\n\n\n\n\nMożna mieć pewne wątpliwości co do normalności rozkładu, ponieważ zarysowuje się delikatna asymetria prawostronna. Nie będziemy jednak weryfikować hipotezy o normalności, tylko przeprowadzimy klasyczny test, nie mając pewności czy może on być stosowany w tej sytuacji.\n\nKodt.test(gss$age, mu = 40)\n\n\n    One Sample t-test\n\ndata:  gss$age\nt = 0.44656, df = 499, p-value = 0.6554\nalternative hypothesis: true mean is not equal to 40\n95 percent confidence interval:\n 39.09567 41.43633\nsample estimates:\nmean of x \n   40.266 \n\n\nWynik testu nie daje podstaw do odrzucenia hipotezy o tym, że przeciętny wiek w badanej populacji wynosi 40 lat. Przeprowadzimy teraz wnioskowanie w oparciu o techniki bootstrap i permutacyjną.\n\nKodnull_mean <- gss |> \n  specify(response = age) |> # określenie zmiennej\n  hypothesise(null = \"point\", mu = 40) |> # ustalienie hipotezy\n  generate(1000, type = \"bootstrap\") |> # generujemy dane\n  calculate(stat = \"mean\")\nnull_mean\n\nResponse: age (numeric)\nNull Hypothesis: point\n# A tibble: 1,000 × 2\n   replicate  stat\n       <int> <dbl>\n 1         1  39.4\n 2         2  40.0\n 3         3  39.3\n 4         4  41.1\n 5         5  40.3\n 6         6  39.8\n 7         7  40.3\n 8         8  38.2\n 9         9  39.6\n10        10  39.7\n# … with 990 more rows\n\nKodsample_mean <- gss |> \n  specify(response = age) |> \n  calculate(stat = \"mean\")\nsample_mean\n\nResponse: age (numeric)\n# A tibble: 1 × 1\n   stat\n  <dbl>\n1  40.3\n\n\nTeraz możemy przyjrzeć się rozkładowi średnich w próbach bootstrapowych.\n\nKodci <- null_mean |> \n  get_confidence_interval(point_estimate = sample_mean,\n                          level = .95,\n                          type = \"se\")\n\nnull_mean |> \n  visualise() + \n  shade_ci(endpoints = ci)\n\n\n\nRysunek 3.2: Histogram średnich bootstrapowych wraz z 95% przedziałem ufności dla średniej\n\n\n\n\nKoncentracja wokół wartości 40 może przemawiać za przyjęciem hipotezy \\(H_0\\). Ponadto wygląda na to, że otrzymany przedział ufności zawiera teoretyczną średnią wieku 40, co jest kolejny argumentem za przyjęciem hipotezy zerowej. Na koniec wyliczymy \\(p\\) dla testu bootstrapowego.\n\nKodnull_mean |> \n  get_p_value(obs_stat = sample_mean, direction = \"two.sided\")\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1   0.634\n\n\nWartość otrzymana z testu bootstrapowego różni się tylko nieznacznie od otrzymanej testem klasycznym.\n\nKodnull_mean |> \n  visualise()+\n  shade_p_value(obs_stat = sample_mean, direction = \"two-sided\")\n\n\n\nRysunek 3.3: Histogram średnich bootstrapowych z zacienonym obszarem ilustrującym na ile ekstremalna jest średnia naszej próby w stosunku do średniej wynikającej z hipotezy zerowej\n\n\n\n\n\n\n\n\n\n\n\nWskazówka\n\n\n\nW przypadku weryfikacji hipotez dotyczących jednej zmiennej wyniki testu bootstrapowego i permutacyjnego są identyczne, ponieważ te dwa rodzaje próbkowania są w tym przypadku identyczne.\n\n\n\nPrzykład 3.2 Tym razem przetestujemy nico bardziej ambitną hipotezę. Będzie to hipoteza o równości median. Dane zostaną wygenerowane z rozkładów asymetrycznych, a w tych przypadkach porównywanie median ma więcej sensu niż średnich.\n\nKodset.seed(44)\nx1 <- rchisq(20, 2)\nx2 <- -rchisq(15, 2)+10\n\ndt <- tibble(x = c(x1,x2)) |> \n  mutate(gr = rep(c(\"A\", \"B\"), times = c(20,15)))\n\ndt |> \n  ggplot(aes(x, fill = gr))+\n  geom_density(alpha = 0.6)+\n  xlim(c(-2, 12))+\n  scale_fill_nord(palette = \"victory_bonds\")+\n  theme_minimal()\n\n\n\nRysunek 3.4: Porównanie rozkładów obu prób\n\n\n\n\nJak widać na Rysunek 3.4 rozkłady różnią się zarówno położeniem, jak i kształtem. Ponieważ różnią się kształtem to porównanie obu rozkładów testem Manna-Whitneya nie odpowie nam na pytanie o równość median6, a jedynie o tym, że z prawdopodobieństwem 50% losowa wartość z jednego rozkładu będzie większa niż losowa wartość z drugiego rozkładu. Zatem wyniki testu klasycznego nie będą wystarczające do oceny postawionej hipotezy. Przeprowadzimy zatem test metodami próbkowania. Najpierw techniką bootstrapową.6 tylko w przypadku jednakowych kształtów rozkładów test ten weryfikuje równość median\n\nKodset.seed(44)\nsample_diff <- dt |> \n  specify(x~gr) |> \n  calculate(stat = \"diff in medians\", order = c(\"A\",\"B\"))\nsample_diff\n\nResponse: x (numeric)\nExplanatory: gr (factor)\n# A tibble: 1 × 1\n   stat\n  <dbl>\n1 -6.83\n\nKodnull_diff <- dt |> \n  specify(x~gr) |> \n  hypothesise(null = \"independence\") |> \n  generate(reps = 1000, type = \"bootstrap\") |> \n  calculate(stat = \"diff in medians\", order = c(\"A\",\"B\"))\n\n\nOceńmy rozkład różnic z prób bootstrapowych. Jeśli hipoteza zerowa jest prawdziwa to rozkład różnicy median powinien oscylować wokół zera.\n\nKodci <- null_diff |> \n  get_confidence_interval(level = .95, type = \"percentile\")\n\nnull_diff |> \n  visualise()+\n  shade_ci(endpoints = ci)+\n  geom_vline(xintercept = 0, \n             linewidth = 2,\n             color = \"red\")\n\n\n\nRysunek 3.5: Rozkład różnic pomiędzy medianami z 95% przedziałem ufności\n\n\n\n\nWidać wyraźnie, że rozkład różnic pomiędzy medianami nie oscyluje wokół zera, co może świadczyć o konieczności odrzucenia hipotezy zerowej.\n\nKodnull_diff |> \n  get_p_value(obs_stat = 0, direction = \"two-sided\")\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1       0\n\nKodwilcox.test(x1,x2)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  x1 and x2\nW = 32, p-value = 2.453e-05\nalternative hypothesis: true location shift is not equal to 0\n\n\nWartość \\(p\\) mniejsza od \\(\\alpha\\) każe odrzucić hipotezę zerową na korzyść alternatywnej, czyli mediany nie są równe.\nTeraz przeprowadzimy test permutacyjny.\n\nKodset.seed(44)\nnull_diff <- dt |> \n  specify(x~gr) |> \n  hypothesise(null = \"independence\") |> \n  generate(reps = 1000, type = \"permute\") |> \n  calculate(stat = \"diff in medians\", order = c(\"A\",\"B\"))\n\nci <- null_diff |> \n  get_ci(level = .95, \n         type = \"percentile\")\n\nnull_diff |> \n  visualise()+\n  shade_ci(endpoints = ci)+\n  geom_vline(xintercept = sample_diff$stat,\n             linewidth = 2,\n             color = \"red\")\n\n\n\nRysunek 3.6: Rozkład różnic pomiędzy medianami z 95% przedziałem ufności\n\n\n\n\nJak widać z Rysunek 3.6 rozkład różnic pomiędzy medianami dla całkowicie losowego układu obserwacji7 oscyluje wokół zera. Wartość różnicy median obliczona na podstawie próby -6.83 nie leży wewnątrz przedziału ufności dla różnicy, należy zatem odrzucić hipotezę o równości median. Potwierdza to wynik testu permutacyjnego.7 co odzwierciedla warunek z hipotezy zerowej\n\nKodnull_diff |> \n  visualise() +\n  shade_p_value(obs_stat = sample_diff, direction = \"two-sided\")\n\nnull_diff |> \n  get_p_value(obs_stat = sample_diff, direction = \"two-sided\")\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1       0\n\n\n\n\nRysunek 3.7: Rozkład różnic pomiędzy medianami z naniesionym p-value\n\n\n\n\n\n\nPrzykład 3.3 W tym przykładzie zbadamy niezależność cech partyid i class ze zbioru gss, które oznaczają odpowiednio sprzyjanie danej partii politycznej i subiektywną identyfikację klasy społeczno-ekonomicznej.\n\nKodlibrary(ggstatsplot)\nggbarstats(data = gss,\n           x = class,\n           y = partyid, \n           label = \"count\",\n           proportion.test = F, \n           bf.message = F)\n\n\n\nRysunek 3.8: Wykres słupkowy ilustrujący udziały poszczególnych grup wraz z wynikiem testu \\(\\chi^2\\) niezależności\n\n\n\n\nPowyższy test każe odrzucić hipotezę o niezależności cech, a ponieważ nie wymaga on spełnienia założeń (poza niezależnością obserwacji w próbie), to można uznać wynik ten za wiarygodny. Jedyny czynnik jaki mógłby wpłynąć na wynik testu \\(\\chi^2\\) to liczebności w tabeli kontyngencji poniżej 5. Dlatego tym bardziej warto przeprowadzić testowanie tej hipotezy za pomocą próbkowania.\n\nKodset.seed(44)\n# obliczamy statystykę testową dla próby\nsample_stat <- gss |> \n  drop_na(partyid, class) |> \n  droplevels() |> \n  specify(partyid~class) |> \n  hypothesise(null = \"independence\") |>\n  calculate(stat = \"Chisq\")\n\n# generujemy próby zgodne z hipotezą zerową za pomocą próbkowania\nnull_chisq_sim <- gss |> \n  drop_na(partyid, class) |> \n  droplevels() |>\n  specify(formula = partyid ~ class) |> \n  hypothesise(null = \"independence\") |> \n  generate(1000, type = \"permute\") |> \n  calculate(stat = \"Chisq\")\n\n# porównujemy rozkład wynikający z hipotezy zerowej z\n# z wartością statystyki obliczoną z próby\nnull_chisq_sim |> \n  visualise() + \n  shade_p_value(obs_stat = sample_stat, direction = \"right\")\n\n# obliczamy p-value\nnull_chisq_sim |> \n  get_p_value(obs_stat = sample_stat, direction = \"right\")\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1   0.002\n\n\n\n\nRysunek 3.9: Porównanie rozkładu statystyki testowej przy założeniu prawdziwości hipotezy zerowej z wartością statystyki testowej z próby\n\n\n\n\nWidzimy, że test każe odrzucić hipotezę o niezależności cech, podobnie jak test teoretyczny. Pakiet infer daje nam możliwość generowania rozkładu teoretycznego zgodnego z hipotezą zerową nieco inaczej niż na podstawie resamplingu.\n\nKodnull_chisq_theory <- gss |> \n  specify(partyid~class) |> \n  assume(distribution = \"Chisq\", df = 9)\n\nnull_chisq_theory |> \n  visualise() + \n  shade_p_value(obs_stat = sample_stat, \n                direction = \"right\")\npchisq(sample_stat$stat, df = 9, lower.tail = F)\n\n   X-squared \n0.0001314536 \n\n\n\n\nRysunek 3.10: Porównanie rozkładu teoretycznego statystyki testowej przy założeniu prawdziwości hipotezy zerowej z wartością statystyki testowej z próby\n\n\n\n\nOczywiście wynik otrzymany metodą teoretyczną pokrywa się dokładnie z wynikiem klasycznego testu \\(\\chi^2\\).\n\n\n\n\n\nCouch, Simon P., Andrew P. Bray, Chester Ismay, Evgeni Chasnovski, Benjamin S. Baumer, i Mine Çetinkaya-Rundel. 2021. „infer: An R package for tidyverse-friendly statistical inference” 6: 3661. https://doi.org/10.21105/joss.03661.\n\n\nKuhn, Max, i Hadley Wickham. 2020. „Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles.” https://www.tidymodels.org."
  },
  {
    "objectID": "measures.html#miary-dopasowania-modeli-regresyjnych",
    "href": "measures.html#miary-dopasowania-modeli-regresyjnych",
    "title": "\n4  Przegląd miar dopasowania modelu\n",
    "section": "\n4.1 Miary dopasowania modeli regresyjnych",
    "text": "4.1 Miary dopasowania modeli regresyjnych\nPrzegląd zaczniemy od najlepiej znanych miar, a skończymy na rzadziej stosowanych, jak funkcja straty Hubera.\n\n4.1.1 \\(R^2\\)\n\nMiara stosowana najczęściej do oceny dopasowania modeli liniowych, a definiowana jako:\n\\[\nR^2=1-\\frac{\\sum_i(y_i-\\hat{y}_i)^2}{\\sum_i(y_i-\\bar{y})^2},\n\\tag{4.1}\\]\ngdzie \\(\\hat{y}_i\\) jest \\(i\\)-tą wartością przewidywaną na podstawie modelu, \\(\\bar{y}\\) jest średnią zmiennej wynikowej, a \\(y_i\\) jest \\(i\\)-tą wartością obserwowaną. Już na kursie modeli liniowych dowiedzieliśmy się o wadach tak zdefiniowanej miary. Wśród nich należy wymienić przede wszystkim fakt, iż dołączając do modelu zmienne, których zdolność predykcyjna jest nieistotna3, to i tak rośnie \\(R^2\\)3 czyli nie mają znaczenia w przewidywaniu wartości wynikowej\nW przypadku modeli liniowych wprowadzaliśmy korektę eliminującą tą wadę, jednak w przypadku modeli predykcyjnych skorygowana miara \\(R^2_{adj}\\) nie wystarcza. W sytuacji gdy modele mają bardzo słabą moc predykcyjną, czyli są np. drzewem regresyjnym bez żadnej reguły podziału4, wówczas można otrzymać ujemne wartości obu miar. Zaleca się zatem wprowadzenie miary, która pozbawiona jest tej wady, a jednocześnie ma tą sama interpretację. Definiuję się ją następująco:4 drzewo składa się tylko z korzenia\n\\[\n\\tilde{R}^2=[\\operatorname{Cor}(Y, \\hat{Y})]^2.\n\\tag{4.2}\\]\nMiara zdefiniowana w (4.2) zapewnia nam wartości w przedziale (0,1), a klasyczna miara (4.1) nie(Kvalseth 1985). Tradycyjna jest zdefiniowana w bibliotece yardstick5 pod nazwą rsq_trad, natomiast miara oparta na korelacji jako rsq. Oczywiście interpretacja jest następująca, że jeśli wartość \\(\\tilde{R}^2\\) jest bliska 1, to model jest dobrze dopasowany, a bliskie 0 oznacza słabe dopasowanie.5 będącej częścią ekosystemu tidymodels\n\n4.1.2 RMSE\nInną powszechnie stosowaną miarą do oceny dopasowania modeli regresyjnych jest pierwiastek błędu średnio-kwadratowego (ang. Root Mean Square Error), zdefiniowany następująco:\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{n}},\n\\tag{4.3}\\]\ngdzie \\(n\\) oznacza liczebność zbioru danych na jakim dokonywana jest ocena dopasowania. Im mniejsza jest wartość błędu RMSE tym lepiej dopasowany jest model. Niestety wadą tej miary jest brak odporności na wartości odstające. Błąd w tym przypadku jest mierzony w tych samych jednostkach co mierzona wielkość wynikowa \\(Y\\). Do wywołania jej używamy funkcji rmse.\n\n4.1.3 MSE\nŚciśle powiązaną miarą dopasowania modelu z RMSE jest błąd średnio-kwadratowy (ang. Mean Square Error). Oczywiście jest on definiowany jako kwadrat RMSE. Interpretacja jest podobna jak w przypadku RMSE. W tym przypadku błąd jest mierzony w jednostkach do kwadratu i również jak w przypadku RMSE miara ta jest wrażliwa na wartości odstające. Wywołujemy ją funkcją mse.\n\n\n\n\n4.1.4 MAE\nChcąc uniknąć (choćby w części) wrażliwości na wartości odstające stosuje się miarę średniego absolutnego błędu (ang. Mean Absolut Error). Definiujemy go następująco:\n\\[\nMAE=\\frac{\\sum_{i=1}^n\\vert y_i-\\hat{y}_i\\vert}{n}.\n\\tag{4.4}\\]\nPonieważ wartości błędów \\(y_i-\\hat{y}_i\\) nie są podnoszone do kwadratu, to miara ta jest mniej wrażliwa na punkty odstające. Interpretacja jej jest podobna jak MSE i RMSE. Do wywołania jej używamy funkcji mae. Błąd w tym przypadku jest również mierzony w tych samych jednostkach co \\(Y\\).\nWymienione miary błędów są nieunormowane, a dopasowania modeli możemy dokonywać jedynie porównując wynik błędu z wartościami \\(Y\\), lub też przez porównanie miar dla różnych modeli.\n\n4.1.5 MAPE\nŚredni bezwzględny błąd procentowy (ang. Mean Absolute Percentage Error) jest przykładem miary błędu wyrażanego w procentach. Definiuje się go następująco:\n\\[\nMAPE=\\frac{1}{n}\\sum_{i=1}^n\\left|\\frac{y_i-\\hat{y}_i}{y_i}\\right|\\cdot 100\\%.\n\\tag{4.5}\\]\nInterpretujemy ten błąd podobnie jak poprzednie pomimo, że jest wyrażony w procentach. Do wywołania go w pakiecie yardstick używamy funkcji mape.\n\n4.1.6 MASE\nŚredni bezwzględny błąd skalowany (ang. Mean Absolute Scaled Error) jest miarą dokładności prognoz. Została zaproponowana w 2005 roku przez statystyka Roba J. Hyndmana i profesora Anne B. Koehler, którzy opisali ją jako “ogólnie stosowaną miarę dokładności prognoz bez problemów widocznych w innych miarach.”(Hyndman i Koehler 2006) Średni bezwzględny błąd skalowany ma korzystne właściwości w porównaniu z innymi metodami obliczania błędów prognoz, takimi jak RMSE, i dlatego jest zalecany do określania dokładności prognoz w szeregach czasowych.(Franses 2016) Definiujemy go następująco\n\\[\nMASE = \\frac{\\sum_{i=1}^n\\vert y_i-\\hat{y}_i\\vert}{\\sum_{i=1}^n\\vert y_i-\\bar{y}_i\\vert}.\n\\tag{4.6}\\]\nDla szeregów czasowych z sezonowością i bez sezonowości definiuje się go jeszcze nieco inaczej(Hyndman i Koehler 2006; 3.4 Evaluating Forecast Accuracy | Forecasting: Principles and Practice (2nd Ed), b.d.) Oczywiście interpretacja jest też podobna jak w przypadku innych miar błędów. Wywołujemy go funkcją mase.\n\n4.1.7 MPE\nŚredni błąd procentowy (ang. Mean Percentage Error) jest miarą błędu względnego definiowaną nastepująco\n\\[\nMPE = \\frac{1}{n}\\sum_{i=1}^n\\frac{y_i-\\hat{y}_i}{y_i}.\n\\tag{4.7}\\]\nPonieważ we wzorze wykorzystywane są rzeczywiste, a nie bezwzględne wartości błędów prognozy, dodatnie i ujemne błędy prognozy mogą się wzajemnie kompensować. W rezultacie wzór ten można wykorzystać jako miarę błędu systematycznego w prognozach. Wadą tej miary jest to, że jest ona nieokreślona zawsze, gdy pojedyncza wartość rzeczywista wynosi zero. Wywołujemy ją za pomocą mpe.\n\n4.1.8 MSD\nŚrednia znakowa różnic (ang. Mean Signed Deviation), znana również jako średnie odchylenie znakowe i średni błąd znakowy, jest statystyką próbkową, która podsumowuje, jak dobrze szacunki \\(\\hat{Y}\\) pasują do wielkości obserwowanych \\(Y\\). Definiujemy ją następująco:\n\\[\nMSD = \\frac{1}{n}\\sum_{i=1}^n(\\hat{y}_i-y_i).\n\\tag{4.8}\\]\nInterpretacja podobnie jak w przypadku innych błędów i mniej wynosi miara tym lepiej dopasowany model. Wywołujemy go funkcją msd.\nIstnieje cały szereg miar specjalistycznych rzadziej stosowanych w zagadnieniach regresyjnych. Wśród nich należy wymienić\n\n4.1.9 Funkcja straty Hubera\nFunkcja straty Hubera (ang. Huber loss) jest nieco bardziej odporną na punkty odstające niż RMSE miarą błędu. Definiujemy ją następująco:\n\\[\nL_{\\delta}(y, \\hat{y})= \\begin{cases}\n  \\frac12 (y_i-\\hat{y}_i)^2, &\\text{ jeśli }\\vert y_i-\\hat{y}_i\\vert\\leq\\delta\\\\\n  \\delta\\cdot \\vert y_i-\\hat{y}_i\\vert-\\tfrac12\\delta, &\\text{ w przeciwnym przypadku}.\n\\end{cases}\n\\tag{4.9}\\] W implementacji yardstick \\(\\delta=1\\) natomiast wyliczanie funkcji straty następuje przez uśrednienie po wszystkih obserwacjach. Z definicji widać, że funkcja straty Hubera jest kombinacją MSE i odpowiednio przekształconej miary MAE, w zależności od tego czy predykcja znacząco odbiegaja od obserwowanych wartości. Wywołujemy ją przez funkcję huber_loss.\n\n4.1.10 Funkcja straty Pseudo-Hubera\nFunkcja straty Pseudo-Hubera (ang. Pseudo-Huber loss) może być stosowana jako gładkie przybliżenie funkcji straty Hubera. Łączy ona najlepsze właściwości straty kwadratowej6 i straty bezwzględnej7, będąc silnie wypukłą, gdy znajduje się blisko celu (minimum) i mniej stromą dla wartości ekstremalnych . Skala, przy której funkcja straty Pseudo-Hubera przechodzi od straty L2 dla wartości bliskich minimum do straty L1 może być kontrolowana przez parametr \\(\\delta\\). Funkcja straty Pseudo-Hubera zapewnia, że pochodne są ciągłe dla wszystkich stopni . Definiujemy ją następująco :6 inaczej w normie L27 w normie L1\n\\[\nL_{\\delta}(y-\\hat{y})=\\delta^2\\left(\\sqrt{1+((y-\\hat{y})/\\delta)^2}-1\\right).\n\\tag{4.10}\\] Wywołujemy ją za pomocą funkcji huber_loss_pseudo.\n\n4.1.11 Logarytm funkcji straty dla rozkładu Poissona\nLogarytm funkcji straty dla rozkładu Poissona (ang. Mean log-loss for Poisson data) definiowany jest w następujący sposób:\n\\[\n\\mathcal{L}=\\frac1n\\sum_{i=11}^n(\\hat{y}_i-y_i\\cdot \\ln(\\hat{y}_i)).\n\\]\nWywołujemy go funkcją poisson_log_los.\n\n4.1.12 SMAPE\nSymetryczny średni bezwzględny błąd procentowy (ang. Symmetric Mean Absolute Percentage Error) jest miarą dokładności opartą na błędach procentowych (lub względnych). Definiujemy ją następująco:\n\\[\nSMAPE = \\frac1n\\sum_{i=1}^n\\frac{\\vert y_i-\\hat{y}_i\\vert}{(|y_i|+|\\hat{y}_i|)/2}\\cdot100\\%.\n\\tag{4.11}\\]\nWywołujemy go funkcją smape.\n\n4.1.13 RPD\nStosunek wydajności do odchylenia standardowego (ang. Ratio of Performance to Deviation) definiujemy jako\n\\[\nRPD = \\frac{SD}{RMSE},\n\\tag{4.12}\\]\ngdzie \\(SD\\) oczywiście oznacza odchylenie standardowe zmiennej zależnej. Tym razem interpretujemy go w ten sposób, że im wyższa jest wartość RPD tym lepiej dopasowany model. Wywołujemy za pomocą rpd.\nW szczególności w dziedzinie spektroskopii, stosunek wydajności do odchylenia (RPD) został użyty jako standardowy sposób raportowania jakości modelu. Jest to stosunek odchylenia standardowego zmiennej do błędu standardowego przewidywania tej zmiennej przez dany model. Jednak jego systematyczne stosowanie zostało skrytykowane przez kilku autorów, ponieważ użycie odchylenia standardowego do reprezentowania rozrzutu zmiennej może być niewłaściwe w przypadku skośnych zbiorów danych. Stosunek wydajności do rozstępu międzykwartylowego został wprowadzony przez Bellon-Maurel i in. (2010) w celu rozwiązania niektórych z tych problemów i uogólnienia RPD na zmienne o rozkładzie nienormalnym.\n\n4.1.14 RPIQ\nStosunek wartości do rozstępu międzykwartylowego (ang. Ratio of Performance to Inter-Quartile) definiujemy następująco:\n\\[\nRPIQ = \\frac{IQ}{RMSE},\n\\tag{4.13}\\]\ngdzie \\(IQ\\) oznacza rozstęp kwartylowy zmiennej zależnej. Wywołujemy go przez funkcję rpiq.\n\n4.1.15 CCC\nKorelacyjny współczynnik zgodności (ang. Concordance Correlation Coefficient) mierzy zgodność pomiędzy wartościami predykcji i obserwowanymi. Definiujemy go w następujący sposób:\n\\[\nCCC = \\frac{2\\rho\\sigma_y\\sigma_{\\hat{y}}}{\\sigma^2_{y}+\\sigma^2_{\\hat{y}}+(\\mu_y-\\mu_{\\hat{y}})^2},\n\\]\ngdzie \\(\\mu_y,\\mu_{\\hat{y}}\\) oznaczają średnią wartości obserwowanych i przewidywanych odpowiednio, \\(\\sigma_{y},\\sigma_{\\hat{y}}\\) stanowią natomiast odchylenia standardowe tych wielkości. \\(\\rho\\) jest współczynnikiem korelacji pomiędzy \\(Y\\) i \\(\\hat{Y}\\). Wywołanie w R to funkcja ccc.\n\n4.1.16 Podsumowanie miar dla modeli regresyjnych\nWśród miar dopasowania modelu można wyróżnić, te które mierzą zgodność pomiędzy wartościami obserwowanymi a przewidywanymi, wyrażone często pewnego rodzaju korelacjami (lub ich kwadratami), a interpretujemy je w ten sposób, że im wyższe wartości tych współczynników tym bardziej zgodne są predykcje z obserwacjami. Drugą duża grupę miar stanowią błędy (bezwzględne i względne), które mierzą w różny sposób różnice pomiędzy wartościami obserwowanymi i przewidywanymi. Jedne są bardziej odporne wartości odstające inne mniej, a wszystkie interpretujemy tak, że jeśli ich wartość jest mniejsza tym lepiej jest dopasowany model.\n\nPrzykład 4.1 Dla zilustrowania działania wspomnianych miar przeanalizujemy przykład modelu regresyjnego. Dla przykładu rozwiążemy zadanie przewidywania wytrzymałości betonu na podstawie jego parametrów. Do tego celu użyjemy danych ze zbioru concrete pakietu modeldata.(Yeh 2006)\n\nKodlibrary(tidymodels)\n\n# charakterystyka danych\nglimpse(concrete)\n\nRows: 1,030\nColumns: 9\n$ cement               <dbl> 540.0, 540.0, 332.5, 332.5, 198.6, 266.0, 380.0, …\n$ blast_furnace_slag   <dbl> 0.0, 0.0, 142.5, 142.5, 132.4, 114.0, 95.0, 95.0,…\n$ fly_ash              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ water                <dbl> 162, 162, 228, 228, 192, 228, 228, 228, 228, 228,…\n$ superplasticizer     <dbl> 2.5, 2.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,…\n$ coarse_aggregate     <dbl> 1040.0, 1055.0, 932.0, 932.0, 978.4, 932.0, 932.0…\n$ fine_aggregate       <dbl> 676.0, 676.0, 594.0, 594.0, 825.5, 670.0, 594.0, …\n$ age                  <int> 28, 28, 270, 365, 360, 90, 365, 28, 28, 28, 90, 2…\n$ compressive_strength <dbl> 79.99, 61.89, 40.27, 41.05, 44.30, 47.03, 43.70, …\n\nKod# modelowania dokonamy bez szczególnego uwzględnienia charakteru zmiennych,\n# tuningowania i innych czynności, które będą nam towarzyszyć w normalnej\n# budowie modelu\n\n# podział danych na uczące i testowe\nset.seed(44)\nsplit <- initial_split(data = concrete,\n                       prop = 0.7)\ntrain_data <- training(split)\ntest_data <- testing(split)\n\n# określenie modeli, wybrałem kNN\nknn5 <-\n  nearest_neighbor(neighbors = 5) |> \n  set_engine('kknn') %>%\n  set_mode('regression')\n\nknn25 <-\n  nearest_neighbor(neighbors = 25) |> \n  set_engine('kknn') %>%\n  set_mode('regression')\n\n# uczymy modele\nfit5 <- knn5 |> \n  fit(compressive_strength~., data = train_data)\n\nfit25 <- knn25 |> \n  fit(compressive_strength~., data = train_data)\n\n# obliczamy predykcję dla obu modeli na obu zbiorach\npred_train5 <- predict(fit5, train_data)\npred_train25 <- predict(fit25, train_data)\npred_test5 <- predict(fit5, test_data)\npred_test25 <- predict(fit25, test_data)\n\n\n\nKodbind_cols(obs = c(train_data$compressive_strength, test_data$compressive_strength),\n          pred5 = c(pred_train5$.pred, pred_test5$.pred),\n          pred25 = c(pred_train25$.pred, pred_test25$.pred)) |> \n  mutate(sample = rep(c(\"train\", \"test\"), c(nrow(train_data), nrow(test_data)))) |> \n  pivot_longer(cols = c(pred5, pred25),\n               names_to = \"model\",\n               values_to = \"pred\") |> \n  mutate(model = case_when(\n    model == \"pred5\" ~ \"knn5\",\n    model == \"pred25\" ~ \"knn25\"\n  )) |> \n  ggplot(aes(x = obs, y = pred))+\n  geom_point(alpha = 0.1)+\n  geom_abline(intercept = 0, \n              slope = 1)+\n  facet_grid(sample~model)+\n  coord_obs_pred()\n\n\n\nRysunek 4.1: Graficzne porównanie obu modeli na obu zbiorach\n\n\n\n\n\nKod# podsumowanie za pomocą miary R2\nbind_cols(obs = c(train_data$compressive_strength, test_data$compressive_strength),\n          pred5 = c(pred_train5$.pred, pred_test5$.pred),\n          pred25 = c(pred_train25$.pred, pred_test25$.pred)) |> \n  mutate(sample = rep(c(\"train\", \"test\"), c(nrow(train_data), nrow(test_data)))) |> \n  pivot_longer(cols = c(pred5, pred25),\n               names_to = \"model\",\n               values_to = \"pred\") |> \n  group_by(model, sample) |> \n  rsq(truth = obs, estimate = pred) |> \n  arrange(model)\n\n# A tibble: 4 × 5\n  model  sample .metric .estimator .estimate\n  <chr>  <chr>  <chr>   <chr>          <dbl>\n1 pred25 test   rsq     standard       0.645\n2 pred25 train  rsq     standard       0.787\n3 pred5  test   rsq     standard       0.737\n4 pred5  train  rsq     standard       0.929\n\nKod# można też podsumować od razu kilkoma miarami\n# będa miary domyślne dla modelu regresyjnego\nbind_cols(obs = c(train_data$compressive_strength, test_data$compressive_strength),\n          pred5 = c(pred_train5$.pred, pred_test5$.pred),\n          pred25 = c(pred_train25$.pred, pred_test25$.pred)) |> \n  mutate(sample = rep(c(\"train\", \"test\"), c(nrow(train_data), nrow(test_data)))) |> \n  pivot_longer(cols = c(pred5, pred25),\n               names_to = \"model\",\n               values_to = \"pred\") |> \n  group_by(model, sample) |> \n  metrics(truth = obs, estimate = pred) |> \n  arrange(model, .metric)\n\n# A tibble: 12 × 5\n   model  sample .metric .estimator .estimate\n   <chr>  <chr>  <chr>   <chr>          <dbl>\n 1 pred25 test   mae     standard       7.73 \n 2 pred25 train  mae     standard       6.50 \n 3 pred25 test   rmse    standard       9.74 \n 4 pred25 train  rmse    standard       8.22 \n 5 pred25 test   rsq     standard       0.645\n 6 pred25 train  rsq     standard       0.787\n 7 pred5  test   mae     standard       6.33 \n 8 pred5  train  mae     standard       3.45 \n 9 pred5  test   rmse    standard       8.26 \n10 pred5  train  rmse    standard       4.68 \n11 pred5  test   rsq     standard       0.737\n12 pred5  train  rsq     standard       0.929\n\nKod# możemy zmienić parametry niektórych miar\nhuber_loss2 <- metric_tweak(\"huber_loss2\", huber_loss, delta = 2)\n\n# można również wybrać jakie miary zostana użyte\nselected_metrics <- metric_set(ccc, rpd, mape, huber_loss2)\n\n\nbind_cols(obs = c(train_data$compressive_strength, test_data$compressive_strength),\n          pred5 = c(pred_train5$.pred, pred_test5$.pred),\n          pred25 = c(pred_train25$.pred, pred_test25$.pred)) |> \n  mutate(sample = rep(c(\"train\", \"test\"), c(nrow(train_data), nrow(test_data)))) |> \n  pivot_longer(cols = c(pred5, pred25),\n               names_to = \"model\",\n               values_to = \"pred\") |> \n  group_by(model, sample) |> \n  selected_metrics(truth = obs, estimate = pred) |> \n  arrange(model, sample)\n\n# A tibble: 16 × 5\n   model  sample .metric     .estimator .estimate\n   <chr>  <chr>  <chr>       <chr>          <dbl>\n 1 pred25 test   ccc         standard       0.750\n 2 pred25 test   rpd         standard       1.64 \n 3 pred25 test   mape        standard      30.9  \n 4 pred25 test   huber_loss2 standard      13.6  \n 5 pred25 train  ccc         standard       0.851\n 6 pred25 train  rpd         standard       2.07 \n 7 pred25 train  mape        standard      24.8  \n 8 pred25 train  huber_loss2 standard      11.1  \n 9 pred5  test   ccc         standard       0.844\n10 pred5  test   rpd         standard       1.93 \n11 pred5  test   mape        standard      24.1  \n12 pred5  test   huber_loss2 standard      10.8  \n13 pred5  train  ccc         standard       0.958\n14 pred5  train  rpd         standard       3.64 \n15 pred5  train  mape        standard      12.8  \n16 pred5  train  huber_loss2 standard       5.19 \n\n\n\nW przypadku gdybyśmy chcieli zdefiniować własną miarę, to oczywiście jest taka możliwość8 polecam stronę pakietu yardstick - https://www.tidymodels.org/learn/develop/metrics/.8 choć liczba już istniejących jest imponująca"
  },
  {
    "objectID": "measures.html#miary-dopasowania-modeli-klasyfikacyjnych",
    "href": "measures.html#miary-dopasowania-modeli-klasyfikacyjnych",
    "title": "\n4  Przegląd miar dopasowania modelu\n",
    "section": "\n4.2 Miary dopasowania modeli klasyfikacyjnych",
    "text": "4.2 Miary dopasowania modeli klasyfikacyjnych\nJak to zostało wspomniane wcześniej w modelach klasyfikacyjnych można podzielić miary dopasowania na te, które dotyczą modeli z binarną zmienną wynikową i ze zmienna wielostanową. Miary można też podzielić na te, które zależą od prawdopodobieństwa poszczególnych stanów i te, które zależą tylko od klasyfikacji wynikowej.\nDo wyliczenia miar probabilistycznych konieczne jest wyliczenie predykcji z prawdopodobieństwami poszczególnych stanów. Aby uzyskać taki efekt wystarczy w predykcji modelu użyć parametru type = \"prob\". W przykładzie podsumowującym miary będzie to zilustrowane.\nNa to, aby przybliżyć miary dopasowania opartych o prawdopodobieństwa stanów, konieczne jest wprowadzenie pojęcia macierzy klasyfikacji (ang. confusion matrix). Można je stosować zarówno do klasyfikacji dwustanowej, jak i wielostanowej. Użyjemy przykładu binarnego aby zilustrować szczegóły tej macierzy.\n\nKod# import danych do przykładu\ndata(two_class_example)\n\n# kilka pierwszych wierszy wyników predykcji\nhead(two_class_example)\n\n   truth      Class1       Class2 predicted\n1 Class2 0.003589243 0.9964107574    Class2\n2 Class1 0.678621054 0.3213789460    Class1\n3 Class2 0.110893522 0.8891064779    Class2\n4 Class1 0.735161703 0.2648382969    Class1\n5 Class2 0.016239960 0.9837600397    Class2\n6 Class1 0.999275071 0.0007249286    Class1\n\nKod# confusion matrix\ntwo_class_example |> \n  conf_mat(truth, predicted) |> \n  autoplot(type = \"heatmap\")\n\n\n\nRysunek 4.2: Przykładowa macierz klasyfikacji\n\n\n\n\nAby przedstawić poszczególne miary na podstawie macierzy klasyfikacji wystarczy przywołać ilustrację z Wikipedii, która w genialny sposób podsumowuje większość miar.\n\n\nRysunek 4.3: Macierz klasyfikacji\n\n\nNa podstawie tej macierzy możemy ocenić dopasowanie modelu za pomocą:\n\naccuacy - informuje o odsetku poprawnie zaklasyfikowanych obserwacji. Jest bardzo powszechnie stosowaną miarą dopasowania modelu choć ma jedną poważną wadę. Mianowicie w przypadku modeli dla danych z wyraźną dysproporcją jednej z klas (powiedzmy jedna stanowi 95% wszystkich obserwacji), może się zdarzyć sytuacja, że nawet bezsensowny model, czyli taki, który zawsze wskazuje tą właśnie wartość, będzie miał accuracy na poziomie 95%.\nkappa - miara podobna miarą do accuracy i jest bardzo przydatna, gdy jedna lub więcej klas dominuje. Definiujemy ją następująco \\(\\kappa = \\frac{p_o-p_e}{1-p_e}\\), gdzie \\(p_o,p_e\\) są odpowiednio zgodnością obserwowaną i oczekiwaną. Zgodność obserwowana jest odsetkiem obserwacji poprawnie zaklasyfikowanych, a oczekiwana to zgodność wynikająca z przypadku.\nprecision - oznaczana też czasem jako PPV (ang. Positive Predictive Value) oznacza stosunek poprawnie zaklasyfikowanych wartości true positive (TP) do wszystkich przewidywanych wartości pozytywnych (ang. positive predictive).\nrecall - nazywany także sensitivity lub True Positive Rate (TPR), który stanowi stosunek true positive do wszystkich przypadków positive.\nspecificity - nazywane również True Negative Rate (TNR), wyraża się stosunkiem pozycji true negative do wszystkich obserwacji negative.\nnegative predictive value (NPV) - oznaczane czasem też jako false omission rate jest liczone jako stosunek false negative do wszystkich przewidywanych negative (PN).\n\\(F_1\\) - jest miarą zdefiniowaną jako \\(\\frac{2PPV*TPR}{PPV+TPR}\\).\nMCC - Mathews Correlation Coeficient - jest zdefiniowany jako \\(\\sqrt{TPR*TNR*PPV*NPV}-\\sqrt{FNR*FPR*NPV*FDR}\\).\nbakanced accuracy - liczona jako średnia sensitivity i specificity.\ndetection prevalence - zdefiniowana jako stosunek poprawnie przewidywanych obserwacji do liczby wszystkich przewidywanych wartości.\nJ index - metryka Youden’a definiowana jako sensitivity + specificity -1, często jest wykorzystywana do określenia poziomu odcięcia prawdopodobieństwa zdarzenia wyróżnionego (ang. threshold).\nkoszt niepoprawnej klasyfikacji - czasami niektóre błędy klasyfikacji są mniej kosztowne z punktu widzenie badacza, a inne bardziej. Wówczas można przypisać koszty błędnych klasyfikacji do poszczególnych klas, nakładając kary za błędne przypisane do innej klasy i w ten sposób zapobiegać takim sytuacjom.\n\nśrednia logarytmu funkcji straty - określana w literaturze jako binary cross-entropy dla przypadku zmiennej wynikowej dwustanowej i multilevel cross-entropy albo categorical cross-entropy w przypadku wielostanowej klasyfikacji. Definiuje się ją następująco:\n\\[\n    \\mathcal{L} = \\frac1n\\sum_{i=1}^n\\left[y_i\\log(\\hat{y}_i)+(1-y_i)\\log(1-\\hat{y}_i)\\right],\n\\tag{4.14}\\] gdzie \\(y_i\\) jest indykatorem klasy wyróżnionej dla \\(i\\)-tej obserwacji, a \\(\\hat{y}_i\\) prawdopodobieństwem wyróżnionego stanu \\(i\\)-tej obserwacji.\n\n\nNależy pamiętać, że większość wspomnianych miar opiera się na wartościach z macierzy klasyfikacji. Przy czym aby obserwacje zaklasyfikować do jednej z klas należy przyjąć pewien punkt odcięcia (threshold) prawdopodobieństwa, od którego przewidywana wartość będzie przyjmowała stan “1”. Domyślnie w wielu modelach ten punkt jest ustalony na poziomie 0,5. Nie jest on jednak optymalny ze względu na jakość klasyfikacji. Zmieniając ten próg otrzymamy różne wartości specificity, sensitivity, precision, recall, itd. Istnieją kryteria doboru progu odcięcia, np. oparte na wartości Youdena, F1, średniej geometrycznej itp. W przykładzie prezentowanym poniżej pokażemy zastosowanie dwóch z tych technik. Bez względu na przyjęty poziom odcięcia istnieją również miary i wykresy, które pozwalają zilustrować jakość modelu. Należą do nich:\n\nwykresy:\n\nROC - Receiver Operating Characteristic - krzywa, która przedstawia kompromis pomiędzy sensitivity i specificity dla różnych poziomów odcięcia.\nPRC - Precision-Recall Curve - krzywa, która pokazuje kompromis pomiędzy precision i recall.\nKrzywa wzrostu (ang. Gain Curve) - to krzywa przedstawiająca stosunek skumulowanej liczby pozytywnych obserwacji w decylu do całkowitej liczby pozytywnych obserwacji w danych.\nKrzywa wyniesienia (ang. Lift Curve) - jest stosunkiem liczby pozytywnych obserwacji w \\(i\\)-tym decylu na podstawie modelu do oczekiwanej liczby pozytywnych obserwacji należących do \\(i\\)-tego decyla na podstawie modelu losowego.\n\n\nmiary:\n\nAUC - Area Under ROC Curve - mierzy pole pod krzywą ROC.\nPRAUC - Area Under Precision-Racall Curve - mierzy pole pod krzywą P-R.\nPole pod krzywą wzrosu.\nPole pod krzywą wyniesienia.\n\n\n\n\nPrzykład 4.2 Tym razem zbudujemy model klasyfikacyjny dla zmiennej wynikowej dwustanowej. Dane pochodzą ze zbioru attrition pakietu modeldata. Naszym zadaniem będzie zbudować modeli przewidujący odejścia z pracy.\n\nKodstr(attrition)\n\n'data.frame':   1470 obs. of  31 variables:\n $ Age                     : int  41 49 37 33 27 32 59 30 38 36 ...\n $ Attrition               : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 1 1 1 1 1 1 ...\n $ BusinessTravel          : Factor w/ 3 levels \"Non-Travel\",\"Travel_Frequently\",..: 3 2 3 2 3 2 3 3 2 3 ...\n $ DailyRate               : int  1102 279 1373 1392 591 1005 1324 1358 216 1299 ...\n $ Department              : Factor w/ 3 levels \"Human_Resources\",..: 3 2 2 2 2 2 2 2 2 2 ...\n $ DistanceFromHome        : int  1 8 2 3 2 2 3 24 23 27 ...\n $ Education               : Ord.factor w/ 5 levels \"Below_College\"<..: 2 1 2 4 1 2 3 1 3 3 ...\n $ EducationField          : Factor w/ 6 levels \"Human_Resources\",..: 2 2 5 2 4 2 4 2 2 4 ...\n $ EnvironmentSatisfaction : Ord.factor w/ 4 levels \"Low\"<\"Medium\"<..: 2 3 4 4 1 4 3 4 4 3 ...\n $ Gender                  : Factor w/ 2 levels \"Female\",\"Male\": 1 2 2 1 2 2 1 2 2 2 ...\n $ HourlyRate              : int  94 61 92 56 40 79 81 67 44 94 ...\n $ JobInvolvement          : Ord.factor w/ 4 levels \"Low\"<\"Medium\"<..: 3 2 2 3 3 3 4 3 2 3 ...\n $ JobLevel                : int  2 2 1 1 1 1 1 1 3 2 ...\n $ JobRole                 : Factor w/ 9 levels \"Healthcare_Representative\",..: 8 7 3 7 3 3 3 3 5 1 ...\n $ JobSatisfaction         : Ord.factor w/ 4 levels \"Low\"<\"Medium\"<..: 4 2 3 3 2 4 1 3 3 3 ...\n $ MaritalStatus           : Factor w/ 3 levels \"Divorced\",\"Married\",..: 3 2 3 2 2 3 2 1 3 2 ...\n $ MonthlyIncome           : int  5993 5130 2090 2909 3468 3068 2670 2693 9526 5237 ...\n $ MonthlyRate             : int  19479 24907 2396 23159 16632 11864 9964 13335 8787 16577 ...\n $ NumCompaniesWorked      : int  8 1 6 1 9 0 4 1 0 6 ...\n $ OverTime                : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 1 1 2 1 1 1 ...\n $ PercentSalaryHike       : int  11 23 15 11 12 13 20 22 21 13 ...\n $ PerformanceRating       : Ord.factor w/ 4 levels \"Low\"<\"Good\"<\"Excellent\"<..: 3 4 3 3 3 3 4 4 4 3 ...\n $ RelationshipSatisfaction: Ord.factor w/ 4 levels \"Low\"<\"Medium\"<..: 1 4 2 3 4 3 1 2 2 2 ...\n $ StockOptionLevel        : int  0 1 0 0 1 0 3 1 0 2 ...\n $ TotalWorkingYears       : int  8 10 7 8 6 8 12 1 10 17 ...\n $ TrainingTimesLastYear   : int  0 3 3 3 3 2 3 2 2 3 ...\n $ WorkLifeBalance         : Ord.factor w/ 4 levels \"Bad\"<\"Good\"<\"Better\"<..: 1 3 3 3 3 2 2 3 3 2 ...\n $ YearsAtCompany          : int  6 10 0 8 2 7 1 1 9 7 ...\n $ YearsInCurrentRole      : int  4 7 0 7 2 7 0 0 7 7 ...\n $ YearsSinceLastPromotion : int  0 1 0 3 2 3 0 0 1 7 ...\n $ YearsWithCurrManager    : int  5 7 0 0 2 6 0 0 8 7 ...\n\nKod# podział zbioru na uczący i testowy\nset.seed(44)\nsplit <- initial_split(attrition, prop = 0.7, strata = \"Attrition\")\ntrain_data <- training(split)\ntest_data <- testing(split)\n\n# określam model\nlr_mod <- logistic_reg() |> \n  set_engine(\"glm\") |> \n  set_mode(\"classification\")\n\n# uczę model\nlr_fit <- lr_mod |> \n  fit(Attrition ~ ., data = train_data)\n\n# podsumowanie modelu\nlr_fit |> \n  tidy() \n\n# A tibble: 58 × 5\n   term                             estimate   std.error statistic   p.value\n   <chr>                               <dbl>       <dbl>     <dbl>     <dbl>\n 1 (Intercept)                     -3.41     1261.       -0.00270  0.998    \n 2 Age                             -0.0375      0.0179   -2.09     0.0364   \n 3 BusinessTravelTravel_Frequently  2.14        0.546     3.92     0.0000875\n 4 BusinessTravelTravel_Rarely      1.31        0.505     2.60     0.00942  \n 5 DailyRate                       -0.000213    0.000279 -0.766    0.444    \n 6 DepartmentResearch_Development   0.783    1261.        0.000621 1.00     \n 7 DepartmentSales                 13.7      1115.        0.0123   0.990    \n 8 DistanceFromHome                 0.0452      0.0137    3.30     0.000964 \n 9 Education.L                      0.353       0.435     0.811    0.417    \n10 Education.Q                      0.170       0.372     0.458    0.647    \n# … with 48 more rows\n\n\nTeraz korzystając z różnych miar podsumujemy dopasowanie modelu.\n\nKod# predykcja z modelu przyjmując threshold = 0.5\npred_class <- predict(lr_fit, new_data = test_data)\n\n# predkcja (prawdopodobieństwa klas)\npred_prob <- predict(lr_fit, new_data = test_data, type = \"prob\")\n\n# ale można też tak\npred <- augment(lr_fit, test_data) |> \n  select(Attrition, .pred_class, .pred_No, .pred_Yes)\n\n# macierz klasyfikacji\ncm <- pred |> \n  conf_mat(truth = Attrition, estimate = .pred_class)\ncm\n\n          Truth\nPrediction  No Yes\n       No  352  39\n       Yes  18  33\n\nKodsummary(cm)\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             binary         0.871\n 2 kap                  binary         0.464\n 3 sens                 binary         0.951\n 4 spec                 binary         0.458\n 5 ppv                  binary         0.900\n 6 npv                  binary         0.647\n 7 mcc                  binary         0.474\n 8 j_index              binary         0.410\n 9 bal_accuracy         binary         0.705\n10 detection_prevalence binary         0.885\n11 precision            binary         0.900\n12 recall               binary         0.951\n13 f_meas               binary         0.925\n\n\nMożemy też narysować krzywe, które nam pokażą, czy dla innych wartości progu model też dobrze przewiduje klasy wynikowe.\n\nKod#ROC\npred |> \n  roc_curve(truth = Attrition, .pred_Yes, event_level = \"second\") |> \n  autoplot()\n\n\n\nKod#PRC\npred |> \n  pr_curve(truth = Attrition, .pred_Yes, event_level = \"second\") |> \n  autoplot()\n\n\n\nKod#AUC\npred |> \n  roc_auc(truth = Attrition, .pred_Yes, event_level = \"second\") \n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.808\n\nKod#PRAUC\npred |> \n  pr_auc(truth = Attrition, .pred_Yes, event_level = \"second\") \n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 pr_auc  binary         0.569\n\n\nWybór optymalnego progu odcięcia.\n\nKodlibrary(probably)\n\n# ustalam zakres threshold\nthresholds <- seq(0.01,1, by = 0.01)\n\n# poszukuje najlepszego progu ze względu kryterium Youden'a\npred |>\n  threshold_perf(Attrition, .pred_Yes, thresholds, event_level = \"second\") |>\n  pivot_wider(names_from = .metric, values_from = .estimate) |>\n  arrange(-j_index)\n\n# A tibble: 100 × 6\n   .threshold .estimator  sens  spec j_index distance\n        <dbl> <chr>      <dbl> <dbl>   <dbl>    <dbl>\n 1       0.19 binary     0.722 0.838   0.560    0.103\n 2       0.17 binary     0.736 0.816   0.552    0.103\n 3       0.18 binary     0.722 0.824   0.547    0.108\n 4       0.16 binary     0.736 0.808   0.544    0.106\n 5       0.2  binary     0.694 0.846   0.540    0.117\n 6       0.22 binary     0.681 0.857   0.537    0.123\n 7       0.15 binary     0.736 0.797   0.533    0.111\n 8       0.21 binary     0.681 0.851   0.532    0.124\n 9       0.12 binary     0.75  0.773   0.523    0.114\n10       0.26 binary     0.639 0.884   0.523    0.144\n# … with 90 more rows\n\nKod# predykjca dla optymalnego progu\npred.optim <- pred |> \n  mutate(class = as.factor(ifelse(.pred_Yes > 0.19, \"Yes\", \"No\")))\n\n# macierz klasyfikacji\ncm2 <- pred.optim |> \n  conf_mat(truth = Attrition, estimate = class)\ncm2\n\n          Truth\nPrediction  No Yes\n       No  310  20\n       Yes  60  52\n\nKodsummary(cm2)\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             binary         0.819\n 2 kap                  binary         0.458\n 3 sens                 binary         0.838\n 4 spec                 binary         0.722\n 5 ppv                  binary         0.939\n 6 npv                  binary         0.464\n 7 mcc                  binary         0.475\n 8 j_index              binary         0.560\n 9 bal_accuracy         binary         0.780\n10 detection_prevalence binary         0.747\n11 precision            binary         0.939\n12 recall               binary         0.838\n13 f_meas               binary         0.886\n\n\n\n\n4.2.1 Miary dopasowania dla modeli ze zmienną wynikową wieloklasową\nWspomniane zostało, że miary dedykowane dla modeli binarnych można również wykorzystać do modeli ze zmienną zależną wielostanową. Oczywiście wówczas trzeba użyć pewnego rodzaju uśredniania. Implementacje wieloklasowe wykorzystują mikro, makro i makro-ważone uśrednianie, a niektóre metryki mają swoje własne wyspecjalizowane implementacje wieloklasowe.\n\n4.2.1.1 Makro uśrednianie\nMakro uśrednianie redukuje wieloklasowe predykcje do wielu zestawów przewidywań binarnych. Oblicza się odpowiednią metrykę dla każdego z przypadków binarnych, a następnie uśrednia wyniki. Jako przykład, rozważmy precision. W przypadku wieloklasowym, jeśli istnieją poziomy A, B, C i D, makro uśrednianie redukuje problem do wielu porównań jeden do jednego. Kolumny truth i estimate są rekodowane tak, że jedynymi dwoma poziomami są A i inne, a następnie precision jest obliczana w oparciu o te rekodowane kolumny, przy czym A jest “wyróżnioną” kolumną. Proces ten jest powtarzany dla pozostałych 3 poziomów, aby uzyskać łącznie 4 wartości precyzji. Wyniki są następnie uśredniane.\nFormuła dla \\(k\\) klas wynikowych prezentuje się następująco:\n\\[\nPr_{macro} = \\frac{Pr_1+Pr_2+\\ldots+Pr_k}{k},\n\\tag{4.15}\\]\ngdzie \\(Pr_i\\) oznacza precision dla \\(i\\)-tej klasy.\n\n4.2.1.2 Makro-ważone uśrednianie\nMakro-ważone uśrednianie jest co do zasady podobne do metody makro uśredniania, z tą jednak zmianą, że wagi poszczególnych czynników w średniej zależą od liczności tych klas, co sprawia, że miara ta jest bardziej optymalna w przypadku wyraźnych dysproporcji zmiennej wynikowej. Formalnie obliczamy to wg reguły:\n\\[\nPr_{weighted-macro}=Pr_1\\frac{\\#Obs_1}{n}+Pr_2\\frac{\\#Obs_2}{n}+\\ldots+Pr_k\\frac{\\#Obs_k}{n},\n\\tag{4.16}\\]\ngdzie \\(\\#Obs_i\\) oznacza liczbę obserwacji w grupie \\(i\\)-tej, a \\(n\\) jest liczebnością całego zbioru.\n\n4.2.1.3 Mikro uśrednianie\nMikro uśrednianie traktuje cały zestaw danych jako jeden wynik zbiorczy i oblicza jedną metrykę zamiast \\(k\\) metryk, które są uśredniane. Dla precision działa to poprzez obliczenie wszystkich true positive wyników dla każdej klasy i użycie tego jako licznika, a następnie obliczenie wszystkich true positive i false positive wyników dla każdej klasy i użycie tego jako mianownika.\n\\[\nPr_{micro} = \\frac{TP_1+TP_2+\\ldots TP_k}{(TP_1+TP_2+\\ldots TP_k)+(FP_1+FP_2+\\ldots FP_k)}.\n\\tag{4.17}\\]\nW tym przypadku, zamiast klas o równej wadze, mamy obserwacje z równą wagą. Dzięki temu klasy z największą liczbą obserwacji mają największy wpływ na wynik.\n\nPrzykład 4.3 Przykład użycia miar dopasowania modelu dla zmiennych wynikowych wieloklasowych.\n\nKod# predykcja wykonana dla sprawdzianu krzyżowego\n# bedzie nas interesować tylko wynik pierwszego folda\nhead(hpc_cv)\n\n  obs pred        VF          F           M            L Resample\n1  VF   VF 0.9136340 0.07786694 0.008479147 1.991225e-05   Fold01\n2  VF   VF 0.9380672 0.05710623 0.004816447 1.011557e-05   Fold01\n3  VF   VF 0.9473710 0.04946767 0.003156287 4.999849e-06   Fold01\n4  VF   VF 0.9289077 0.06528949 0.005787179 1.564496e-05   Fold01\n5  VF   VF 0.9418764 0.05430830 0.003808013 7.294581e-06   Fold01\n6  VF   VF 0.9510978 0.04618223 0.002716177 3.841455e-06   Fold01\n\nKod# macierz klasyfikacji\ncm <- hpc_cv |> \n  conf_mat(truth = obs, estimate = pred)\ncm\n\n          Truth\nPrediction   VF    F    M    L\n        VF 1620  371   64    9\n        F   141  647  219   60\n        M     6   24   79   28\n        L     2   36   50  111\n\nKod# poniższe miary są makro uśrednione\nsummary(cm)\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             multiclass     0.709\n 2 kap                  multiclass     0.508\n 3 sens                 macro          0.560\n 4 spec                 macro          0.879\n 5 ppv                  macro          0.631\n 6 npv                  macro          0.896\n 7 mcc                  multiclass     0.515\n 8 j_index              macro          0.440\n 9 bal_accuracy         macro          0.720\n10 detection_prevalence macro          0.25 \n11 precision            macro          0.631\n12 recall               macro          0.560\n13 f_meas               macro          0.570\n\nKod# poniższe miary są makro uśrednione\nsummary(cm, estimator = \"micro\")\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             multiclass     0.709\n 2 kap                  multiclass     0.508\n 3 sens                 micro          0.709\n 4 spec                 micro          0.903\n 5 ppv                  micro          0.709\n 6 npv                  micro          0.903\n 7 mcc                  multiclass     0.515\n 8 j_index              micro          0.612\n 9 bal_accuracy         micro          0.806\n10 detection_prevalence micro          0.25 \n11 precision            micro          0.709\n12 recall               micro          0.709\n13 f_meas               micro          0.709\n\nKod# ROC\nhpc_cv |> \n  roc_curve(truth = obs, VF:L) |> \n  autoplot()\n\n\n\nKod# AUC\nhpc_cv |> \n  roc_auc(truth = obs, VF:L)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc hand_till      0.829\n\n\n\n\n\n\n\n3.4 Evaluating Forecast Accuracy | Forecasting: Principles and Practice (2nd Ed). b.d.\n\n\nBellon-Maurel, Véronique, Elvira Fernandez-Ahumada, Bernard Palagos, Jean-Michel Roger, i Alex McBratney. 2010. „Critical Review of Chemometric Indicators Commonly Used for Assessing the Quality of the Prediction of Soil Attributes by NIR Spectroscopy”. TrAC Trends in Analytical Chemistry 29 (9): 1073–81. https://doi.org/10.1016/j.trac.2010.05.006.\n\n\nFranses, Philip Hans. 2016. „A Note on the Mean Absolute Scaled Error”. International Journal of Forecasting 32 (1): 20–22. https://doi.org/10.1016/j.ijforecast.2015.03.008.\n\n\nHyndman, Rob J., i Anne B. Koehler. 2006. „Another Look at Measures of Forecast Accuracy”. International Journal of Forecasting 22 (4): 679–88. https://doi.org/10.1016/j.ijforecast.2006.03.001.\n\n\nKvalseth, Tarald O. 1985. „Cautionary Note about R2”. The American Statistician 39 (4): 279–85. https://doi.org/10.2307/2683704.\n\n\nYeh, I.-Cheng. 2006. „Analysis of Strength of Concrete Using Design of Experiments and Neural Networks”. Journal of Materials in Civil Engineering 18 (4): 597–604. https://doi.org/10.1061/(ASCE)0899-1561(2006)18:4(597)."
  },
  {
    "objectID": "resampling.html#podział-na-próbę-uczącą-i-testową",
    "href": "resampling.html#podział-na-próbę-uczącą-i-testową",
    "title": "\n5  Zarządzanie danymi\n",
    "section": "\n5.1 Podział na próbę uczącą i testową",
    "text": "5.1 Podział na próbę uczącą i testową\nPodstawowym podejściem w empirycznej walidacji modelu jest podział istniejącej puli danych na dwa odrębne zbiory - treningowy i testowy. Jedna część danych jest wykorzystywana do budowy i optymalizacji modelu. Ten zbiór treningowy stanowi zazwyczaj większość danych. Dane te służą dla budowy modelu, poszukiwania optymalnych parametrów modelu, czy selekcji cech istotnych z punktu widzenia predykcji.\nPozostała część danych stanowi zbiór testowy. Jest on trzymany aż do momentu, gdy jeden lub dwa modele zostaną wybrane jako metody najlepiej opisujące badane zjawisko. Zestaw testowy jest wtedy używany jako ostateczny arbiter do określenia dopasowania modelu. Krytyczne jest, aby użyć zbiór testowy tylko raz; w przeciwnym razie staje się on częścią procesu modelowania.\nProporcje w jakich należy podzielić dane nie są wyraźnie sprecyzowane. Choć istnieją prace, jak np. Joseph (2022), które wskazują konkretne reguły podziału zbioru danych na uczący i testowy dla modeli regresyjnych, to nie ma co do tego powszechnej zgody, że tylko jedna proporcja jest optymalna. W przypadku wspomnianej metody próba testowa powinna stanowić\n\\[\n\\frac{1}{\\sqrt{p}+1},\n\\]\nzbioru danych, gdzie \\(p\\) oznacza liczbę predyktorów modelu.\n\n\n\n\n\n\nWskazówka\n\n\n\nDecydując się na wybór proporcji podziału należy pamiętać, że przy mniejszej ilości danych uczących, oszacowania parametrów mają wysoką wariancję. Z drugiej strony, mniejsza ilość danych testowych prowadzi do wysokiej wariancji w miarach dopasowania (czyli mamy duże różnice pomiędzy dopasowaniem na zbiorze uczącym i testowym).\n\n\nW podziale zbioru danych na uczący i testowy, ważny jest jeszcze jeden aspekt. W jaki sposób podziału dokonujemy. Najpowszechniej stosowany jest podział losowy (losowanie proste). Próbkowanie losowe jest najstarszą i najbardziej popularną metodą dzielenia zbioru danych. Jak sama nazwa wskazuje, losowane są indeksy obserwacji, które będą przyporządkowane do zbioru uczącego. Pozostałe obserwacje będą należeć do zbioru testowego.\nMetoda ta ma jednak istotną wadę. Próbkowanie losowe działa poprawnie na zbiorach danych zbalansowanych klasowo, czyli takich, w których liczba próbek w każdej kategorii jest mniej więcej taka sama. W przypadku zbiorów danych niezbalansowanych klasowo, taka metoda podziału danych może tworzyć obciążenie modelu.\nNa przykład, jeśli zbiór danych zawiera 100 obrazów, z których 80 należy do kategorii “pies” i 20 należy do kategorii “kot”, a losowe próbkowanie jest stosowane do podziału danych na zbiory uczący i testowy w stosunku 80%-20% (odpowiednio), może się tak zdarzyć, że zbiór treningowy składa się tylko z obrazów psów, podczas gdy zbiór testowy składa się tylko z obrazów kotów. Nawet jeśli nie zdarzy się tak ekstremalny przypadek, to nierównowaga rozkładów w obu zbiorach może być wyraźna.\nLosowanie warstwowe zastosowane do podziału zbioru danych łagodzi problem próbkowania losowego w zbiorach danych z niezrównoważonym rozkładem klas. W tym przypadku, rozkład klas w każdym z zestawów treningowych i testowych jest zachowany.\nZałóżmy, że zbiór danych składa się z 100 obrazów, z których 60 to obrazy psów, a 40 to obrazy kotów. W takim przypadku próbkowanie warstwowe zapewnia, że 60% obrazów należy do kategorii “pies”, a 40% do kategorii “kot” w zbiorach uczącym i testowym. Oznacza to, że jeśli pożądany jest podział w proporcji 80%-20%, z 80 obrazów w zbiorze treningowym, 48 obrazów (60%) będzie należało do psów, a pozostałe 32 (40%) do kotów.\nRozważmy inny przykład. W zadaniach wykrywania obiektów, pojedyncze obrazy mogą zawierać kilka różnych obiektów należących do różnych kategorii. W zbiorze danych niektóre obrazy mogą zawierać 10 psów, ale tylko 1 osobę, inne mogą zawierać 10 osób i 2 psy, a jeszcze inne mogą zawierać 5 kotów i 5 psów, bez ludzi. W takich przypadkach, losowy podział obrazów może zakłócić rozkład kategorii obiektów. Próbkowanie warstwowe z drugiej strony pozwala podzielić dane w taki sposób, że wynikowy rozkład kategorii obiektów jest zrównoważony.\n\n\n\n\n\n\nWskazówka\n\n\n\nW przypadku dużych zbiorów danych różnice pomiędzy losowaniem prostym i warstwowym się zacierają.\n\n\n\nPrzykład 5.1 Przykładem zastosowania obu wspomnianych technik będzie podział zbioru attrition pakietu modeldata.\n\nKodlibrary(tidymodels)\n# proporcje zmiennej wynikowej w całym zbiorze\nset.seed(4)\ndane <- attrition |> \n  slice_sample(n = 100)\n\ndane|> \n  count(Attrition) |> \n  mutate(prop = round(n/sum(n)*100, 0))\n\n  Attrition  n prop\n1        No 83   83\n2       Yes 17   17\n\nKodset.seed(4)\n# podział losowy\nrandom_split <- initial_split(dane, prop = 0.9)\nrandom_split\n\n<Training/Testing/Total>\n<90/10/100>\n\nKodtr_random <- training(random_split)\nte_random <- testing(random_split)\n\n# podział warstwowy\nstrata_split <- initial_split(dane, prop = 0.9, strata = Attrition)\nstrata_split\n\n<Training/Testing/Total>\n<89/11/100>\n\nKodtr_strata <- training(strata_split)\nte_strata <- testing(strata_split)\n\n# proporcje zmiennej wynikowej w obu zbiorach uczących\ntr_random |> \n  count(Attrition) |> \n  mutate(prop = round(n/sum(n)*100, 0))\n\n  Attrition  n prop\n1        No 73   81\n2       Yes 17   19\n\nKodtr_strata |> \n  count(Attrition) |> \n  mutate(prop = round(n/sum(n)*100, 0))\n\n  Attrition  n prop\n1        No 74   83\n2       Yes 15   17\n\n\n\nCo ciekawe parametru strata w funkcji initial_split można też użyć do zmiennej ilościowej. Wówczas algorytm podzieli próbę z zachowaniem proporcji obserwacji w kolejnych kwartylach. Ta technika może się okazać szczególnie ważna w przypadku gdy zmienna wynikowa ma znaczącą asymetrię rozkładu.\n\nPrzykład 5.2  \n\nKod# rozkład zmiennej Sale_Price\nset.seed(44)\ndane <- ames |> \n  slice_sample(n = 200)\np1 <- dane |> \n  ggplot(aes(Sale_Price))+\n  geom_density(fill = \"grey\")+\n  labs(title = \"Oryginał\")\n\n# podział losowy\nset.seed(44)\names_split <- initial_split(dane, prop = 0.8)\names_train1 <- training(ames_split)\names_test1 <- testing(ames_split)\n\np2 <- bind_rows(ames_train1, ames_test1) |> \n  mutate(sample = rep(c(\"train\", \"test\"), \n                      c(nrow(ames_train1), nrow(ames_test1)))) |>\n  ggplot(aes(Sale_Price))+\n  geom_density(aes(fill = sample), alpha = 0.5)+\n  labs(title = \"Losowanie proste\")\n\n\names_split <- initial_split(dane, prop = 0.8, strata = Sale_Price)\names_train2 <- training(ames_split)\names_test2 <- testing(ames_split)\n\np3 <- bind_rows(ames_train2, ames_test2) |> \n  mutate(sample = rep(c(\"train\", \"test\"), \n                      c(nrow(ames_train1), nrow(ames_test1)))) |>\n  ggplot(aes(Sale_Price))+\n  geom_density(aes(fill = sample), alpha = 0.5)+\n  labs(title = \"Losowanie warstwowe\")\n\nlibrary(patchwork)\np1/p2/p3\n\n\n\nRysunek 5.1: Porównanie próbkowań dla zmiennej typu ilościowego\n\n\n\n\n\n\n\n\n\n\n\nWskazówka\n\n\n\nProporcja użyta do podziału, w dużym stopniu zależy od kontekstu rozpatrywanego problemu. Zbyt mała ilość danych w zbiorze treningowym ogranicza zdolność modelu do znalezienia odpowiednich oszacowań parametrów. I odwrotnie, zbyt mała ilość danych w zbiorze testowym obniża jakość oszacowań wydajności. Niektórzy statystycy unikają zbiorów testowych w ogóle, ponieważ uważają, że wszystkie dane powinny być używane do estymacji parametrów. Chociaż argument ten jest słuszny, dobrą praktyką modelowania jest posiadanie bezstronnego zestawu obserwacji jako ostatecznego arbitra jakości modelu. Zestawu testowego należy unikać tylko wtedy, gdy dane są drastycznie małe.\n\n\nJeszcze jedna uwaga na temat losowego podziału zbioru na uczący i testowy. W jednej sytuacji podział losowy i warstwowy nie są najlepszym rozwiązaniem - chodzi o szeregi czasowe lub dane zawierające znaczący czynnik zmienności w czasie. Wówczas stosuje się podział zbioru za pomocą funkcji initial_time_split, która parametrem prop określa jaka proporcja obserwacji z początku zbioru danych będzie wybrana do zbioru uczącego1.1 zakłada się, że dane są posortowane wg czasu\nOpisując cele podziału danych, wyróżniliśmy zbiór testowy jako dane, które powinny być wykorzystane do właściwej oceny działania modelu na modelu końcowym (modelach). To rodzi pytanie: “Jak możemy określić, co jest najlepsze, jeśli nie mierzymy wydajności aż do zbioru testowego?”.\nCzęsto słyszy się o zbiorach walidacyjnych jako odpowiedzi na to pytanie, szczególnie w literaturze dotyczącej sieci neuronowych i głębokiego uczenia. U początków sieci neuronowych badacze zdali sobie sprawę, że mierzenie wydajności poprzez przewidywanie na podstawie zbioru treningowego prowadziło do wyników, które były zbyt optymistyczne (nierealistycznie). Prowadziło to do modeli, które były nadmiernie dopasowane, co oznaczało, że osiągały bardzo dobre wyniki na zbiorze treningowym, ale słabe na zbiorze testowym. Aby ograniczyć ten problem, wybierano ze zbioru uczącego niewielki zbiór walidacyjny i użyto go do pomiaru wydajności podczas trenowania sieci. Gdy poziom błędu na zbiorze walidacyjnym zaczynał rosnąć, trening był wstrzymywany. Innymi słowy, zbiór walidacyjny był środkiem do uzyskania przybliżonego poczucia, jak dobrze model działał przed zbiorem testowym.\nW przypadku danych dotyczących mieszkań w Ames, nieruchomość jest traktowana jako niezależna jednostka eksperymentalna. Można bezpiecznie założyć, że statystycznie dane z danej nieruchomości są niezależne od innych nieruchomości. W przypadku innych zastosowań nie zawsze tak jest:\n\nNa przykład w przypadku danych podłużnych (ang. longitudinal data) ta sama niezależna jednostka eksperymentalna może być mierzona w wielu punktach czasowych. Przykładem może być osoba w badaniu medycznym.\nPartia wyprodukowanego produktu może być również uważana za niezależną jednostkę doświadczalną. W powtarzanych pomiarach, replikowane punkty danych z partii są zbierane w wielu momentach.\n\nJohnson i in. (2018) opisują eksperyment, w którym próbki różnych drzew były pobierane w górnej i dolnej części pnia. Tutaj drzewo jest jednostką eksperymentalną, a hierarchię danych stanowi próbka w obrębie części pnia dla każdego drzewa.\n\nKuhn i Johnson (2019) zawiera inne przykłady, w których dla pojedyncze obserwacje mierzone są kilkukrotnie, losowanie przypadków wśród wierszy nie zapewni niezależności obserwacji. Wówczas powinno się losować całe zestawy obserwacji przyporządkowanych do jednostki eksperymentalnej. Operatem losowania powinna być lista jednostek eksperymentalnych.\n\n\n\n\n\n\nWażne\n\n\n\nProblem wycieku informacji (ang. data leakage) występuje, gdy w procesie modelowania wykorzystywane są dane spoza zbioru treningowego\n\n\nNa przykład, w konkursach uczenia maszynowego, dane zbioru testowego mogą być dostarczone bez prawdziwych wartości zmiennej wynikowej, aby model mógł być rzetelnie oceniony. Jedną z potencjalnych metod poprawy wyniku może być dopasowanie modelu przy użyciu obserwacji zbioru treningowego, które są najbardziej podobne do wartości zbioru testowego. Chociaż zbiór testowy nie jest bezpośrednio wykorzystywany do dopasowania modelu, nadal ma duży wpływ. Ogólnie rzecz biorąc, ta technika jest wysoce problematyczna, ponieważ zmniejsza błąd generalizacji modelu w celu optymalizacji wydajności na określonym zbiorze danych. Istnieją bardziej subtelne sposoby, w jakie dane z zestawu testowego mogą być wykorzystywane podczas szkolenia. Zapisywanie danych treningowych w oddzielnej ramce danych od zbioru testowego jest jednym sposobem zapewnienia, aby wyciek informacji nie nastąpił przez przypadek.\nPo drugie, techniki podpróbkowania (ang. subsampling) zbioru treningowego mogą łagodzić pewne problemy (np. nierównowagę klas). Jest to ważna i powszechna technika, która celowo powoduje, że dane zbioru treningowego odbiegają od populacji, z której dane zostały pobrane. Ważne jest, aby zbiór testowy nadal odzwierciedlał to, co model napotkałby w środowisku naturalnym.\nNa początku tego rozdziału, ostrzegaliśmy przed używaniem tych samych danych do różnych zadań. W dalszej części tej książki omówimy metodologie wykorzystania danych, które pozwolą zmniejszyć ryzyko związane z tendencyjnością, nadmiernym dopasowaniem i innymi problemami.\n\n\n\n\nJohnson, Dan, Phoebe Eckart, Noah Alsamadisi, Hilary Noble, Celia Martin, i Rachel Spicer. 2018. „Polar Auxin Transport Is Implicated in Vessel Differentiation and Spatial Patterning During Secondary Growth in Populus”. American Journal of Botany 105 (2): 186–96. https://doi.org/10.1002/ajb2.1035.\n\n\nJoseph, V. Roshan. 2022. „Optimal Ratio for Data Splitting”. Statistical Analysis and Data Mining: The ASA Data Science Journal 15 (4): 531–38. https://doi.org/10.1002/sam.11583.\n\n\nKuhn, Max, i Kjell Johnson. 2019. Feature Engineering and Selection. Chapman; Hall/CRC. https://doi.org/10.1201/9781315108230."
  },
  {
    "objectID": "tidymodels.html#budowa-modelu",
    "href": "tidymodels.html#budowa-modelu",
    "title": "\n6  Praca z tidymodels\n",
    "section": "\n6.1 Budowa modelu",
    "text": "6.1 Budowa modelu\nOddzielny pakiet przeznaczony do budowy modeli zawarty w ekosystemie tidymodels o nazwie parsnip pozwala w uniwersalny sposób budować i dopasowywać modele. Wracając do przykładu modelu liniowego postaramy się pokazać wszystkie zalety tego podejścia. Choć regresje liniową możemy zbudować z wykorzystaniem 11 różnych pakietów, to my się ograniczymy tylko do stats, glmnet i rstanarm.\n\nKod# w wersji klasycznej należałoby je budować następująco\nmod_stat <- lm(formula, data = ...)\nmod_glmnet <- glmnet(x = matrix, y = vector, family = \"gaussian\", ...)\nmod_stan <- stan_glm(formula, data, family = \"gaussian\", ...)\n\n\nJuż na poziomie definiowana modeli widzimy różnice w definicjach, np. glmnet potrzebuje danych w formacie \\((X,y)\\). W przypadku tidymodels podejście do określania modelu ma być bardziej zunifikowane:\n\nOkreśl typ modelu na podstawie jego struktury matematycznej (np. regresja liniowa, las losowy, KNN, itp.).\nOkreślenie silnika do dopasowania modelu. Najczęściej odzwierciedla to pakiet, który powinien być użyty, jak stan1 lub glmnet. Są to modele same w sobie, a parsnip zapewnia spójne interfejsy, używając ich jako silników do modelowania.\nJeśli jest to wymagane, zadeklaruj tryb pracy modelu2. Tryb odzwierciedla typ przewidywanego wyniku. Dla wyników liczbowych trybem jest regresja; dla wyników jakościowych jest to klasyfikacja. Jeśli algorytm modelu może realizować tylko jeden typ wyniku, takim jak regresja liniowa, tryb jest już ustawiony.\n\n1 jest to bibliotek języka C++2 klasyfikacja czy regresja\nKodlibrary(tidymodels)\n\n# to samo z wykorzystaniem parsnip\nlinear_reg() %>% set_engine(\"lm\")\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nKodlinear_reg() %>% set_engine(\"glmnet\")\n\nLinear Regression Model Specification (regression)\n\nComputational engine: glmnet \n\nKodlinear_reg() %>% set_engine(\"stan\")\n\nLinear Regression Model Specification (regression)\n\nComputational engine: stan \n\n\nPo ustaleniu modeli można je podać uczeniu, za pomocą funkcji fit w przypadku gdy określaliśmy zależność formułą lub fit_xy gdy zmienne niezależne i zależna były określone oddzielnie. Drugi przypadek ma miejsce gdy w procedurze przygotowania danych mamy je w postaci \\((X,y)\\). Nie mniej jednak pakiet parsnip pozwala na użycie fit nawet gdy oryginalna funkcja wymagała podania zmiennych niezależnych i zależnej. Ponadto funkcja translate pozwala na przetłumaczenie modelu parsnip na język danego pakietu.\n\nKodlinear_reg() %>% set_engine(\"lm\") |> translate()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nModel fit template:\nstats::lm(formula = missing_arg(), data = missing_arg(), weights = missing_arg())\n\nKodlinear_reg(penalty = 1) %>% set_engine(\"glmnet\") |> translate()\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1\n\nComputational engine: glmnet \n\nModel fit template:\nglmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    family = \"gaussian\")\n\nKodlinear_reg() %>% set_engine(\"stan\") |> translate()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: stan \n\nModel fit template:\nrstanarm::stan_glm(formula = missing_arg(), data = missing_arg(), \n    weights = missing_arg(), family = stats::gaussian, refresh = 0)\n\n\nWykorzystując dane ames dopasujemy cenę (Sale_Price) na podstawie długości i szerokości geograficznej domu.\n\nKodset.seed(44)\names <- ames %>% mutate(Sale_Price = log10(Sale_Price))\names_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train <- training(ames_split)\names_test  <-  testing(ames_split)\n\nlm_model <- \n  linear_reg() %>% \n  set_engine(\"lm\")\n\nlm_form_fit <- \n  lm_model %>% \n  fit(Sale_Price ~ Longitude + Latitude, data = ames_train)\n\nlm_xy_fit <- \n  lm_model %>% \n  fit_xy(\n    x = ames_train %>% select(Longitude, Latitude),\n    y = ames_train %>% pull(Sale_Price)\n  )\n\nlm_form_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Sale_Price ~ Longitude + Latitude, data = data)\n\nCoefficients:\n(Intercept)    Longitude     Latitude  \n   -321.755       -2.091        3.120  \n\nKodlm_xy_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)    Longitude     Latitude  \n   -321.755       -2.091        3.120  \n\n\nKolejną zaletą pakietu parsnip jest unifikacja nazw parametrów modeli. Dla przykładu gdybyśmy chcieli dopasować trzy różne modele lasów losowych, korzystając z pakietów ranger, randomForest i sparklyr, musielibyśmy określać parametry modelu używając za każdym razem innych nazw.\n\n\nRysunek 6.1: Różne sposoby określania parametrów modelu\n\n\nW przypadku budowy w parsnip nazwy parametrów zostały zunifikowane:\n\n\nmtry - liczba wybranych predyktorów;\n\ntrees - liczba drzew;\n\nmin_n - minimalna liczba obserwacji aby dokonać podziału.\n\nUnifikacja po pierwsze pozwala lepiej zapamiętać nazwy parametrów, a po drugie ich nazwy są zrozumiałe dla czytelnika, który nie koniecznie musi się znać na różnicach pomiędzy pakietami.\nDla wspomnianego przykładu lasów losowych, model można zdefiniować następująco.\n\nKodrand_forest(trees = 1000, min_n = 5) %>% \n  set_engine(\"ranger\") %>% \n  set_mode(\"regression\") %>% \n  translate() # translate nie musi być używane, w tym przypadku było\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n  min_n = 5\n\nComputational engine: ranger \n\nModel fit template:\nranger::ranger(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    num.trees = 1000, min.node.size = min_rows(~5, x), num.threads = 1, \n    verbose = FALSE, seed = sample.int(10^5, 1))\n\nKod# użyte aby pokazać jak parsnip zamienił z unikalnej funkcji \n# rand_forest na model ranger\n\n\nGłówne parametry modelu są przekazywane przez główną funkcję (w przykładzie była to rand_forest), ale pozostałe parametry, charakterystyczne dla danego silnika można przekazać przez argumenty silnika.\n\nKodrand_forest(trees = 1000, min_n = 5) %>% \n  set_engine(\"ranger\", verbose = TRUE) %>% \n  set_mode(\"regression\") # parametr verbose = T przekazany został oddzielnie\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n  min_n = 5\n\nEngine-Specific Arguments:\n  verbose = TRUE\n\nComputational engine: ranger"
  },
  {
    "objectID": "tidymodels.html#użycie-modelu",
    "href": "tidymodels.html#użycie-modelu",
    "title": "\n6  Praca z tidymodels\n",
    "section": "\n6.2 Użycie modelu",
    "text": "6.2 Użycie modelu\nPo utworzeniu i dopasowaniu modelu możemy wykorzystać wyniki na wiele sposobów; możemy chcieć narysować, podsumować lub w inny sposób zbadać model wyjściowy. W obiekcie modelu parsnip przechowywanych jest kilka wielkości, w tym dopasowany model. Można go znaleźć w elemencie o nazwie fit, który może być zwrócony za pomocą funkcji extract_fit_engine.\n\nKodlm_form_fit %>% extract_fit_engine()\n\n\nCall:\nstats::lm(formula = Sale_Price ~ Longitude + Latitude, data = data)\n\nCoefficients:\n(Intercept)    Longitude     Latitude  \n   -321.755       -2.091        3.120  \n\nKodlm_form_fit %>% extract_fit_engine() %>% vcov()\n\n            (Intercept)    Longitude     Latitude\n(Intercept)  214.194437  1.611665120 -1.505256159\nLongitude      1.611665  0.016726120 -0.001079561\nLatitude      -1.505256 -0.001079561  0.033404800\n\n\n\n\n\n\n\n\nZagrożenie\n\n\n\nNigdy nie przekazuj elementu fit modelu parsnip do funkcji predict(lm_form_fit), tzn. nie używaj predict(lm_form_fit$fit). Jeśli dane zostały wstępnie przetworzone w jakikolwiek sposób, zostaną wygenerowane nieprawidłowe predykcje (czasami bez błędów). Funkcja predykcji modelu bazowego nie ma pojęcia czy jakiekolwiek przekształcenia zostały dokonane na danych przed uruchomieniem modelu.\n\n\nKolejną zaletę unifikacji parsnip możemy dostrzec przeglądając podsumowanie modeli. Nie zawsze wyniki modelu są przedstawiane w jednakowy sposób. Czasami różnice są niewielkie, gdy w jednym podsumowaniu zobaczymy p-value a w innym Pr(>|t|) ale czasem mogą być większe. I o ile nie da się zunifikować wszystkich podsumować modeli, ponieważ zawierają różne elementy, to w pakiecie parsnip korzysta się z funkcji tidy pakietu broom do podsumowania modelu.\n\nKodtidy(lm_form_fit)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  -322.      14.6       -22.0 1.57e-97\n2 Longitude      -2.09     0.129     -16.2 7.71e-56\n3 Latitude        3.12     0.183      17.1 1.17e-61\n\n\nOczywiście nie wszystkie modele da się w ten sposób podsumować."
  },
  {
    "objectID": "tidymodels.html#predykcja-z-modelu",
    "href": "tidymodels.html#predykcja-z-modelu",
    "title": "\n6  Praca z tidymodels\n",
    "section": "\n6.3 Predykcja z modelu",
    "text": "6.3 Predykcja z modelu\n\n\n\nPredykcja z modelu jest kolejnym elementem, w którym unifikacja daje o sobie znać:\n\nwyniki zawsze są w formacie tibble;\nnazwy kolumn są zawsze przewidywalne;\nw tibble wynikowej wierszy jest zawsze tyle samo co w zbiorze, na którym predykcja była przeprowadzona;\nkolejność wierszy jest ta sama co w oryginalnym zbiorze.\n\n\nKodames_test_small <- ames_test %>% slice(1:5)\npredict(lm_form_fit, new_data = ames_test_small)\n\n# A tibble: 5 × 1\n  .pred\n  <dbl>\n1  5.23\n2  5.29\n3  5.25\n4  5.25\n5  5.23\n\n\nTo sprawia, że łatwiej można korzystać z wyników predykcji, ponieważ zawsze jesteśmy pewni jaki układ ramki danych predykcji się pojawi.\n\nKodames_test_small %>% \n  select(Sale_Price) %>% \n  bind_cols(predict(lm_form_fit, ames_test_small)) %>% \n  # Add 95% prediction intervals to the results:\n  bind_cols(predict(lm_form_fit, ames_test_small, type = \"pred_int\")) \n\n# A tibble: 5 × 4\n  Sale_Price .pred .pred_lower .pred_upper\n       <dbl> <dbl>       <dbl>       <dbl>\n1       5.24  5.23        4.91        5.54\n2       5.33  5.29        4.97        5.60\n3       5.06  5.25        4.93        5.56\n4       5.26  5.25        4.93        5.56\n5       5.08  5.23        4.92        5.55\n\n\n\n\nNazwy zmiennych jakie mogą się pojawić w wyniku predykcji\n\n\n\nKodtree_model <- \n  decision_tree(min_n = 2) %>% \n  set_engine(\"rpart\") %>% \n  set_mode(\"regression\")\n\ntree_fit <- \n  tree_model %>% \n  fit(Sale_Price ~ Longitude + Latitude, data = ames_train)\n\names_test_small %>% \n  select(Sale_Price) %>% \n  bind_cols(predict(tree_fit, ames_test_small))\n\n# A tibble: 5 × 2\n  Sale_Price .pred\n       <dbl> <dbl>\n1       5.24  5.16\n2       5.33  5.31\n3       5.06  5.16\n4       5.26  5.16\n5       5.08  5.16\n\n\n\n6.3.1 Rozszerzenia\nSam pakiet parsnip zawiera interfejsy do wielu modeli. Jednakże, dla ułatwienia instalacji i konserwacji pakietu, istnieją inne pakiety tidymodels, które posiadają definicje modeli nie zawartych w parsnip. Np. pakiet discrim posiada definicje modeli klasyfikacyjnych zwanych metodami analizy dyskryminacyjnej (takich jak liniowa lub kwadratowa analiza dyskryminacyjna). Lista wszystkich modeli, które mogą być używane z parsnip znajduje się na stronie https://www.tidymodels.org/find/.\nPrzydatnym narzędziem w budowaniu modeli z wykorzystaniem pakietu tidymodels jest dodatek programu Rstudio3.3 addin instalowany razem z pakietem parsnip\n\nVideo\nPrzykład działania parsnip_addin()"
  },
  {
    "objectID": "tidymodels.html#przepływy-w-modelowaniu",
    "href": "tidymodels.html#przepływy-w-modelowaniu",
    "title": "\n6  Praca z tidymodels\n",
    "section": "\n6.4 Przepływy w modelowaniu",
    "text": "6.4 Przepływy w modelowaniu\nDo tej pory o modelowaniu myśleliśmy w uproszczony sposób, ponieważ zakładaliśmy pewną strukturę modelu, dobieraliśmy silnik i uczyliśmy model na zbiorze treningowym. W “prawdziwych” zadaniach z zakresu uczenia maszynowego, proces ten jest znacznie bardziej złożony. W fazie, którą się powszechnie nazywa przygotowaniem danych (ang. pre-processing), dokonuje się transformacji, agregacji i imputacji danych w celu wykształcenia predyktorów o większej mocy predykcyjnej. W tej fazie dochodzi również do selekcji cech (ang. feature engineering), która ma na celu odfiltrowanie nieużytecznych cech zbioru danych.\nKolejna faza budowania poprawnego modelu to jego optymalizacja (ang. tuning). Często bowiem budowane modele zawierają hiperparametry, których nie oszacujemy podczas uczenia modelu, dlatego należy je skalibrować na podstawie innych metod4.4 szerzej o tej części będziemy mówić w dalszej części tej książki\nRównież w końcowej fazie uczenia modelu tzw. post-processing-u dokonuje się jego modyfikacji, np. dobierając optymalny poziom odcięcia dla regresji logistycznej.\nTo wszystko powoduje, że procedura modelowania składa się z kilku elementów. Do ich połączenia w ekosystemie tidymodels używa się przepływów (ang. workflow). Pakiet workflow zawiera szereg funkcji pozwalających skutecznie obsługiwać potoki workflow5.5 tak nazywa się funkcja do tworzenia potoku\nPomimo złożoności procedury modelowania można się dalej zastawiać nad koniecznością stosowania przepływów, skoro można te czynności wykonywać oddzielnie. Postaramy się na przykładzie pokazać zasadności stosowania przepływów.\nWeźmy, dajmy na to, że predyktory w zbiorze danych są wysoce skorelowane. Wiem, że zjawisko współliniowości może przeszkodzić w modelowaniu zjawiska, np. za pomocą modelu liniowego, ponieważ znacznie rosną wówczas błędy standardowe estymacji. Jednym ze sposobów radzenia sobie z tym problemem jest zrzutowanie danych na nową przestrzeń mniej wymiarową za pomocą PCA. I gdyby PCA była metodą deterministyczną, czyli nie towarzyszyła jej żadna niepewność6, to tę procedurę preprocessingu użyli byśmy do zbioru uczącego w procesie uczenia modelu, a w predykcji do zbioru testowego, bez konsekwencji w postaci niedokładnego oszacowania wartości wynikowych. Jednak PCA wiąże się z niepewnością, dlatego procedura ta powinna być włączona do przepływu, czyli być immanentną częścią procesu modelowania.6 jak np. logarytmowanie zmiennej\nChoć workflow pozwalają na łączenie preprocessingu, tuningu i postprocessingu, to w następnym przykładzie pokażemy zastosowanie workflow do prostego ucznia modelu bez tych elementów.\n\nKod# określenie modelu\nlm_model <- \n  linear_reg() |> \n  set_engine(\"lm\")\n\n# zebranie elementów w workflow\nlm_wflow <- \n  workflow() |> \n  add_model(lm_model) |> \n  add_formula(Sale_Price ~ Longitude + Latitude)\n\n# workflow\nlm_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nSale_Price ~ Longitude + Latitude\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nKod# uczenie modelu\nlm_fit <- fit(lm_wflow, ames_train)\nlm_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nSale_Price ~ Longitude + Latitude\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)    Longitude     Latitude  \n   -321.755       -2.091        3.120  \n\nKod# predykcja\npredict(lm_fit, ames_test %>% slice(1:3))\n\n# A tibble: 3 × 1\n  .pred\n  <dbl>\n1  5.23\n2  5.29\n3  5.25\n\n\nPomimo tego, że model został zebrany w jedną całość (przepływ), to cały czas możemy modyfikować jego elementy. Przykładowo ujmijmy jeden z predyktorów.\n\nKodlm_fit %>% update_formula(Sale_Price ~ Longitude)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nSale_Price ~ Longitude\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nJeszcze inny przykład modyfikacji przepływu pokazuje przeformatowanie zależności opisywanej modelem.\n\nKodlm_wflow <- \n  lm_wflow %>% \n  remove_formula() %>% \n  add_variables(outcome = Sale_Price, predictors = c(Longitude, Latitude))\n\nlm_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Variables\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nOutcomes: Sale_Price\nPredictors: c(Longitude, Latitude)\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nKodfit(lm_wflow, ames_train)\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Variables\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nOutcomes: Sale_Price\nPredictors: c(Longitude, Latitude)\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)    Longitude     Latitude  \n   -321.755       -2.091        3.120  \n\n\nGenialną właściwością przepływów jest to, że gdy uczymy model wymagający zamiany zmiennych typu faktor na indykatory (ang. dummy variables), to przepływ to zrobi za nas. Przykładowo gdy uczymy model boost_tree z silnikiem xgboost to przepływ zamieni faktory na indykatory, a gdy uczymy z silnikiem C5.0 to już nie, ponieważ ten pakiet tego nie wymaga. Są jednak sytuacje, w których niewielka interwencja w workflow jest potrzebna. Np. jeśli uczymy model z efektami losowymi.\n\nKodlibrary(lme4)\nlibrary(nlme)\nlmer(distance ~ Sex + (age | Subject), data = Orthodont)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: distance ~ Sex + (age | Subject)\n   Data: Orthodont\nREML criterion at convergence: 471.1635\nRandom effects:\n Groups   Name        Std.Dev. Corr \n Subject  (Intercept) 7.3912        \n          age         0.6943   -0.97\n Residual             1.3100        \nNumber of obs: 108, groups:  Subject, 27\nFixed Effects:\n(Intercept)    SexFemale  \n     24.517       -2.145  \n\nKod# tej formuły nie możemy bezpośrednio przekazać do workflow\n# za pomocą add_formula\nlibrary(multilevelmod)\n\nmultilevel_spec <- linear_reg() %>% set_engine(\"lmer\")\n\nmultilevel_workflow <- \n  workflow() %>% \n  # Pass the data along as-is: \n  add_variables(outcome = distance, predictors = c(Sex, age, Subject)) %>% \n  add_model(multilevel_spec, \n            # This formula is given to the model\n            formula = distance ~ Sex + (age | Subject))\n\nmultilevel_fit <- fit(multilevel_workflow, data = Orthodont)\nmultilevel_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Variables\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nOutcomes: distance\nPredictors: c(Sex, age, Subject)\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear mixed model fit by REML ['lmerMod']\nFormula: distance ~ Sex + (age | Subject)\n   Data: data\nREML criterion at convergence: 471.1635\nRandom effects:\n Groups   Name        Std.Dev. Corr \n Subject  (Intercept) 7.3912        \n          age         0.6943   -0.97\n Residual             1.3100        \nNumber of obs: 108, groups:  Subject, 27\nFixed Effects:\n(Intercept)    SexFemale  \n     24.517       -2.145  \n\n\nKolejną zaletą pakietu workflowset jest możliwość jednoczesnego uczenia wielu wariantów modeli.\n\nKod# określamy potencjalne formuły modeli\nlocation <- list(\n  longitude = Sale_Price ~ Longitude,\n  latitude = Sale_Price ~ Latitude,\n  coords = Sale_Price ~ Longitude + Latitude,\n  neighborhood = Sale_Price ~ Neighborhood\n)\n\n\nlibrary(workflowsets)\n\n# zestaw przepływów do uczenia\nlocation_models <- workflow_set(preproc = location, models = list(lm = lm_model))\nlocation_models\n\n# A workflow set/tibble: 4 × 4\n  wflow_id        info             option    result    \n  <chr>           <list>           <list>    <list>    \n1 longitude_lm    <tibble [1 × 4]> <opts[0]> <list [0]>\n2 latitude_lm     <tibble [1 × 4]> <opts[0]> <list [0]>\n3 coords_lm       <tibble [1 × 4]> <opts[0]> <list [0]>\n4 neighborhood_lm <tibble [1 × 4]> <opts[0]> <list [0]>\n\nKod# pierwszy przepływ\nlocation_models$info[[1]]\n\n# A tibble: 1 × 4\n  workflow   preproc model      comment\n  <list>     <chr>   <chr>      <chr>  \n1 <workflow> formula linear_reg \"\"     \n\nKod# wyciągamy informacje o przepływie trzecim\nextract_workflow(location_models, id = \"coords_lm\")\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nSale_Price ~ Longitude + Latitude\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nKod# uczymy modele\nlocation_models <-\n   location_models %>%\n   mutate(fit = map(info, ~ fit(.x$workflow[[1]], ames_train)))\n\n# wyniki uczenia wszystkich modeli\nlocation_models\n\n# A workflow set/tibble: 4 × 5\n  wflow_id        info             option    result     fit       \n  <chr>           <list>           <list>    <list>     <list>    \n1 longitude_lm    <tibble [1 × 4]> <opts[0]> <list [0]> <workflow>\n2 latitude_lm     <tibble [1 × 4]> <opts[0]> <list [0]> <workflow>\n3 coords_lm       <tibble [1 × 4]> <opts[0]> <list [0]> <workflow>\n4 neighborhood_lm <tibble [1 × 4]> <opts[0]> <list [0]> <workflow>\n\nKod# wynik uczenia modelu 1\nlocation_models$fit[[1]]\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nSale_Price ~ Longitude\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)    Longitude  \n   -181.180       -1.991  \n\n\nJeszcze jedną wygodną funkcja do oceny ostatecznego modelu jest funkcja last_fit, której używamy do ostatecznego modelu. Wywołanie jej powoduje uczenie modelu na zbiorze uczącym i predykcje na zbiorze testowym.\n\nKod# ostatnie dopasowanie\nfinal_lm_res <- last_fit(lm_wflow, ames_split)\n\n# wynik dopasowania\nfinal_lm_res\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits             id               .metrics .notes   .predictions .workflow \n  <list>             <chr>            <list>   <list>   <list>       <list>    \n1 <split [2342/588]> train/test split <tibble> <tibble> <tibble>     <workflow>\n\nKod# podsumowanie modelu\ncollect_metrics(final_lm_res)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.163 Preprocessor1_Model1\n2 rsq     standard       0.131 Preprocessor1_Model1\n\nKod# predykjca na pierwszych pięciu obserwacjach\ncollect_predictions(final_lm_res) %>% slice(1:5)\n\n# A tibble: 5 × 5\n  id               .pred  .row Sale_Price .config             \n  <chr>            <dbl> <int>      <dbl> <chr>               \n1 train/test split  5.23     3       5.24 Preprocessor1_Model1\n2 train/test split  5.29     7       5.33 Preprocessor1_Model1\n3 train/test split  5.25    28       5.06 Preprocessor1_Model1\n4 train/test split  5.25    29       5.26 Preprocessor1_Model1\n5 train/test split  5.23    35       5.08 Preprocessor1_Model1"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "3.4 Evaluating Forecast Accuracy |\nForecasting: Principles and\nPractice (2nd Ed). n.d.\n\n\nBellon-Maurel, Véronique, Elvira Fernandez-Ahumada, Bernard Palagos,\nJean-Michel Roger, and Alex McBratney. 2010. “Critical Review of\nChemometric Indicators Commonly Used for Assessing the Quality of the\nPrediction of Soil Attributes by NIR Spectroscopy.”\nTrAC Trends in Analytical Chemistry 29 (9): 1073–81. https://doi.org/10.1016/j.trac.2010.05.006.\n\n\nBolstad, Benjamin Milo. 2004. Low-Level Analysis of\nHigh-density Oligonucleotide Array Data:\nBackground, Normalization and\nSummarization. University of California,\nBerkeley.\n\n\nCouch, Simon P., Andrew P. Bray, Chester Ismay, Evgeni Chasnovski,\nBenjamin S. Baumer, and Mine Çetinkaya-Rundel. 2021.\n“Infer: An r\nPackage for Tidyverse-Friendly Statistical Inference” 6: 3661. https://doi.org/10.21105/joss.03661.\n\n\nFranses, Philip Hans. 2016. “A Note on the Mean Absolute\nScaled Error.” International Journal of\nForecasting 32 (1): 20–22. https://doi.org/10.1016/j.ijforecast.2015.03.008.\n\n\nGentleman, Robert, Vincent Carey, Wolfgang Huber, Sandrine Dudoit, and\nRafael Irizarry. 2005. Bioinformatics and Computational\nBiology Solutions Using R and Bioconductor.\nSpringer.\n\n\nHyndman, Rob J., and Anne B. Koehler. 2006. “Another Look at\nMeasures of Forecast Accuracy.” International Journal of\nForecasting 22 (4): 679–88. https://doi.org/10.1016/j.ijforecast.2006.03.001.\n\n\nJohnson, Dan, Phoebe Eckart, Noah Alsamadisi, Hilary Noble, Celia\nMartin, and Rachel Spicer. 2018. “Polar Auxin Transport Is\nImplicated in Vessel Differentiation and Spatial Patterning During\nSecondary Growth in Populus.” American Journal\nof Botany 105 (2): 186–96. https://doi.org/10.1002/ajb2.1035.\n\n\nJoseph, V. Roshan. 2022. “Optimal Ratio for Data\nSplitting.” Statistical Analysis and Data Mining: The ASA\nData Science Journal 15 (4): 531–38. https://doi.org/10.1002/sam.11583.\n\n\nKuhn, Max, and Kjell Johnson. 2019. Feature Engineering and\nSelection. Chapman; Hall/CRC. https://doi.org/10.1201/9781315108230.\n\n\n———. 2021. Feature Engineering and\nSelection: A Practical Approach for\nPredictive Models. Taylor & Francis\nGroup.\n\n\nKuhn, Max, and Hadley Wickham. 2020. “Tidymodels: A Collection of\nPackages for Modeling and Machine Learning Using Tidyverse\nPrinciples.” https://www.tidymodels.org.\n\n\nKvalseth, Tarald O. 1985. “Cautionary Note about\nR2.” The American Statistician 39 (4):\n279–85. https://doi.org/10.2307/2683704.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the Tidyverse.” Journal of Open Source\nSoftware 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nYeh, I.-Cheng. 2006. “Analysis of Strength of\nConcrete Using Design of Experiments and\nNeural Networks.” Journal of Materials in Civil\nEngineering 18 (4): 597–604. https://doi.org/10.1061/(ASCE)0899-1561(2006)18:4(597)."
  }
]