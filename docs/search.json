[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Metody walidacji modeli statystycznych",
    "section": "",
    "text": "Wprowadzenie\nNiniejsza książka została przygotowana z myślą o studentach kierunków Inżynierii i Analizy Danych oraz Matematyka. Ma ona posłużyć w nauce szeroko rozumianej walidacji modeli statystycznych. Pojęcie zostanie dosyć szeroko potraktowane, dlatego w książce nawiązuję do metod próbkowania (ang. resampling), stosowanych do oceny jakości modelu oraz w procesie tuningowaniu modelu, jak również do miar stosowanych do oceny dopasowania modeli. Ponieważ książka ta ma stanowić przegląd metod używanych w ocenie jak dobrze model przewiduje nieznane wartości, to zostanie również uzupełniona o analizę wrażliwości pozwalającą na określenie jakie poszczególne czynniki (predyktory) i w jaki sposób wpływają na predykcję.\nW statystyce walidacja modelu jest zadaniem polegającym na ocenie, czy wybrany model statystyczny jest odpowiedni, czy nie. Często we wnioskowaniu statystycznym wnioski z modeli, które wydają się pasować do danych, mogą być błędne, co powoduje, że badacze nie poznają rzeczywistej wartości swojego modelu. Aby temu zapobiec, stosuje się walidację modelu, aby sprawdzić, czy model statystyczny posiada tzw. zdolność generalizacji. Owa zdolność polega na możliwości poprawnego przewidywania wartości wyjściowej nie tylko dla obserwacji ze zbioru uczącego, ale również z testowego, bez znaczącej straty jakości wspomnianej predykcji. Tego tematu nie należy mylić z blisko związanym zadaniem wyboru modelu, czyli procesem rozróżniania wielu modeli kandydujących: walidacja modelu nie dotyczy tak bardzo konceptualnej konstrukcji modeli, lecz testuje jedynie spójność pomiędzy wybranym modelem a jego deklarowanymi wynikami. Nadrzędnym celem walidacji modelu jest chęć poprawienia jego właściwości.\n\n\n\n\n\nWalidacja modelu występuje w wielu odmianach, a konkretna metoda walidacji modelu stosowana przez badacza jest często uwarunkowana jego projektem badawczym. Oznacza to, że nie ma jednej uniwersalnej metody walidacji modelu. Na przykład, jeśli badacz pracuje z bardzo ograniczonym zestawem danych, ale ma silne założenia wstępne dotyczące danych, może rozważyć walidację dopasowania swojego modelu poprzez zastosowanie metod bayesowskich i testowanie dopasowania modelu przy użyciu różnych rozkładów wstępnych. Jeśli jednak badacz ma dużo danych i testuje wiele zagnieżdżonych modeli, warunki te mogą sprzyjać walidacji krzyżowej i ewentualnie testowi “leave one out”. Są to dwa abstrakcyjne przykłady i każda rzeczywista walidacja modelu będzie musiała rozważyć znacznie więcej zawiłości niż tu opisano, ale przykład ten ilustruje, że metody walidacji modelu zawsze będą miały charakter poszlakowy. Ogólnie rzecz biorąc, modele mogą być walidowane przy użyciu istniejących danych lub przy użyciu nowych danych, a obie metody są omówione w kolejnych rozdziałach.\nNiezbędnym narzędziem w ocenie jakości dopasowania modeli będą metryki, czy inne narzędzia stosowane w ocenie poprawności modelu jak wykresy diagnostyczne, czy macierze klasyfikacji (ang. confusion matrix). W zależności od realizowanego zadania inne metryki mogą być wykorzystywane. I tak w zadaniach klasyfikacyjnych, w zależności od liczby klas zmiennej wyjściowej, będziemy stosowali inne miary do oceny modelu. Również w modelach regresyjnych istnieje cała gama miar pomagających ocenić dopasowanie. Jeszcze inne miary służą do ewaluacji metod nienadzorowanego uczenia maszynowego. Istnieje bowiem wiele technik określenia stopnia jednorodności powstałych skupień, a jednocześnie odpowiedzi napytanie czy wybrana została właściwa liczba grup."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Ekosystem",
    "section": "",
    "text": "Książka zawiera szereg przykładów ilustrujących zasadę działania poszczególnych technik walidacji modeli. Ponieważ od wielu lat używam języka R do budowania modeli uczenia maszynowego, to również w tej książce znajdą się implementacje metod ewaluacji modeli wykonane w R. Spośród trzech dominujących ekosystemów w języku R do budowania modeli uczenia maszynowego, czyli caret1,mlr32 i tidymodels3 postanowiłem postawić na ten ostatn i. Stanowi on przeniesienie filozofii “tidy data” realizowanej w pakiecie tidyverse4 na modele ML. tidymodels jest meta-pakietem zawierającym w sob ie:1 biblioteka napisana przez Maxa Kuhna, która niestety nie dostaje już wsparcia - https://topepo.github.io/caret/2 jedna biblioteka z całego ekosystemu przygotowanego głównie przez niemieckich programistów Lars Kotthoff, Raphael Sonabend, Michel Lang, Bernd Bischl. Jej największą zaletą jest fakt, iż wszelkie operacje na danych są dokonywane w formacie data.table uznawanym za najszybszy - https://mlr3book.mlr-org.com3 https://www.tidymodels.org4 pakiet został stworzony przez Julie Silge i Maxa Kuhna, a ich motto podane podczas prezentacji biblioteki brzmiało Whenever possible, the software should be able to protect users from committing mistakes. Software should make it easy for users to do the right thing\n\n\nbroom - przekształcanie obiektów statystycznych w tibble;\ndials - narzędzia do tuningowania parametrów modelu;\ndplyr - narzędzia do manipulacji danymi;\nggplot2 - narzędzia do wizualizacji;\ninfer - narzędzia do wnioskowania statystycznego;\nmodeldata - pakiet przykładowych danych do budowania modeli;\nparsnip - narzędzia do tworzenia i manipulowania funkcjami powszechnie używanymi podczas modelowania;\npurrr - zestaw narzędzi do programowania funkcyjnego;\nrecipes - narzędzie do tworzenia modeli;\nrsample - narzędzie do resamplingu;\ntibble - narzędzie do operacji na ramkach danych;\ntidyr - narzędzie do oczyszczania o transformacji danych;\ntune - narzędzie do tuningu hiperparametrów modelu;\nworkflows - narzędzia do zarządzania procesem uczenia modeli;\nworkflowsets - narzędzie do tworzenia zestawów workflow;\nyardstick - narzędzia do oceny dopasowania modelu.\n\nOprócz wspomnianych pakietów w ramach tidymodels występują także: applicable, baguette, butcher, discrim, embed, hardhat, corrr, rules, text recipes, tidypredict, modeldb i tidyposterior.\n\n\n\nWybór padł na tidymodels ponieważ filozofia tworzenia i walidacji modeli przypominająca pieczenie ciasta bardzo mi przypadła do gustu."
  },
  {
    "objectID": "modeling.html#typy-modeli",
    "href": "modeling.html#typy-modeli",
    "title": "\n2  Modelowanie statystyczne\n",
    "section": "\n2.1 Typy modeli",
    "text": "2.1 Typy modeli\nZanim przejdziemy dalej, opiszmy taksonomię rodzajów modeli, pogrupowanych według przeznaczenia. Taksonomia ta informuje zarówno o tym, jak model jest używany, jak i o wielu aspektach tego, jak model może być tworzony lub oceniany. Chociaż lista ta nie jest wyczerpująca, większość modeli należy do przynajmniej jednej z tych kategorii:\n\n2.1.1 Modele opisowe\nCelem modelu opisowego jest opis lub zilustrowanie pewnych cech danych. Analiza może nie mieć innego celu niż wizualne podkreślenie jakiegoś trendu lub artefaktu (lub defektu) w danych.\nNa przykład, pomiary RNA na dużą skalę są możliwe od pewnego czasu przy użyciu mikromacierzy. Wczesne metody laboratoryjne umieszczały próbkę biologiczną na małym mikrochipie. Bardzo małe miejsca na chipie mogą mierzyć sygnał oparty na bogactwie specyficznej sekwencji RNA. Chip zawierałby tysiące (lub więcej) wyników, z których każdy byłby kwantyfikacją RNA związanego z procesem biologicznym. Jednakże na chipie mogłyby wystąpić problemy z jakością, które mogłyby prowadzić do słabych wyników. Na przykład, odcisk palca przypadkowo pozostawiony na części chipa mógłby spowodować niedokładne pomiary podczas skanowania.\n\n\nRysunek 2.1: Ocena jakości odczytów z mikroczipa\n\n\nWczesną metodą oceny takich zagadnień były modele na poziomie sondy, czyli PLM (Bolstad 2004). Tworzono model statystyczny, który uwzględniał pewne różnice w danych, takie jak chip, sekwencja RNA, typ sekwencji i tak dalej. Jeśli w danych występowałyby inne, nieznane czynniki, to efekty te zostałyby uchwycone w resztach modelu. Gdy reszty zostały wykreślone według ich lokalizacji na chipie, dobrej jakości chip nie wykazywałby żadnych wzorców. W przypadku wystąpienia problemu, pewien rodzaj wzorca przestrzennego byłby dostrzegalny. Często typ wzorca sugerowałby problem (np. odcisk palca) oraz możliwe rozwiązanie (wytarcie chipa i ponowne skanowanie, powtórzenie próbki, itp.) Rysunek 2.1 pokazuje zastosowanie tej metody dla dwóch mikromacierzy (Gentleman i in. 2005). Obrazy pokazują dwie różne wartości kolorystyczne; obszary, które są ciemniejsze to miejsca, gdzie intensywność sygnału była większa niż oczekuje model, podczas gdy jaśniejszy kolor pokazuje wartości niższe niż oczekiwane. Lewy panel pokazuje w miarę losowy wzór, podczas gdy prawy panel wykazuje niepożądany artefakt w środku chipa.\n\n2.1.2 Modele do wnioskowania\nCelem modelu inferencyjnego jest podjęcie decyzji dotyczącej pytania badawczego lub sprawdzenie określonej hipotezy, podobnie jak w przypadku testów statystycznych. Model inferencyjny zaczyna się od wcześniej zdefiniowanego przypuszczenia lub pomysłu na temat populacji i tworzy wniosek statystyczny, taki jak szacunek przedziału lub odrzucenie hipotezy.\nNa przykład, celem badania klinicznego może być potwierdzenie, że nowa terapia pozwala wydłużyć życie w porównaniu z istniejącą terapią lub brakiem leczenia. Jeśli kliniczny punkt końcowy dotyczyłby przeżycia pacjenta, hipoteza zerowa mogłaby brzmieć, że nowa terapia ma równą lub niższą medianę czasu przeżycia, a hipoteza alternatywna, że nowa terapia ma wyższą medianę czasu przeżycia. Jeśli ta próba byłaby oceniana przy użyciu tradycyjnego testowania istotności hipotezy zerowej poprzez modelowanie, testowanie istotności dałoby wartość \\(p\\) przy użyciu jakiejś wcześniej zdefiniowanej metodologii opartej na zestawie założeń. Małe wartości dla wartości \\(p\\) w wynikach modelu wskazywałyby na istnienie przesłanek, że nowa terapia pomaga pacjentom żyć dłużej. Duże wartości \\(p\\) w wynikach modelu wskazywałyby, że nie udało się wykazać takiej różnicy; ten brak przesłanek mógłby wynikać z wielu powodów, w tym z tego, że terapia nie działa.\nJakie są ważne aspekty tego typu analizy? Techniki modelowania inferencyjnego zazwyczaj dają pewien rodzaj danych wyjściowych o charakterze probabilistycznym, takich jak wartość \\(p\\), przedział ufności lub prawdopodobieństwo a posteriori. Zatem, aby obliczyć taką wielkość, należy przyjąć formalne założenia probabilistyczne dotyczące danych i procesów, które je wygenerowały. Jakość wyników modelowania statystycznego w dużym stopniu zależy od tych wcześniej określonych założeń, jak również od tego, w jakim stopniu obserwowane dane wydają się z nimi zgadzać. Najbardziej krytycznymi czynnikami są tutaj założenia teoretyczne: “Jeśli moje obserwacje były niezależne, a reszty mają rozkład X, to statystyka testowa Y może być użyta do uzyskania wartości \\(p\\). W przeciwnym razie wynikowa wartość \\(p\\) może być niewłaściwa.”\nJednym z aspektów analiz inferencyjnych jest to, że istnieje tendencja do opóźnionego sprzężenia zwrotnego w zrozumieniu, jak dobrze dane odpowiadają założeniom modelu. W naszym przykładzie badania klinicznego, jeśli znaczenie statystyczne (i kliniczne) wskazuje, że nowa terapia powinna być dostępna do stosowania przez pacjentów, mogą minąć lata zanim zostanie ona zastosowana w terenie i zostanie wygenerowana wystarczająca ilość danych do niezależnej oceny, czy pierwotna analiza statystyczna doprowadziła do podjęcia właściwej decyzji.\n\n2.1.3 Modele predykcyjne\nCzasami modelowania używamy w celu uzyskania jak najdokładniejszej prognozy dla nowych danych. W tym przypadku głównym celem jest, aby przewidywane wartości (ang. prediction) miały najwyższą możliwą zgodność z prawdziwą wartością (ang. observed).\nProstym przykładem może być przewidywanie przez sprzedającego książki, ile egzemplarzy danej książki powinno być wysłanych do jego sklepu w następnym miesiącu. Nadmierna prognoza powoduje marnowanie miejsca i pieniędzy z powodu nadmiaru książek. Jeśli przewidywanie jest mniejsze niż powinno, następuje utrata potencjału i mniejszy zysk.\nCelem tego typu modeli jest raczej estymacja niż wnioskowanie. Na przykład nabywca zwykle nie jest zainteresowany pytaniem typu “Czy w przyszłym miesiącu sprzedam więcej niż 100 egzemplarzy książki X?”, ale raczej “Ile egzemplarzy książki X klienci kupią w przyszłym miesiącu?”. Również, w zależności od kontekstu, może nie być zainteresowania tym, dlaczego przewidywana wartość wynosi X. Innymi słowy, bardziej interesuje go sama wartość niż ocena formalnej hipotezy związanej z danymi. Prognoza może również zawierać miary niepewności. W przypadku nabywcy książek podanie błędu prognozy może być pomocne przy podejmowaniu decyzji, ile książek należy kupić. Może też służyć jako metryka pozwalająca ocenić, jak dobrze zadziałała metoda predykcji.\nJakie są najważniejsze czynniki wpływające na modele predykcyjne? Istnieje wiele różnych sposobów, w jaki można stworzyć model predykcyjny, dlatego w ocenie wpływu poszczególnych czynników kluczowej jest to jak model został opracowany.\nModel mechanistyczny może być wyprowadzony przy użyciu podstawowych zasad w celu uzyskania równania modelowego, które zależy od pewnych założeń. Na przykład przy przewidywaniu ilości leku, która znajduje się w organizmie danej osoby w określonym czasie, przyjmuje się pewne formalne założenia dotyczące sposobu podawania, wchłaniania, metabolizowania i eliminacji leku. Na tej podstawie można wykorzystać układ równań różniczkowych do wyprowadzenia określonego równania modelowego. Dane są wykorzystywane do oszacowania nieznanych parametrów tego równania, tak aby można było wygenerować prognozy. Podobnie jak modele inferencyjne, mechanistyczne modele predykcyjne w dużym stopniu zależą od założeń, które definiują ich równania modelowe. Jednakże, w przeciwieństwie do modeli inferencyjnych, łatwo jest formułować oparte na danych stwierdzenia dotyczące tego, jak dobrze model działa, na podstawie tego, jak dobrze przewiduje istniejące dane. W tym przypadku pętla sprzężenia zwrotnego dla osoby zajmującej się modelowaniem jest znacznie szybsza niż w przypadku testowania hipotez.\nModele empiryczne są tworzone przy bardziej niejasnych założeniach. Modele te należą zwykle do kategorii uczenia maszynowego. Dobrym przykładem jest model K-najbliższych sąsiadów (KNN). Biorąc pod uwagę zestaw danych referencyjnych, nowa obserwacja jest przewidywana przy użyciu wartości K najbardziej podobnych danych w zestawie referencyjnym. Na przykład, jeśli kupujący książkę potrzebuje prognozy dla nowej książki, a dodatkowo posiada dane historyczne o istniejących książkach, wówczas model 5-najbliższych sąsiadów może posłużyć do estymacji liczby nowych książek do zakupu na podstawie liczby sprzedaży pięciu książek, które są najbardziej podobne do nowej książki (dla pewnej definicji “podobnej”). Model ten jest zdefiniowany jedynie przez samą funkcję predykcji (średnia z pięciu podobnych książek). Nie przyjmuje się żadnych teoretycznych lub probabilistycznych założeń dotyczących sprzedaży lub zmiennych, które są używane do określenia podobieństwa pomiędzy książkami. W rzeczywistości podstawową metodą oceny adekwatności modelu jest ocena jego precyzji przy użyciu istniejących danych. Jeśli model jest dobrym wyborem, predykcje powinny być zbliżone do wartości rzeczywistych."
  },
  {
    "objectID": "modeling.html#związki-pomiędzy-typami-modeli",
    "href": "modeling.html#związki-pomiędzy-typami-modeli",
    "title": "\n2  Modelowanie statystyczne\n",
    "section": "\n2.2 Związki pomiędzy typami modeli",
    "text": "2.2 Związki pomiędzy typami modeli\nZwykły model regresji może należeć do którejś z tych trzech klas modeli, w zależności od sposobu jego wykorzystania:\n\nmodel regresji liniowej może być użyty do opisania trendów w danych;\nmodel analizy wariancji (ANOVA) jest specjalnym rodzajem modelu liniowego, który może być użyty do wnioskowania o prawdziwości hipotezy;\nmodel regresji liniowej wykorzystywany jako model predykcyjny.\n\nIstnieje dodatkowy związek między typami modeli, ponieważ konstrukcje, których celem był opis zjawiska lub wnioskowanie o nim, nie są zwykle wykorzystywane do predykcji, to nie należy całkowicie ignorować ich zdolności predykcyjnych. W przypadku pierwszych dwóch typów modeli, badacz skupia się głównie na wyselekcjonowaniu statystycznie istotnych zmiennych w modelu oraz spełnieniu szeregu założeń pozwalających na bezpieczne wnioskowanie. Takie podejście może być niebezpieczne, gdy istotność statystyczna jest używana jako jedyna miara jakości modelu. Jest możliwe, że ten statystycznie zoptymalizowany model ma słabą dokładność wyrażoną pewną miarą dopasowania. Wiemy więc, że model może nie być używany do przewidywania, ale jak bardzo należy ufać wnioskom z modelu, który ma istotne wartości \\(p\\), ale fatalną dokładność?\n\n\n\n\n\n\n\n\n\nWażne\n\n\n\nJeśli model nie jest dobrze dopasowany do danych, wnioski uzyskane na jego podstawie mogą być wysoce wątpliwe. Innymi słowy, istotność statystyczna może nie być wystarczającym dowodem na to, że model jest właściwy.\n\n\nIstnieje również podział samych modeli uczenia maszynowego. Po pierwsze, wiele modeli można skategoryzować jako nadzorowane lub nienadzorowane. Modele nienadzorowane to takie, które uczą się wzorców, skupisk lub innych cech danych, ale nie mają zmiennej wynikowej (nauczyciela). Analiza głównych składowych (PCA), analiza skupień czy autoenkodery są przykładami modeli nienadzorowanych; są one używane do zrozumienia relacji pomiędzy zmiennymi lub zestawami zmiennych bez wyraźnego związku pomiędzy predyktorami i wynikiem. Modele nadzorowane to takie, które mają zmienną wynikową. Regresja liniowa, sieci neuronowe i wiele innych metodologii należą do tej kategorii.\nW ramach modeli nadzorowanych można wyróżnić dwie główne podkategorie:\n\nregresyjne - przewidujące zmienną wynikową będącą zmienną o charakterze ilościowym;\nklasyfikacyjne - przewidujące zmienną wynikową będącą zmienną o charakterze jakościowym.\n\nRóżne zmienne modelu mogą pełnić różne role, zwłaszcza w nadzorowanym uczeniu maszynowym. Zmienna zależna lub objaśniana (ang. outcome) to wartość przewidywana w modelach nadzorowanych. Zmienne niezależne, które są podłożem do tworzenia przewidywań wyniku, są również określane jako predyktory, cechy lub kowarianty (w zależności od kontekstu)."
  },
  {
    "objectID": "modeling.html#proces-tworzenie-modelu",
    "href": "modeling.html#proces-tworzenie-modelu",
    "title": "\n2  Modelowanie statystyczne\n",
    "section": "\n2.3 Proces tworzenie modelu",
    "text": "2.3 Proces tworzenie modelu\nPo pierwsze, należy pamiętać o chronicznie niedocenianym procesie czyszczenia danych. Bez względu na okoliczności, należy przeanalizować dane pod kątem tego, czy są one odpowiednie do celów projektu i czy są właściwe. Te kroki mogą z łatwością zająć więcej czasu niż cała reszta procesu analizy danych (w zależności od okoliczności).\nCzyszczenie danych może również pokrywać się z drugą fazą eksploracji danych, często określaną jako eksploracyjna analiza danych (ang. exploratory data analysis - EDA). EDA wydobywa na światło dzienne to, jak różne zmienne są ze sobą powiązane, ich rozkłady, typowe zakresy zmienności i inne atrybuty. Dobrym pytaniem, które należy zadać w tej fazie, jest “Jak dotarłem do tych danych?”. To pytanie może pomóc zrozumieć, w jaki sposób dane, o których mowa, były próbkowane lub filtrowane i czy te operacje były właściwe. Na przykład podczas łączenia tabel bazy danych może dojść do nieudanego złączenia, które może przypadkowo wyeliminować jedną lub więcej subpopulacji.\nWreszcie, przed rozpoczęciem procesu analizy danych, powinny istnieć jasne oczekiwania co do celu modelu i sposobu oceny jego wydajności. Należy zidentyfikować przynajmniej jedną metrykę wydajności z realistycznymi celami dotyczącymi tego, co można osiągnąć. Typowe metryki statystyczne, to dokładność klasyfikacji (ang. accuracy), odsetek poprawnie i niepoprawnie zaklasyfikowanych sukcesów (przez sukces rozumiemy wyróżnioną klasę), pierwiastek błędu średniokwadratowego i tak dalej. Należy rozważyć względne korzyści i wady tych metryk. Ważne jest również, aby metryka była zgodna z szerszymi celami analizy danych.\n\n\nTypowy przebieg budowy modelu\n\n\nProces badania danych może nie być prosty. (Wickham i in. 2019) przedstawili doskonałą ilustrację ogólnego procesu analizy danych . Import danych i czyszczenie/porządkowanie są pokazane jako początkowe kroki. Kiedy rozpoczynają się kroki analityczne dla zrozumienia relacji panujących pomiędzy predyktorami i/lub zmienną wynikową, nie możemy wstępnie określić, ile czasu mogą zająć. Cykl transformacji, modelowania i wizualizacji często wymaga wielu iteracji.\nW ramach czynności zaznaczonych na szarym polu możemy wyróżnić:\n\neksploracyjna analiza danych - to kombinacja pewnych obliczeń statystycznych i wizualizacji, w celu odpowiedzi na podstawowe pytania i postawienia kolejnych. Przykładowo jeśli na wykresie histogramu lub gęstości zmiennej wynikowej w zadaniu regresyjnym zauważymy wyraźną dwumodalność, to może ona świadczyć, że badana zbiorowość nie jest homogeniczna w kontekście analizowanej zmiennej, a co w konsekwencji może skłonić nas do oddzielnego modelowania zjawisk w każdej z podpopulacji.\ninżynieria cech (ang. feature engineering) - zespół czynności mający na celu transformację i selekcję cech w procesie budowania modelu.\ntuning modeli - zespół czynności mający na celu optymalizację hiperparametrów modeli, poprzez wybór różnych ich konfiguracji oraz porównanie efektów uczenia.\nocena dopasowania modeli - ocena jakości otrzymanych modeli na podstawie miar oraz wykresów diagnostycznych.\n\n\n\nPrzykładowy przebieg budowy modelu\n\n\nPrzykładowo w pracy Kuhn i Johnson (2021) autorzy badając natężenie ruchu kolei publicznej w Chicago, przeprowadzili następujące rozumowanie podczas budowy modelu (oryginalna pisownia):\n\nKoddt <- tibble::tribble(\n                                                                                                                           ~Thoughts,             ~Activity,\n                                                             \"The daily ridership values between stations are extremely correlated.\",                 \"EDA\",\n                                                                                \"Weekday and weekend ridership look very different.\",                 \"EDA\",\n                                                           \"One day in the summer of 2010 has an abnormally large number of riders.\",                 \"EDA\",\n                                                                             \"Which stations had the lowest daily ridership values?\",                 \"EDA\",\n                                                                    \"Dates should at least be encoded as day-of-the-week, and year.\", \"Feature Engineering\",\n                                \"Maybe PCA could be used on the correlated predictors to make it easier for the models to use them.\", \"Feature Engineering\",\n                                                     \"Hourly weather records should probably be summarized into daily measurements.\", \"Feature Engineering\",\n                                      \"Let’s start with simple linear regression, K-nearest neighbors, and a boosted decision tree.\",       \"Model Fitting\",\n                                                                                                \"How many neighbors should be used?\",        \"Model Tuning\",\n                                                                         \"Should we run a lot of boosting iterations or just a few?\",        \"Model Tuning\",\n                                                                           \"How many neighbors seemed to be optimal for these data?\",        \"Model Tuning\",\n                                                                            \"Which models have the lowest root mean squared errors?\",    \"Model Evaluation\",\n                                                                                                 \"Which days were poorly predicted?\",                 \"EDA\",\n  \"Variable importance scores indicate that the weather information is not predictive. We’ll drop them from the next set of models.\",    \"Model Evaluation\",\n                                                     \"It seems like we should focus on a lot of boosting iterations for that model.\",    \"Model Evaluation\",\n                                            \"We need to encode holiday features to improve predictions on (and around) those dates.\", \"Feature Engineering\",\n                                                                                               \"Let’s drop KNN from the model list.\",    \"Model Evaluation\"\n  )\ndt |> \n  gt::gt()\n\n\n\n\n\n\nThoughts\n      Activity\n    \n\n\nThe daily ridership values between stations are extremely correlated.\nEDA\n\n\nWeekday and weekend ridership look very different.\nEDA\n\n\nOne day in the summer of 2010 has an abnormally large number of riders.\nEDA\n\n\nWhich stations had the lowest daily ridership values?\nEDA\n\n\nDates should at least be encoded as day-of-the-week, and year.\nFeature Engineering\n\n\nMaybe PCA could be used on the correlated predictors to make it easier for the models to use them.\nFeature Engineering\n\n\nHourly weather records should probably be summarized into daily measurements.\nFeature Engineering\n\n\nLet’s start with simple linear regression, K-nearest neighbors, and a boosted decision tree.\nModel Fitting\n\n\nHow many neighbors should be used?\nModel Tuning\n\n\nShould we run a lot of boosting iterations or just a few?\nModel Tuning\n\n\nHow many neighbors seemed to be optimal for these data?\nModel Tuning\n\n\nWhich models have the lowest root mean squared errors?\nModel Evaluation\n\n\nWhich days were poorly predicted?\nEDA\n\n\nVariable importance scores indicate that the weather information is not predictive. We’ll drop them from the next set of models.\nModel Evaluation\n\n\nIt seems like we should focus on a lot of boosting iterations for that model.\nModel Evaluation\n\n\nWe need to encode holiday features to improve predictions on (and around) those dates.\nFeature Engineering\n\n\nLet’s drop KNN from the model list.\nModel Evaluation\n\n\n\n\n\n\n\n\n\n\nBolstad, Benjamin Milo. 2004. Low-Level Analysis of High-Density Oligonucleotide Array Data: Background, Normalization and Summarization. University of California, Berkeley.\n\n\nGentleman, Robert, Vincent Carey, Wolfgang Huber, Sandrine Dudoit, i Rafael Irizarry. 2005. Bioinformatics and Computational Biology Solutions Using R and Bioconductor. Springer.\n\n\nKuhn, Max, i Kjell Johnson. 2021. Feature Engineering and Selection: A Practical Approach for Predictive Models. Taylor & Francis Group.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, i in. 2019. „Welcome to the Tidyverse”. Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686."
  },
  {
    "objectID": "infer.html",
    "href": "infer.html",
    "title": "\n3  Modele inferencyjne\n",
    "section": "",
    "text": "Jak to zostało wspomniane w poprzednim rozdziale modele inferencyjne służą do wyciągania wniosków na podstawie modelu. W większości przypadków dotyczy to przedziałów ufności pewnych charakterystyk, czy weryfikacji hipotez. Pod pojęciem modeli inferencyjnych będziemy rozumieli wszelkie modele stosowane w procedurze wnioskowania.\nW tym rozdziale zostanie przedstawiona procedura weryfikacyjna mająca na celu ocenę jakości przedstawionych rozwiązań w klasycznym podejściu do przedziałów ufności i weryfikacji hipotez. Największa trudnością w szacowaniu parametrów rozkładu za pomocą przedziałów ufności oraz w weryfikacji hipotez jest konieczność spełnienia założeń stosowalności tych metod. Bardzo często badacz nie posiada wystarczającej wiedzy o konsekwencji naruszenia tych założeń, a czasem nawet o ich istnieniu. Nawet wówczas, gdy badacz jest świadom konieczności spełnienia założeń w estymacji przedziałowej i weryfikacji hipotez, wymagania te mogą się okazać trudne do wypełnienia. W wielu przypadkach podczas weryfikacji hipotez za pomocą testu t-Studenta, weryfikując hipotezę o normalności rozkładu badanej cechy pojawiają się pewne wątpliwości. Po pierwsze, czy wybrany test mogę stosować do weryfikacji hipotezy o normalności w przypadku tak mało licznej próby lub tak licznej próby. Wiemy bowiem, że często stosowany test Shapiro-Wilka do weryfikacji hipotezy o zgodności populacji z rozkładem normalnym, może zbyt często odrzucać hipotezę o zgodności z rozkładem jeśli test jest wykonywany na dużej próbie1. Z drugiej strony dla prób o małej liczności test w większości nie odrzuca hipotezy o normalności, a to dlatego, że nie sposób jej odrzucić np. na podstawie 5 obserwacji. Podniesiony problem normalności rozkładu badanej cechy nie jest jedynym z jakim badacz może się spotkać chcąc spełnić wszystkie założenia modelu2 . Założenia o równości wariancji badanej cechy pomiędzy grupami, czy brak nadmiarowości3, to kolejne przykłady problemów z jakimi może spotkać się badac z.1 moc tego testu rośnie bardzo szybko wraz ze wzrostem liczebności próby2 mam na myśli zarówno modele przedziałów ufności, jaki i modele statystyczne do testowania hipotez3 w przypadku badania efektu za pomocą modelu liniowego\nKonieczność spełnienia wymienionych w stosowanej metodzie wnioskowania założeń jest konieczna, ponieważ w przeciwnym przypadku nie możemy być pewni czy wyniki zastosowanej metody są trafne4. Konsekwencją niespełnienia warunków początkowych metody, nie możemy być pewni czy rozkład statystyki testowej jest taki jak głosi metoda. I choć istnieją prace, które wyraźnie wskazują na odporność pewnych metod statystycznych na niespełnienie założeń, to nie zwalniają nas z weryfikacji tychże założeń, ponieważ w przypadku niektórych z nich nie znamy konsekwencji ich naruszenia.4 czy wniosek wyciągnięty na podstawie modelu jest właściwy\nW przypadku wspomnianych wyżej wątpliwości co do stosowalności poszczególnych metod weryfikacyjnych należy poszukać rozwiązań, które uprawdopodobnią wyniki uzyskane metodami klasycznymi. Powszechnie polecane w takiej sytuacji są rozwiązania opierające się na próbkowaniu (ang. resampling), wśród których najbardziej znane, to bootstrap i metody permutacyjne.\n\n\n\n\nNiezależnie od stawianej hipotezy, badacz zadaje sobie ten sam rodzaj pytania podczas wnioskowania statystycznego: czy efekt/różnica w obserwowanych danych jest rzeczywista, czy wynika z przypadku? Aby odpowiedzieć na to pytanie, analityk zakłada, że efekt w obserwowanych danych był spowodowany przypadkiem i nazywa to założenie hipotezą zerową5. Analityk następnie oblicza statystykę testową z danych, która opisuje obserwowany efekt. Może użyć tej statystyki testowej do obliczenia wartości \\(p\\) poprzez zestawienie jej z rozkładem wynikającym z hipotezy zerowej. Jeśli to prawdopodobieństwo jest poniżej jakiegoś wcześniej zdefiniowanego poziomu istotności \\(\\alpha\\), to analityk powinien odrzucić hipotezę zerową.5 W rzeczywistości, może nie wierzyć, że hipoteza zerowa jest prawdziwa - hipoteza zerowa jest w opozycji do hipotezy alternatywnej, która zakłada, że efekt obecny w obserwowanych danych jest rzeczywiście spowodowany faktem, że “coś się dzieje”\nPoniżej przedstawione zostaną przykłady zastosowania obu metod we wnioskowaniu. Można te zadania realizować na różne sposoby, my natomiast wykorzystamy bibliotekę infer (Couch i in. 2021) ekosystemu tidymodels (Kuhn i Wickham 2020).\n\nPrzykład 3.1 W tym przykładzie przetestujemy hipotezę o równości średniej z wartością teoretyczną. Dane weźmiemy ze zbioru gss biblioteki infer zawierającego podzbiór wyników spisu powszechnego przeprowadzonego w 1972 r. w USA.\n\nKodlibrary(tidymodels)\nlibrary(nord) # palettes\nglimpse(gss)\n\nRows: 500\nColumns: 11\n$ year    <dbl> 2014, 1994, 1998, 1996, 1994, 1996, 1990, 2016, 2000, 1998, 20…\n$ age     <dbl> 36, 34, 24, 42, 31, 32, 48, 36, 30, 33, 21, 30, 38, 49, 25, 56…\n$ sex     <fct> male, female, male, male, male, female, female, female, female…\n$ college <fct> degree, no degree, degree, no degree, degree, no degree, no de…\n$ partyid <fct> ind, rep, ind, ind, rep, rep, dem, ind, rep, dem, dem, ind, de…\n$ hompop  <dbl> 3, 4, 1, 4, 2, 4, 2, 1, 5, 2, 4, 3, 4, 4, 2, 2, 3, 2, 1, 2, 5,…\n$ hours   <dbl> 50, 31, 40, 40, 40, 53, 32, 20, 40, 40, 23, 52, 38, 72, 48, 40…\n$ income  <ord> $25000 or more, $20000 - 24999, $25000 or more, $25000 or more…\n$ class   <fct> middle class, working class, working class, working class, mid…\n$ finrela <fct> below average, below average, below average, above average, ab…\n$ weight  <dbl> 0.8960034, 1.0825000, 0.5501000, 1.0864000, 1.0825000, 1.08640…\n\n\nPrzetestujmy hipotezę, że średnia wieku wynosi 40 lat. Zacznijmy od sprawdzenia jak wygląda rozkład badanej cechy.\n\nKodgss |> \n  ggplot(aes(age))+\n  geom_histogram(color = \"white\", bins = 15)\n\n\n\nRysunek 3.1: Histogram wieku\n\n\n\n\nMożna mieć pewne wątpliwości co do normalności rozkładu, ponieważ zarysowuje się delikatna asymetria prawostronna. Nie będziemy jednak weryfikować hipotezy o normalności, tylko przeprowadzimy klasyczny test, nie mając pewności czy może on być stosowany w tej sytuacji.\n\nKodt.test(gss$age, mu = 40)\n\n\n    One Sample t-test\n\ndata:  gss$age\nt = 0.44656, df = 499, p-value = 0.6554\nalternative hypothesis: true mean is not equal to 40\n95 percent confidence interval:\n 39.09567 41.43633\nsample estimates:\nmean of x \n   40.266 \n\n\nWynik testu nie daje podstaw do odrzucenia hipotezy o tym, że przeciętny wiek w badanej populacji wynosi 40 lat. Przeprowadzimy teraz wnioskowanie w oparciu o techniki bootstrap i permutacyjną.\n\nKodnull_mean <- gss |> \n  specify(response = age) |> # określenie zmiennej\n  hypothesise(null = \"point\", mu = 40) |> # ustalienie hipotezy\n  generate(1000, type = \"bootstrap\") |> # generujemy dane\n  calculate(stat = \"mean\")\nnull_mean\n\nResponse: age (numeric)\nNull Hypothesis: point\n# A tibble: 1,000 × 2\n   replicate  stat\n       <int> <dbl>\n 1         1  39.8\n 2         2  39.4\n 3         3  40.2\n 4         4  40.1\n 5         5  40.9\n 6         6  40.2\n 7         7  40.6\n 8         8  39.5\n 9         9  39.0\n10        10  39.3\n# … with 990 more rows\n\nKodsample_mean <- gss |> \n  specify(response = age) |> \n  calculate(stat = \"mean\")\nsample_mean\n\nResponse: age (numeric)\n# A tibble: 1 × 1\n   stat\n  <dbl>\n1  40.3\n\n\nTeraz możemy przyjrzeć się rozkładowi średnich w próbach bootstrapowych.\n\nKodci <- null_mean |> \n  get_confidence_interval(point_estimate = sample_mean,\n                          level = .95,\n                          type = \"se\")\n\nnull_mean |> \n  visualise() + \n  shade_ci(endpoints = ci)\n\n\n\nRysunek 3.2: Histogram średnich bootstrapowych wraz z 95% przedziałem ufności dla średniej\n\n\n\n\nKoncentracja wokół wartości 40 może przemawiać za przyjęciem hipotezy \\(H_0\\). Ponadto wygląda na to, że otrzymany przedział ufności zawiera teoretyczną średnią wieku 40, co jest kolejny argumentem za przyjęciem hipotezy zerowej. Na koniec wyliczymy \\(p\\) dla testu bootstrapowego.\n\nKodnull_mean |> \n  get_p_value(obs_stat = sample_mean, direction = \"two.sided\")\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1   0.654\n\n\nWartość otrzymana z testu bootstrapowego różni się tylko nieznacznie od otrzymanej testem klasycznym.\n\nKodnull_mean |> \n  visualise()+\n  shade_p_value(obs_stat = sample_mean, direction = \"two-sided\")\n\n\n\nRysunek 3.3: Histogram średnich bootstrapowych z zacienonym obszarem ilustrującym na ile ekstremalna jest średnia naszej próby w stosunku do średniej wynikającej z hipotezy zerowej\n\n\n\n\n\n\n\n\n\n\n\nWskazówka\n\n\n\nW przypadku weryfikacji hipotez dotyczących jednej zmiennej wyniki testu bootstrapowego i permutacyjnego są identyczne, ponieważ te dwa rodzaje próbkowania są w tym przypadku identyczne.\n\n\n\nPrzykład 3.2 Tym razem przetestujemy nico bardziej ambitną hipotezę. Będzie to hipoteza o równości median. Dane zostaną wygenerowane z rozkładów asymetrycznych, a w tych przypadkach porównywanie median ma więcej sensu niż średnich.\n\nKodset.seed(44)\nx1 <- rchisq(20, 2)\nx2 <- -rchisq(15, 2)+10\n\ndt <- tibble(x = c(x1,x2)) |> \n  mutate(gr = rep(c(\"A\", \"B\"), times = c(20,15)))\n\ndt |> \n  ggplot(aes(x, fill = gr))+\n  geom_density(alpha = 0.6)+\n  xlim(c(-2, 12))+\n  scale_fill_nord(palette = \"victory_bonds\")+\n  theme_minimal()\n\n\n\nRysunek 3.4: Porównanie rozkładów obu prób\n\n\n\n\nJak widać na Rysunek 3.4 rozkłady różnią się zarówno położeniem, jak i kształtem. Ponieważ różnią się kształtem to porównanie obu rozkładów testem Manna-Whitneya nie odpowie nam na pytanie o równość median6, a jedynie o tym, że z prawdopodobieństwem 50% losowa wartość z jednego rozkładu będzie większa niż losowa wartość z drugiego rozkładu. Zatem wyniki testu klasycznego nie będą wystarczające do oceny postawionej hipotezy. Przeprowadzimy zatem test metodami próbkowania. Najpierw techniką bootstrapową.6 tylko w przypadku jednakowych kształtów rozkładów test ten weryfikuje równość median\n\nKodset.seed(44)\nsample_diff <- dt |> \n  specify(x~gr) |> \n  calculate(stat = \"diff in medians\", order = c(\"A\",\"B\"))\nsample_diff\n\nResponse: x (numeric)\nExplanatory: gr (factor)\n# A tibble: 1 × 1\n   stat\n  <dbl>\n1 -6.83\n\nKodnull_diff <- dt |> \n  specify(x~gr) |> \n  hypothesise(null = \"independence\") |> \n  generate(reps = 1000, type = \"bootstrap\") |> \n  calculate(stat = \"diff in medians\", order = c(\"A\",\"B\"))\n\n\nOceńmy rozkład różnic z prób bootstrapowych. Jeśli hipoteza zerowa jest prawdziwa to rozkład różnicy median powinien oscylować wokół zera.\n\nKodci <- null_diff |> \n  get_confidence_interval(level = .95, type = \"percentile\")\n\nnull_diff |> \n  visualise()+\n  shade_ci(endpoints = ci)+\n  geom_vline(xintercept = 0, \n             linewidth = 2,\n             color = \"red\")\n\n\n\nRysunek 3.5: Rozkład różnic pomiędzy medianami z 95% przedziałem ufności\n\n\n\n\nWidać wyraźnie, że rozkład różnic pomiędzy medianami nie oscyluje wokół zera, co może świadczyć o konieczności odrzucenia hipotezy zerowej.\n\nKodnull_diff |> \n  get_p_value(obs_stat = 0, direction = \"two-sided\")\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1       0\n\nKodwilcox.test(x1,x2)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  x1 and x2\nW = 32, p-value = 2.453e-05\nalternative hypothesis: true location shift is not equal to 0\n\n\nWartość \\(p\\) mniejsza od \\(\\alpha\\) każe odrzucić hipotezę zerową na korzyść alternatywnej, czyli mediany nie są równe.\nTeraz przeprowadzimy test permutacyjny.\n\nKodset.seed(44)\nnull_diff <- dt |> \n  specify(x~gr) |> \n  hypothesise(null = \"independence\") |> \n  generate(reps = 1000, type = \"permute\") |> \n  calculate(stat = \"diff in medians\", order = c(\"A\",\"B\"))\n\nci <- null_diff |> \n  get_ci(level = .95, \n         type = \"percentile\")\n\nnull_diff |> \n  visualise()+\n  shade_ci(endpoints = ci)+\n  geom_vline(xintercept = sample_diff$stat,\n             linewidth = 2,\n             color = \"red\")\n\n\n\nRysunek 3.6: Rozkład różnic pomiędzy medianami z 95% przedziałem ufności\n\n\n\n\nJak widać z Rysunek 3.6 rozkład różnic pomiędzy medianami dla całkowicie losowego układu obserwacji7 oscyluje wokół zera. Wartość różnicy median obliczona na podstawie próby -6.83 nie leży wewnątrz przedziału ufności dla różnicy, należy zatem odrzucić hipotezę o równości median. Potwierdza to wynik testu permutacyjnego.7 co odzwierciedla warunek z hipotezy zerowej\n\nKodnull_diff |> \n  visualise() +\n  shade_p_value(obs_stat = sample_diff, direction = \"two-sided\")\n\nnull_diff |> \n  get_p_value(obs_stat = sample_diff, direction = \"two-sided\")\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1       0\n\n\n\n\nRysunek 3.7: Rozkład różnic pomiędzy medianami z naniesionym p-value\n\n\n\n\n\n\nPrzykład 3.3 W tym przykładzie zbadamy niezależność cech partyid i class ze zbioru gss, które oznaczają odpowiednio sprzyjanie danej partii politycznej i subiektywną identyfikację klasy społeczno-ekonomicznej.\n\nKodlibrary(ggstatsplot)\nggbarstats(data = gss,\n           x = class,\n           y = partyid, \n           label = \"count\",\n           proportion.test = F, \n           bf.message = F)\n\n\n\nRysunek 3.8: Wykres słupkowy ilustrujący udziały poszczególnych grup wraz z wynikiem testu \\(\\chi^2\\) niezależności\n\n\n\n\nPowyższy test każe odrzucić hipotezę o niezależności cech, a ponieważ nie wymaga on spełnienia założeń (poza niezależnością obserwacji w próbie), to można uznać wynik ten za wiarygodny. Jedyny czynnik jaki mógłby wpłynąć na wynik testu \\(\\chi^2\\) to liczebności w tabeli kontyngencji poniżej 5. Dlatego tym bardziej warto przeprowadzić testowanie tej hipotezy za pomocą próbkowania.\n\nKodset.seed(44)\n# obliczamy statystykę testową dla próby\nsample_stat <- gss |> \n  drop_na(partyid, class) |> \n  droplevels() |> \n  specify(partyid~class) |> \n  hypothesise(null = \"independence\") |>\n  calculate(stat = \"Chisq\")\n\n# generujemy próby zgodne z hipotezą zerową za pomocą próbkowania\nnull_chisq_sim <- gss |> \n  drop_na(partyid, class) |> \n  droplevels() |>\n  specify(formula = partyid ~ class) |> \n  hypothesise(null = \"independence\") |> \n  generate(1000, type = \"permute\") |> \n  calculate(stat = \"Chisq\")\n\n# porównujemy rozkład wynikający z hipotezy zerowej z\n# z wartością statystyki obliczoną z próby\nnull_chisq_sim |> \n  visualise() + \n  shade_p_value(obs_stat = sample_stat, direction = \"right\")\n\n# obliczamy p-value\nnull_chisq_sim |> \n  get_p_value(obs_stat = sample_stat, direction = \"right\")\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1   0.002\n\n\n\n\nRysunek 3.9: Porównanie rozkładu statystyki testowej przy założeniu prawdziwości hipotezy zerowej z wartością statystyki testowej z próby\n\n\n\n\nWidzimy, że test każe odrzucić hipotezę o niezależności cech, podobnie jak test teoretyczny. Pakiet infer daje nam możliwość generowania rozkładu teoretycznego zgodnego z hipotezą zerową nieco inaczej niż na podstawie resamplingu.\n\nKodnull_chisq_theory <- gss |> \n  specify(partyid~class) |> \n  assume(distribution = \"Chisq\", df = 9)\n\nnull_chisq_theory |> \n  visualise() + \n  shade_p_value(obs_stat = sample_stat, \n                direction = \"right\")\npchisq(sample_stat$stat, df = 9, lower.tail = F)\n\n   X-squared \n0.0001314536 \n\n\n\n\nRysunek 3.10: Porównanie rozkładu teoretycznego statystyki testowej przy założeniu prawdziwości hipotezy zerowej z wartością statystyki testowej z próby\n\n\n\n\nOczywiście wynik otrzymany metodą teoretyczną pokrywa się dokładnie z wynikiem klasycznego testu \\(\\chi^2\\).\n\n\n\n\n\nCouch, Simon P., Andrew P. Bray, Chester Ismay, Evgeni Chasnovski, Benjamin S. Baumer, i Mine Çetinkaya-Rundel. 2021. „infer: An R package for tidyverse-friendly statistical inference” 6: 3661. https://doi.org/10.21105/joss.03661.\n\n\nKuhn, Max, i Hadley Wickham. 2020. „Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles.” https://www.tidymodels.org."
  },
  {
    "objectID": "measures.html#miary-dopasowania-modeli-regresyjnych",
    "href": "measures.html#miary-dopasowania-modeli-regresyjnych",
    "title": "\n4  Przegląd miar dopasowania modelu\n",
    "section": "\n4.1 Miary dopasowania modeli regresyjnych",
    "text": "4.1 Miary dopasowania modeli regresyjnych\nPrzegląd zaczniemy od najlepiej znanych miar, a skończymy na rzadziej stosowanych, jak funkcja straty Hubera.\n\n4.1.1 \\(R^2\\)\n\nMiara stosowana najczęściej do oceny dopasowania modeli liniowych, a definiowana jako:\n\\[\nR^2=1-\\frac{\\sum_i(y_i-\\hat{y}_i)^2}{\\sum_i(y_i-\\bar{y})^2},\n\\tag{4.1}\\]\ngdzie \\(\\hat{y}_i\\) jest \\(i\\)-tą wartością przewidywaną na podstawie modelu, \\(\\bar{y}\\) jest średnią zmiennej wynikowej, a \\(y_i\\) jest \\(i\\)-tą wartością obserwowaną. Już na kursie modeli liniowych dowiedzieliśmy się o wadach tak zdefiniowanej miary. Wśród nich należy wymienić przede wszystkim fakt, iż dołączając do modelu zmienne, których zdolność predykcyjna jest nieistotna3, to i tak rośnie \\(R^2\\)3 czyli nie mają znaczenia w przewidywaniu wartości wynikowej\nW przypadku modeli liniowych wprowadzaliśmy korektę eliminującą tą wadę, jednak w przypadku modeli predykcyjnych skorygowana miara \\(R^2_{adj}\\) nie wystarcza. W sytuacji gdy modele mają bardzo słabą moc predykcyjną, czyli są np. drzewem regresyjnym bez żadnej reguły podziału4, wówczas można otrzymać ujemne wartości obu miar. Zaleca się zatem wprowadzenie miary, która pozbawiona jest tej wady, a jednocześnie ma tą sama interpretację. Definiuję się ją następująco:4 drzewo składa się tylko z korzenia\n\\[\n\\tilde{R}^2=[\\operatorname{Cor}(Y, \\hat{Y})]^2.\n\\tag{4.2}\\]\nMiara zdefiniowana w (4.2) zapewnia nam wartości w przedziale (0,1), a klasyczna miara (4.1) nie(Kvalseth 1985). Tradycyjna jest zdefiniowana w bibliotece yardstick5 pod nazwą rsq_trad, natomiast miara oparta na korelacji jako rsq. Oczywiście interpretacja jest następująca, że jeśli wartość \\(\\tilde{R}^2\\) jest bliska 1, to model jest dobrze dopasowany, a bliskie 0 oznacza słabe dopasowanie.5 będącej częścią ekosystemu tidymodels\n\n4.1.2 RMSE\nInną powszechnie stosowaną miarą do oceny dopasowania modeli regresyjnych jest pierwiastek błędu średnio-kwadratowego (ang. Root Mean Square Error), zdefiniowany następująco:\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{n}},\n\\tag{4.3}\\]\ngdzie \\(n\\) oznacza liczebność zbioru danych na jakim dokonywana jest ocena dopasowania. Im mniejsza jest wartość błędu RMSE tym lepiej dopasowany jest model. Niestety wadą tej miary jest brak odporności na wartości odstające. Błąd w tym przypadku jest mierzony w tych samych jednostkach co mierzona wielkość wynikowa \\(Y\\). Do wywołania jej używamy funkcji rmse.\n\n4.1.3 MSE\nŚciśle powiązaną miarą dopasowania modelu z RMSE jest błąd średnio-kwadratowy (ang. Mean Square Error). Oczywiście jest on definiowany jako kwadrat RMSE. Interpretacja jest podobna jak w przypadku RMSE. W tym przypadku błąd jest mierzony w jednostkach do kwadratu i również jak w przypadku RMSE miara ta jest wrażliwa na wartości odstające. Wywołujemy ją funkcją mse.\n\n\n\n\n4.1.4 MAE\nChcąc uniknąć (choćby w części) wrażliwości na wartości odstające stosuje się miarę średniego absolutnego błędu (ang. Mean Absolut Error). Definiujemy go następująco:\n\\[\nMAE=\\frac{\\sum_{i=1}^n\\vert y_i-\\hat{y}_i\\vert}{n}.\n\\tag{4.4}\\]\nPonieważ wartości błędów \\(y_i-\\hat{y}_i\\) nie są podnoszone do kwadratu, to miara ta jest mniej wrażliwa na punkty odstające. Interpretacja jej jest podobna jak MSE i RMSE. Do wywołania jej używamy funkcji mae. Błąd w tym przypadku jest również mierzony w tych samych jednostkach co \\(Y\\).\nWymienione miary błędów są nieunormowane, a dopasowania modeli możemy dokonywać jedynie porównując wynik błędu z wartościami \\(Y\\), lub też przez porównanie miar dla różnych modeli.\n\n4.1.5 MAPE\nŚredni bezwzględny błąd procentowy (ang. Mean Absolute Percentage Error) jest przykładem miary błędu wyrażanego w procentach. Definiuje się go następująco:\n\\[\nMAPE=\\frac{1}{n}\\sum_{i=1}^n\\left|\\frac{y_i-\\hat{y}_i}{y_i}\\right|\\cdot 100\\%.\n\\tag{4.5}\\]\nInterpretujemy ten błąd podobnie jak poprzednie pomimo, że jest wyrażony w procentach. Do wywołania go w pakiecie yardstick używamy funkcji mape.\n\n4.1.6 MASE\nŚredni bezwzględny błąd skalowany (ang. Mean Absolute Scaled Error) jest miarą dokładności prognoz. Została zaproponowana w 2005 roku przez statystyka Roba J. Hyndmana i profesora Anne B. Koehler, którzy opisali ją jako “ogólnie stosowaną miarę dokładności prognoz bez problemów widocznych w innych miarach.”(Hyndman i Koehler 2006) Średni bezwzględny błąd skalowany ma korzystne właściwości w porównaniu z innymi metodami obliczania błędów prognoz, takimi jak RMSE, i dlatego jest zalecany do określania dokładności prognoz w szeregach czasowych.(Franses 2016) Definiujemy go następująco\n\\[\nMASE = \\frac{\\sum_{i=1}^n\\vert y_i-\\hat{y}_i\\vert}{\\sum_{i=1}^n\\vert y_i-\\bar{y}_i\\vert}.\n\\tag{4.6}\\]\nDla szeregów czasowych z sezonowością i bez sezonowości definiuje się go jeszcze nieco inaczej(Hyndman i Koehler 2006; 3.4 Evaluating Forecast Accuracy | Forecasting: Principles and Practice (2nd Ed), b.d.) Oczywiście interpretacja jest też podobna jak w przypadku innych miar błędów. Wywołujemy go funkcją mase.\n\n4.1.7 MPE\nŚredni błąd procentowy (ang. Mean Percentage Error) jest miarą błędu względnego definiowaną nastepująco\n\\[\nMPE = \\frac{1}{n}\\sum_{i=1}^n\\frac{y_i-\\hat{y}_i}{y_i}.\n\\tag{4.7}\\]\nPonieważ we wzorze wykorzystywane są rzeczywiste, a nie bezwzględne wartości błędów prognozy, dodatnie i ujemne błędy prognozy mogą się wzajemnie kompensować. W rezultacie wzór ten można wykorzystać jako miarę błędu systematycznego w prognozach. Wadą tej miary jest to, że jest ona nieokreślona zawsze, gdy pojedyncza wartość rzeczywista wynosi zero. Wywołujemy ją za pomocą mpe.\n\n4.1.8 MSD\nŚrednia znakowa różnic (ang. Mean Signed Deviation), znana również jako średnie odchylenie znakowe i średni błąd znakowy, jest statystyką próbkową, która podsumowuje, jak dobrze szacunki \\(\\hat{Y}\\) pasują do wielkości obserwowanych \\(Y\\). Definiujemy ją następująco:\n\\[\nMSD = \\frac{1}{n}\\sum_{i=1}^n(\\hat{y}_i-y_i).\n\\tag{4.8}\\]\nInterpretacja podobnie jak w przypadku innych błędów i mniej wynosi miara tym lepiej dopasowany model. Wywołujemy go funkcją msd.\nIstnieje cały szereg miar specjalistycznych rzadziej stosowanych w zagadnieniach regresyjnych. Wśród nich należy wymienić\n\n4.1.9 Funkcja straty Hubera\nFunkcja straty Hubera (ang. Huber loss) jest nieco bardziej odporną na punkty odstające niż RMSE miarą błędu. Definiujemy ją następująco:\n\\[\nL_{\\delta}(y, \\hat{y})= \\begin{cases}\n  \\frac12 (y_i-\\hat{y}_i)^2, &\\text{ jeśli }\\vert y_i-\\hat{y}_i\\vert\\leq\\delta\\\\\n  \\delta\\cdot \\vert y_i-\\hat{y}_i\\vert-\\tfrac12\\delta, &\\text{ w przeciwnym przypadku}.\n\\end{cases}\n\\tag{4.9}\\] W implementacji yardstick \\(\\delta=1\\) natomiast wyliczanie funkcji straty następuje przez uśrednienie po wszystkih obserwacjach. Z definicji widać, że funkcja straty Hubera jest kombinacją MSE i odpowiednio przekształconej miary MAE, w zależności od tego czy predykcja znacząco odbiegaja od obserwowanych wartości. Wywołujemy ją przez funkcję huber_loss.\n\n4.1.10 Funkcja straty Pseudo-Hubera\nFunkcja straty Pseudo-Hubera (ang. Pseudo-Huber loss) może być stosowana jako gładkie przybliżenie funkcji straty Hubera. Łączy ona najlepsze właściwości straty kwadratowej6 i straty bezwzględnej7, będąc silnie wypukłą, gdy znajduje się blisko celu (minimum) i mniej stromą dla wartości ekstremalnych . Skala, przy której funkcja straty Pseudo-Hubera przechodzi od straty L2 dla wartości bliskich minimum do straty L1 może być kontrolowana przez parametr \\(\\delta\\). Funkcja straty Pseudo-Hubera zapewnia, że pochodne są ciągłe dla wszystkich stopni . Definiujemy ją następująco :6 inaczej w normie L27 w normie L1\n\\[\nL_{\\delta}(y-\\hat{y})=\\delta^2\\left(\\sqrt{1+((y-\\hat{y})/\\delta)^2}-1\\right).\n\\tag{4.10}\\] Wywołujemy ją za pomocą funkcji huber_loss_pseudo.\n\n4.1.11 Logarytm funkcji straty dla rozkładu Poissona\nLogarytm funkcji straty dla rozkładu Poissona (ang. Mean log-loss for Poisson data) definiowany jest w następujący sposób:\n\\[\n\\mathcal{L}=\\frac1n\\sum_{i=11}^n(\\hat{y}_i-y_i\\cdot \\ln(\\hat{y}_i)).\n\\]\nWywołujemy go funkcją poisson_log_los.\n\n4.1.12 SMAPE\nSymetryczny średni bezwzględny błąd procentowy (ang. Symmetric Mean Absolute Percentage Error) jest miarą dokładności opartą na błędach procentowych (lub względnych). Definiujemy ją następująco:\n\\[\nSMAPE = \\frac1n\\sum_{i=1}^n\\frac{\\vert y_i-\\hat{y}_i\\vert}{(|y_i|+|\\hat{y}_i|)/2}\\cdot100\\%.\n\\tag{4.11}\\]\nWywołujemy go funkcją smape.\n\n4.1.13 RPD\nStosunek wydajności do odchylenia standardowego (ang. Ratio of Performance to Deviation) definiujemy jako\n\\[\nRPD = \\frac{SD}{RMSE},\n\\tag{4.12}\\]\ngdzie \\(SD\\) oczywiście oznacza odchylenie standardowe zmiennej zależnej. Tym razem interpretujemy go w ten sposób, że im wyższa jest wartość RPD tym lepiej dopasowany model. Wywołujemy za pomocą rpd.\nW szczególności w dziedzinie spektroskopii, stosunek wydajności do odchylenia (RPD) został użyty jako standardowy sposób raportowania jakości modelu. Jest to stosunek odchylenia standardowego zmiennej do błędu standardowego przewidywania tej zmiennej przez dany model. Jednak jego systematyczne stosowanie zostało skrytykowane przez kilku autorów, ponieważ użycie odchylenia standardowego do reprezentowania rozrzutu zmiennej może być niewłaściwe w przypadku skośnych zbiorów danych. Stosunek wydajności do rozstępu międzykwartylowego został wprowadzony przez Bellon-Maurel i in. (2010) w celu rozwiązania niektórych z tych problemów i uogólnienia RPD na zmienne o rozkładzie nienormalnym.\n\n4.1.14 RPIQ\nStosunek wartości do rozstępu międzykwartylowego (ang. Ratio of Performance to Inter-Quartile) definiujemy następująco:\n\\[\nRPIQ = \\frac{IQ}{RMSE},\n\\tag{4.13}\\]\ngdzie \\(IQ\\) oznacza rozstęp kwartylowy zmiennej zależnej. Wywołujemy go przez funkcję rpiq.\n\n4.1.15 CCC\nKorelacyjny współczynnik zgodności (ang. Concordance Correlation Coefficient) mierzy zgodność pomiędzy wartościami predykcji i obserwowanymi. Definiujemy go w następujący sposób:\n\\[\nCCC = \\frac{2\\rho\\sigma_y\\sigma_{\\hat{y}}}{\\sigma^2_{y}+\\sigma^2_{\\hat{y}}+(\\mu_y-\\mu_{\\hat{y}})^2},\n\\]\ngdzie \\(\\mu_y,\\mu_{\\hat{y}}\\) oznaczają średnią wartości obserwowanych i przewidywanych odpowiednio, \\(\\sigma_{y},\\sigma_{\\hat{y}}\\) stanowią natomiast odchylenia standardowe tych wielkości. \\(\\rho\\) jest współczynnikiem korelacji pomiędzy \\(Y\\) i \\(\\hat{Y}\\). Wywołanie w R to funkcja ccc.\n\n4.1.16 Podsumowanie miar dla modeli regresyjnych\nWśród miar dopasowania modelu można wyróżnić, te które mierzą zgodność pomiędzy wartościami obserwowanymi a przewidywanymi, wyrażone często pewnego rodzaju korelacjami (lub ich kwadratami), a interpretujemy je w ten sposób, że im wyższe wartości tych współczynników tym bardziej zgodne są predykcje z obserwacjami. Drugą duża grupę miar stanowią błędy (bezwzględne i względne), które mierzą w różny sposób różnice pomiędzy wartościami obserwowanymi i przewidywanymi. Jedne są bardziej odporne wartości odstające inne mniej, a wszystkie interpretujemy tak, że jeśli ich wartość jest mniejsza tym lepiej jest dopasowany model.\n\nPrzykład 4.1 Dla zilustrowania działania wspomnianych miar przeanalizujemy przykład modelu regresyjnego. Dla przykładu rozwiążemy zadanie przewidywania wytrzymałości betonu na podstawie jego parametrów. Do tego celu użyjemy danych ze zbioru concrete pakietu modeldata.(Yeh 2006)\n\nKodlibrary(tidymodels)\n\n# charakterystyka danych\nglimpse(concrete)\n\nRows: 1,030\nColumns: 9\n$ cement               <dbl> 540.0, 540.0, 332.5, 332.5, 198.6, 266.0, 380.0, …\n$ blast_furnace_slag   <dbl> 0.0, 0.0, 142.5, 142.5, 132.4, 114.0, 95.0, 95.0,…\n$ fly_ash              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ water                <dbl> 162, 162, 228, 228, 192, 228, 228, 228, 228, 228,…\n$ superplasticizer     <dbl> 2.5, 2.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,…\n$ coarse_aggregate     <dbl> 1040.0, 1055.0, 932.0, 932.0, 978.4, 932.0, 932.0…\n$ fine_aggregate       <dbl> 676.0, 676.0, 594.0, 594.0, 825.5, 670.0, 594.0, …\n$ age                  <int> 28, 28, 270, 365, 360, 90, 365, 28, 28, 28, 90, 2…\n$ compressive_strength <dbl> 79.99, 61.89, 40.27, 41.05, 44.30, 47.03, 43.70, …\n\nKod# modelowania dokonamy bez szczególnego uwzględnienia charakteru zmiennych,\n# tuningowania i innych czynności, które będą nam towarzyszyć w normalnej\n# budowie modelu\n\n# podział danych na uczące i testowe\nset.seed(44)\nsplit <- initial_split(data = concrete,\n                       prop = 0.7)\ntrain_data <- training(split)\ntest_data <- testing(split)\n\n# określenie modeli, wybrałem kNN\nknn5 <-\n  nearest_neighbor(neighbors = 5) |> \n  set_engine('kknn') %>%\n  set_mode('regression')\n\nknn25 <-\n  nearest_neighbor(neighbors = 25) |> \n  set_engine('kknn') %>%\n  set_mode('regression')\n\n# uczymy modele\nfit5 <- knn5 |> \n  fit(compressive_strength~., data = train_data)\n\nfit25 <- knn25 |> \n  fit(compressive_strength~., data = train_data)\n\n# obliczamy predykcję dla obu modeli na obu zbiorach\npred_train5 <- predict(fit5, train_data)\npred_train25 <- predict(fit25, train_data)\npred_test5 <- predict(fit5, test_data)\npred_test25 <- predict(fit25, test_data)\n\n\n\nKodbind_cols(obs = c(train_data$compressive_strength, test_data$compressive_strength),\n          pred5 = c(pred_train5$.pred, pred_test5$.pred),\n          pred25 = c(pred_train25$.pred, pred_test25$.pred)) |> \n  mutate(sample = rep(c(\"train\", \"test\"), c(nrow(train_data), nrow(test_data)))) |> \n  pivot_longer(cols = c(pred5, pred25),\n               names_to = \"model\",\n               values_to = \"pred\") |> \n  mutate(model = case_when(\n    model == \"pred5\" ~ \"knn5\",\n    model == \"pred25\" ~ \"knn25\"\n  )) |> \n  ggplot(aes(x = obs, y = pred))+\n  geom_point(alpha = 0.1)+\n  geom_abline(intercept = 0, \n              slope = 1)+\n  facet_grid(sample~model)+\n  coord_obs_pred()\n\n\n\nRysunek 4.1: Graficzne porównanie obu modeli na obu zbiorach\n\n\n\n\n\nKod# podsumowanie za pomocą miary R2\nbind_cols(obs = c(train_data$compressive_strength, test_data$compressive_strength),\n          pred5 = c(pred_train5$.pred, pred_test5$.pred),\n          pred25 = c(pred_train25$.pred, pred_test25$.pred)) |> \n  mutate(sample = rep(c(\"train\", \"test\"), c(nrow(train_data), nrow(test_data)))) |> \n  pivot_longer(cols = c(pred5, pred25),\n               names_to = \"model\",\n               values_to = \"pred\") |> \n  group_by(model, sample) |> \n  rsq(truth = obs, estimate = pred) |> \n  arrange(model)\n\n# A tibble: 4 × 5\n  model  sample .metric .estimator .estimate\n  <chr>  <chr>  <chr>   <chr>          <dbl>\n1 pred25 test   rsq     standard       0.645\n2 pred25 train  rsq     standard       0.787\n3 pred5  test   rsq     standard       0.737\n4 pred5  train  rsq     standard       0.929\n\nKod# można też podsumować od razu kilkoma miarami\n# będa miary domyślne dla modelu regresyjnego\nbind_cols(obs = c(train_data$compressive_strength, test_data$compressive_strength),\n          pred5 = c(pred_train5$.pred, pred_test5$.pred),\n          pred25 = c(pred_train25$.pred, pred_test25$.pred)) |> \n  mutate(sample = rep(c(\"train\", \"test\"), c(nrow(train_data), nrow(test_data)))) |> \n  pivot_longer(cols = c(pred5, pred25),\n               names_to = \"model\",\n               values_to = \"pred\") |> \n  group_by(model, sample) |> \n  metrics(truth = obs, estimate = pred) |> \n  arrange(model, .metric)\n\n# A tibble: 12 × 5\n   model  sample .metric .estimator .estimate\n   <chr>  <chr>  <chr>   <chr>          <dbl>\n 1 pred25 test   mae     standard       7.73 \n 2 pred25 train  mae     standard       6.50 \n 3 pred25 test   rmse    standard       9.74 \n 4 pred25 train  rmse    standard       8.22 \n 5 pred25 test   rsq     standard       0.645\n 6 pred25 train  rsq     standard       0.787\n 7 pred5  test   mae     standard       6.33 \n 8 pred5  train  mae     standard       3.45 \n 9 pred5  test   rmse    standard       8.26 \n10 pred5  train  rmse    standard       4.68 \n11 pred5  test   rsq     standard       0.737\n12 pred5  train  rsq     standard       0.929\n\nKod# możemy zmienić parametry niektórych miar\nhuber_loss2 <- metric_tweak(\"huber_loss2\", huber_loss, delta = 2)\n\n# można również wybrać jakie miary zostana użyte\nselected_metrics <- metric_set(ccc, rpd, mape, huber_loss2)\n\n\nbind_cols(obs = c(train_data$compressive_strength, test_data$compressive_strength),\n          pred5 = c(pred_train5$.pred, pred_test5$.pred),\n          pred25 = c(pred_train25$.pred, pred_test25$.pred)) |> \n  mutate(sample = rep(c(\"train\", \"test\"), c(nrow(train_data), nrow(test_data)))) |> \n  pivot_longer(cols = c(pred5, pred25),\n               names_to = \"model\",\n               values_to = \"pred\") |> \n  group_by(model, sample) |> \n  selected_metrics(truth = obs, estimate = pred) |> \n  arrange(model, sample)\n\n# A tibble: 16 × 5\n   model  sample .metric     .estimator .estimate\n   <chr>  <chr>  <chr>       <chr>          <dbl>\n 1 pred25 test   ccc         standard       0.750\n 2 pred25 test   rpd         standard       1.64 \n 3 pred25 test   mape        standard      30.9  \n 4 pred25 test   huber_loss2 standard      13.6  \n 5 pred25 train  ccc         standard       0.851\n 6 pred25 train  rpd         standard       2.07 \n 7 pred25 train  mape        standard      24.8  \n 8 pred25 train  huber_loss2 standard      11.1  \n 9 pred5  test   ccc         standard       0.844\n10 pred5  test   rpd         standard       1.93 \n11 pred5  test   mape        standard      24.1  \n12 pred5  test   huber_loss2 standard      10.8  \n13 pred5  train  ccc         standard       0.958\n14 pred5  train  rpd         standard       3.64 \n15 pred5  train  mape        standard      12.8  \n16 pred5  train  huber_loss2 standard       5.19 \n\n\n\nW przypadku gdybyśmy chcieli zdefiniować własną miarę, to oczywiście jest taka możliwość8 polecam stronę pakietu yardstick - https://www.tidymodels.org/learn/develop/metrics/.8 choć liczba już istniejących jest imponująca\n\n\n\n\n3.4 Evaluating Forecast Accuracy | Forecasting: Principles and Practice (2nd Ed). b.d.\n\n\nBellon-Maurel, Véronique, Elvira Fernandez-Ahumada, Bernard Palagos, Jean-Michel Roger, i Alex McBratney. 2010. „Critical Review of Chemometric Indicators Commonly Used for Assessing the Quality of the Prediction of Soil Attributes by NIR Spectroscopy”. TrAC Trends in Analytical Chemistry 29 (9): 1073–81. https://doi.org/10.1016/j.trac.2010.05.006.\n\n\nFranses, Philip Hans. 2016. „A Note on the Mean Absolute Scaled Error”. International Journal of Forecasting 32 (1): 20–22. https://doi.org/10.1016/j.ijforecast.2015.03.008.\n\n\nHyndman, Rob J., i Anne B. Koehler. 2006. „Another Look at Measures of Forecast Accuracy”. International Journal of Forecasting 22 (4): 679–88. https://doi.org/10.1016/j.ijforecast.2006.03.001.\n\n\nKvalseth, Tarald O. 1985. „Cautionary Note about R2”. The American Statistician 39 (4): 279–85. https://doi.org/10.2307/2683704.\n\n\nYeh, I.-Cheng. 2006. „Analysis of Strength of Concrete Using Design of Experiments and Neural Networks”. Journal of Materials in Civil Engineering 18 (4): 597–604. https://doi.org/10.1061/(ASCE)0899-1561(2006)18:4(597)."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "3.4 Evaluating Forecast Accuracy |\nForecasting: Principles and\nPractice (2nd Ed). n.d.\n\n\nBellon-Maurel, Véronique, Elvira Fernandez-Ahumada, Bernard Palagos,\nJean-Michel Roger, and Alex McBratney. 2010. “Critical Review of\nChemometric Indicators Commonly Used for Assessing the Quality of the\nPrediction of Soil Attributes by NIR Spectroscopy.”\nTrAC Trends in Analytical Chemistry 29 (9): 1073–81. https://doi.org/10.1016/j.trac.2010.05.006.\n\n\nBolstad, Benjamin Milo. 2004. Low-Level Analysis of\nHigh-density Oligonucleotide Array Data:\nBackground, Normalization and\nSummarization. University of California,\nBerkeley.\n\n\nCouch, Simon P., Andrew P. Bray, Chester Ismay, Evgeni Chasnovski,\nBenjamin S. Baumer, and Mine Çetinkaya-Rundel. 2021.\n“Infer: An r\nPackage for Tidyverse-Friendly Statistical Inference” 6: 3661. https://doi.org/10.21105/joss.03661.\n\n\nFranses, Philip Hans. 2016. “A Note on the Mean Absolute\nScaled Error.” International Journal of\nForecasting 32 (1): 20–22. https://doi.org/10.1016/j.ijforecast.2015.03.008.\n\n\nGentleman, Robert, Vincent Carey, Wolfgang Huber, Sandrine Dudoit, and\nRafael Irizarry. 2005. Bioinformatics and Computational\nBiology Solutions Using R and Bioconductor.\nSpringer.\n\n\nHyndman, Rob J., and Anne B. Koehler. 2006. “Another Look at\nMeasures of Forecast Accuracy.” International Journal of\nForecasting 22 (4): 679–88. https://doi.org/10.1016/j.ijforecast.2006.03.001.\n\n\nKuhn, Max, and Kjell Johnson. 2021. Feature Engineering\nand Selection: A Practical Approach for\nPredictive Models. Taylor & Francis\nGroup.\n\n\nKuhn, Max, and Hadley Wickham. 2020. “Tidymodels: A Collection of\nPackages for Modeling and Machine Learning Using Tidyverse\nPrinciples.” https://www.tidymodels.org.\n\n\nKvalseth, Tarald O. 1985. “Cautionary Note about\nR2.” The American Statistician 39 (4):\n279–85. https://doi.org/10.2307/2683704.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the Tidyverse.” Journal of Open Source\nSoftware 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nYeh, I.-Cheng. 2006. “Analysis of Strength of\nConcrete Using Design of Experiments and\nNeural Networks.” Journal of Materials in Civil\nEngineering 18 (4): 597–604. https://doi.org/10.1061/(ASCE)0899-1561(2006)18:4(597)."
  }
]