[
  {
    "objectID": "iterative.html",
    "href": "iterative.html",
    "title": "\n12  Przeszukiwanie iteracyjne\n",
    "section": "",
    "text": "12.1 Optymalizacja bayesowska\nW poprzednich rozdziałach pokazano, jak wyszukiwanie oparte o siatkę przyjmuje wstępnie zdefiniowany zestaw wartości kandydujących, ocenia je, a następnie wybiera najlepsze ustawienia. Iteracyjne metody wyszukiwania realizują inną strategię. Podczas procesu wyszukiwania przewidują one, które wartości należy przetestować w następnej kolejności.\nW tym rozdziale przedstawiono dwie metody wyszukiwania. Najpierw omówimy optymalizację bayesowską, która wykorzystuje model statystyczny do przewidywania lepszych ustawień parametrów. Następnie rozdział opisuje metodę globalnego wyszukiwania zwaną symulowanym wyżarzaniem (ang. simulated annealing).\nDo ilustracji wykorzystujemy te same dane dotyczące charakterystyki komórek, co w poprzednim rozdziale, ale zmieniamy model. W tym rozdziale użyjemy modelu maszyny wektorów nośnych (ang. Support Vector Machine), ponieważ zapewnia on ładne dwuwymiarowe wizualizacje procesów wyszukiwania. Dwa dostrajane parametry w optymalizacji to wartość kosztu SVM i parametr jądra funkcji radialnej \\(\\sigma\\). Oba parametry mogą mieć znaczący wpływ na złożoność i wydajność modelu.\nModel SVM wykorzystuje iloczyn skalarny i z tego powodu konieczne jest wyśrodkowanie i przeskalowanie predyktorów. Obiekty tidymodels: svm_rec, svm_spec i svm_wflow definiują proces tworzenia modelu:\nOto domyślne zakresy tuningowanych parametrów:\nDla ilustracji, zmieńmy nieco zakres parametrów jądra, aby poprawić wizualizacje wyszukiwania:\nZanim omówimy szczegóły dotyczące wyszukiwania iteracyjnego i jego działania, zbadajmy związek między dwoma parametrami dostrajania SVM a obszarem pod krzywą ROC dla tego konkretnego zestawu danych. Skonstruowaliśmy bardzo dużą regularną siatkę, składającą się z 2500 wartości kandydujących, i oceniliśmy siatkę przy użyciu resamplingu. Jest to oczywiście niepraktyczne w regularnej analizie danych i ogromnie nieefektywne. Jednakże, wyjaśnia ścieżkę, którą powinien podążać proces wyszukiwania i gdzie występuje numerycznie optymalna wartość (wartości).\nRys. 12.1 pokazuje wyniki oceny tej siatki, przy czym jaśniejszy kolor odpowiada wyższej (lepszej) wydajności modelu. W dolnej przekątnej przestrzeni parametrów znajduje się duży obszar, który jest stosunkowo płaski i charakteryzuje się słabą wydajnością. Grzbiet najlepszej wydajności występuje w prawej górnej części przestrzeni. Czarna kropka wskazuje najlepsze ustawienia. Przejście od płaskiego obszaru słabych wyników do grzbietu najlepszej wydajności jest bardzo ostre. Występuje również gwałtowny spadek obszaru pod krzywą ROC tuż po prawej stronie grzbietu.\nProcedury wyszukiwania, które będziemy przestawiać, wymagają przynajmniej kilku statystyk obliczonych na podstawie resamplingu. W tym celu poniższy kod tworzy małą regularną siatkę, która znajduje się w płaskiej części przestrzeni parametrów. Funkcja tune_grid() dokonuje próbkowania tej siatki:\nTa początkowa siatka pokazuje dość równoważne wyniki, przy czym żaden pojedynczy punkt nie jest znacznie lepszy od pozostałych. Wyniki te mogą być wykorzystane przez funkcje iteracyjnego dostrajania jako wartości początkowe.\nTechniki optymalizacji bayesowskiej analizują bieżące wyniki próbkowania i tworzą model predykcyjny, aby zasugerować wartości parametrów dostrajania, które nie zostały jeszcze ocenione. Sugerowana kombinacja parametrów jest następnie ponownie próbkowana. Wyniki te są następnie wykorzystywane w innym modelu predykcyjnym, który rekomenduje więcej wartości kandydatów do testowania, i tak dalej. Proces ten przebiega przez ustaloną liczbę iteracji lub do momentu, gdy nie pojawią się dalsze poprawy. Shahriari i in. (2016) i Frazier (2018) są dobrymi wprowadzeniami do optymalizacji bayesowskiej.\nPodczas korzystania z optymalizacji bayesowskiej, podstawowe problemy to sposób tworzenia modelu i wybór parametrów rekomendowanych przez ten model. Najpierw rozważmy technikę najczęściej stosowaną w optymalizacji bayesowskiej, czyli model procesu gaussowskiego.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Przeszukiwanie iteracyjne</span>"
    ]
  },
  {
    "objectID": "iterative.html#optymalizacja-bayesowska",
    "href": "iterative.html#optymalizacja-bayesowska",
    "title": "\n12  Przeszukiwanie iteracyjne\n",
    "section": "",
    "text": "12.1.1 Model procesu gaussowskiego\nModele procesu gaussowskiego (GP) (Schulz, Speekenbrink, i Krause 2016), to techniki statystyczne, które mają swoją historię w statystyce przestrzennej (pod nazwą metod krigingu). Mogą być wyprowadzone na wiele sposobów, w tym jako model Bayesowski.\nMatematycznie, GP jest zbiorem zmiennych losowych, których wspólny rozkład prawdopodobieństwa jest wielowymiarowy normalny. W kontekście naszych zastosowań jest to zbiór metryk wydajności dla wartości kandydujących parametrów dostrajania. Dla początkowej siatki czterech próbek, realizacje tych czterech zmiennych losowych wynosiły 0.8639, 0.8625, 0.8627 i 0.8659. Zakłada się, że mają rozkład wielowymiarowy normalny. Wejściami definiującymi zmienne niezależne dla modelu GP są odpowiednie wartości dostrajania parametrów (przedstawione w Tab. 12.1).\n\n\n\nTab. 12.1: Wartości startowe w procedurze poszukiwania optymalnych parametrów\n\n\n\n\n\n\nROC\ncost\nrbf_sigma\n\n\n\n0.8639\n0.01562\n1e-06\n\n\n0.8625\n2.00000\n1e-06\n\n\n0.8627\n0.01562\n1e-04\n\n\n0.8659\n2.00000\n1e-04\n\n\n\n\n\n\n\n\n\nModele procesów gaussowskich są określone przez ich funkcje średniej i kowariancji, choć to ta ostatnia ma większy wpływ na charakter modelu GP. Funkcja kowariancji jest często parametryzowana w kategoriach wartości wejściowych (oznaczanych jako \\(x\\)). Przykładowo, powszechnie stosowaną funkcją kowariancji jest funkcja wykładnicza kwadratowa:\n\\[\n\\operatorname{cov}(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\exp\\left(-\\frac{1}{2}|\\boldsymbol{x}_i - \\boldsymbol{x}_j|^2\\right) + \\sigma^2_{ij}\n\\tag{12.1}\\]\ngdzie \\(\\sigma_{i,j}^2\\) jest wariancją błędu modelu równą zero jeśli \\(i=j\\). Możemy to interpretować jako, że wraz ze wzrostem odległości pomiędzy dwoma kombinacjami parametrów, kowariancja pomiędzy metrykami wydajności rośnie wykładniczo. Z równania wynika również, że zmienność metryki wynikowej jest minimalizowana w punktach, które już zostały zaobserwowane (tzn. gdy \\(|x_i - x_j|^2\\) wynosi zero). Charakter tej funkcji kowariancji pozwala procesowi gaussowskiemu reprezentować wysoce nieliniowe zależności między wydajnością modelu a dostrajaniem parametrów, nawet jeśli istnieje tylko niewielka ilość danych.\nWażną zaletą tego modelu jest to, że ponieważ określony jest pełny model prawdopodobieństwa, przewidywania dla nowych wejść mogą odzwierciedlać cały rozkład wyniku. Innymi słowy, nowe statystyki wydajności mogą być przewidywane zarówno pod względem średniej jak i wariancji.\nNa podstawie początkowej siatki czterech wyników, model GP jest dopasowywany, następnie są obliczane z modelu predykcje dla kandydatów, a piąta kombinacja parametrów dostrajania jest wybierana. Obliczamy szacunkową wydajność dla nowej konfiguracji, GP jest ponownie dopasowywany do pięciu istniejących wyników (i tak dalej).\n\n12.1.2 Funkcja akwizycji\nJak wykorzystać model procesu gaussowskiego po dopasowaniu do aktualnych danych? Naszym celem jest wybranie następnej kombinacji dostrajania parametrów, która najprawdopodobniej da “lepsze wyniki” niż obecna najlepsza. Jednym z podejść do tego jest stworzenie dużego zbioru kandydatów, a następnie wykonanie prognoz średniej i wariancji dla każdego z nich. Korzystając z tych informacji, wybieramy najkorzystniejszą wartość parametru dostrajania.\n\n\n\n\nKlasa funkcji celu, zwanych funkcjami akwizycji, ułatwia kompromis pomiędzy średnią a wariancją. Przypomnijmy, że przewidywana wariancja modeli GP zależy głównie od tego, jak bardzo są one oddalone od istniejących danych. Kompromis pomiędzy przewidywaną średnią i wariancją dla nowych kandydatów jest często postrzegany przez pryzmat eksploracji i eksploatacji:\n\nEksploracja (ang. exploration) - powoduje wybór tych regionów, w których jest mniej obserwowanych modeli kandydujących. W ten sposób nadaje się większą wagę kandydatom o wyższej wariancji i koncentruje się na poszukiwaniu nowych wyników.\nEksploatacja (ang. exploitation) - zasadniczo opiera się istniejących wynikach, w celu odnalezienia najlepszej wartości średniej.\n\nAby zademonstrować, spójrzmy na przykład z jednym parametrem, który ma wartości pomiędzy [0, 1], a metryką wydajności jest \\(R^2\\). Prawdziwa funkcja jest pokazana na Rys. 12.2 wraz z pięcioma wartościami kandydującymi, które mają istniejące wyniki jako punkty.\n\n\n\n\n\nRys. 12.2: Hipotetyczny rzeczywisty profil wydajności dla arbitralnie wybranego parametru dostrajania, z pięcioma szacowanymi punktami\n\n\nDla tych danych dopasowanie modelu GP przedstawiono na Rys. 12.3. Zacieniowany obszar wskazuje średnią \\(\\pm\\) 1 błąd standardowy. Dwie pionowe linie wskazują dwa punkty kandydujące, które są później bardziej szczegółowo badane. Zacieniowany obszar ufności pokazuje funkcję wykładniczej wariancji kwadratowej; staje się ona bardzo duża między punktami i zbiega do zera w istniejących punktach danych.\n\n\n\n\n\nRys. 12.3: Przykładowy przebieg procesu gaussowskiego z zaznaczoną funkcją średniej i wariancji\n\n\nTa nieliniowa funkcja przechodzi przez każdy obserwowany punkt, ale model nie jest doskonały. Nie ma obserwowanych punktów w pobliżu prawdziwego optimum ustawienia, a w tym regionie dopasowanie mogłoby być znacznie lepsze. Pomimo tego, model GP może skutecznie wskazać nam właściwy kierunek.\nZ punktu widzenia czystej eksploatacji, najlepszym wyborem byłoby wybranie wartości parametru, który ma najlepszą średnią predykcję. W tym przypadku byłaby to wartość 0,106, tuż na prawo od istniejącego najlepszego zaobserwowanego punktu na poziomie 0,09.\nJako sposób na zachęcenie do eksploracji, prostym (ale nie często stosowanym) podejściem jest znalezienie parametru dostrajania związanego z największym przedziałem ufności.\nJedną z najczęściej stosowanych funkcji akwizycji jest oczekiwana poprawa. Pojęcie poprawy wymaga wartości dla bieżących najlepszych wyników (w przeciwieństwie do podejścia opartego na przedziałach ufności). Ponieważ GP może opisać nowy punkt kandydacki za pomocą rozkładu, możemy ważyć fragmenty rozkładu, które wykazują poprawę, używając prawdopodobieństwa wystąpienia poprawy.\nNa przykład, rozważmy dwie wartości parametrów kandydujących 0,10 i 0,25 (wskazane przez pionowe linie na Rys. 12.3). Używając dopasowanego modelu GP, ich przewidywane \\(R^2\\) są pokazane na Rys. 12.3 wraz z linią odniesienia dla aktualnych najlepszych wyników.\n\n\n\n\n\nRys. 12.4: Przewidywane rozkłady wydajności dla dwóch próbkowanych wartości parametrów dostrajania\n\n\nRozpatrując tylko średnią \\(R^2\\) lepszym wyborem jest wartość parametru 0,10 (patrz Tab. 12.1). Rekomendacja parametru dostrajania dla 0,25 ma gorsze przewidywanie średnie niż aktualny najlepszy kandydat. Jednakże, ponieważ ma wyższą wariancję, ma większy ogólny obszar prawdopodobieństwa powyżej aktualnego najlepszego. W rezultacie ma większą oczekiwaną poprawę:\n\nKodtab2 &lt;- tibble::tribble(\n  ~Parameter.Value,  ~Mean,  ~Std.Dev, ~Expected.Improvment,\n               0.1, 0.8679, 0.0004317,              0.00019,\n              0.25, 0.8671, 0.0039301,             0.001216\n  )\ngt(tab2)\n\n\nTab. 12.2: Oczekiwana poprawa dla dwóch kandydujących parametrów dostrajania.\n\n\n\n\n\n\nParameter.Value\nMean\nStd.Dev\nExpected.Improvment\n\n\n\n0.10\n0.8679\n0.0004317\n0.000190\n\n\n0.25\n0.8671\n0.0039301\n0.001216\n\n\n\n\n\n\n\n\n\nKiedy oczekiwana poprawa jest obliczana w całym zakresie dostrajania parametrów, zalecany punkt do próbkowania jest znacznie bliższy 0,25 niż 0,10, jak pokazano na Rys. 12.5.\n\n\n\n\n\nRys. 12.5: Szacowany profil wydajności wygenerowany przez model procesu gaussowskiego (górny panel) oraz oczekiwana poprawa (dolny panel). Pionowa linia wskazuje punkt maksymalnej poprawy.\n\n\nAby zaimplementować wyszukiwanie iteracyjne poprzez optymalizację bayesowską, należy użyć funkcji tune_bayes(). Jej składnia jest bardzo podobna do tune_grid(), z kilkoma dodatkowymi argumentami:\n\n\niter to maksymalna liczba iteracji wyszukiwania.\n\ninitial może być liczbą całkowitą, obiektem utworzonym przy pomocy tune_grid(), albo jedną z funkcji wyścigowych. Użycie liczby całkowitej określa rozmiar konstrukcji wypełniającej przestrzeń, która jest próbkowana przed pierwszym modelem GP.\n\nobjective jest argumentem, dla którego należy użyć funkcji akwizycji. Pakiet tune zawiera funkcje takie jak exp_improve() lub conf_bound().\nArgument param_info, w tym przypadku określa zakres parametrów, jak również wszelkie transformacje.\n\nArgument control funkcji tune_bayes() ustawia się za pomocą control_bayes(). Niektóre istotne argumenty to:\n\n\nno_improve to liczba całkowita, która zatrzyma wyszukiwanie, jeśli ulepszone parametry nie zostaną odkryte w ciągu iteracji no_improve.\n\nuncertain jest również liczbą całkowitą (lub Inf), używaną do ustalenia liczby przejść algorytmu wg reguły eksploatacji bez poprawy, aby następnie wybrać próbę z zakresu z wysoką wariancją, po to żeby dokonać eksploracji.\n\nverbose jest parametrem, który decyduje co będzie się wyświetlało podczas przebiegu algorytmu.\n\nUżyjmy pierwszych wyników SVM jako początkowego podłoża dla modelu GP. Przypomnijmy, że w tym zastosowaniu chcemy zmaksymalizować obszar pod krzywą ROC.\n\nKodctrl &lt;- control_bayes(verbose = TRUE)\n\nset.seed(1403)\nsvm_bo &lt;-\n  svm_wflow %&gt;%\n  tune_bayes(\n    resamples = cell_folds,\n    metrics = roc_res,\n    initial = svm_initial,\n    param_info = svm_param,\n    iter = 25,\n    control = ctrl\n  )\n\n\n\nKodcollect_metrics(svm_bo) |&gt; \n  gt()\n\n\nTab. 12.3: Przebieg optymalizacji baysowskiej\n\n\n\n\n\n\ncost\nrbf_sigma\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n.iter\n\n\n\n0.015625000\n1.000000e-06\nroc_auc\nbinary\n0.8638724\n10\n0.008637894\nPreprocessor1_Model1\n0\n\n\n2.000000000\n1.000000e-06\nroc_auc\nbinary\n0.8625326\n10\n0.008672545\nPreprocessor1_Model2\n0\n\n\n0.015625000\n1.000000e-04\nroc_auc\nbinary\n0.8627495\n10\n0.008624554\nPreprocessor1_Model3\n0\n\n\n2.000000000\n1.000000e-04\nroc_auc\nbinary\n0.8659439\n10\n0.008545691\nPreprocessor1_Model4\n0\n\n\n0.110831031\n2.665887e-02\nroc_auc\nbinary\n0.8882389\n10\n0.008697493\nIter1\n1\n\n\n0.002570159\n1.498759e-02\nroc_auc\nbinary\n0.8733510\n10\n0.008784422\nIter2\n2\n\n\n0.076743095\n3.257686e-02\nroc_auc\nbinary\n0.8820147\n10\n0.008962769\nIter3\n3\n\n\n0.158777868\n8.699292e-04\nroc_auc\nbinary\n0.8646797\n10\n0.008717232\nIter4\n4\n\n\n0.028725163\n2.614512e-02\nroc_auc\nbinary\n0.8748373\n10\n0.009077821\nIter5\n5\n\n\n0.110836919\n4.063308e-07\nroc_auc\nbinary\n0.8640364\n10\n0.008588054\nIter6\n6\n\n\n0.161280423\n2.376079e-02\nroc_auc\nbinary\n0.8912983\n10\n0.008546152\nIter7\n7\n\n\n0.191548445\n1.753100e-02\nroc_auc\nbinary\n0.8924207\n10\n0.008604149\nIter8\n8\n\n\n0.322506923\n2.024837e-02\nroc_auc\nbinary\n0.8951338\n10\n0.008488570\nIter9\n9\n\n\n0.616915747\n2.547641e-02\nroc_auc\nbinary\n0.8953431\n10\n0.008324862\nIter10\n10\n\n\n0.528787926\n2.052531e-02\nroc_auc\nbinary\n0.8954957\n10\n0.008263982\nIter11\n11\n\n\n0.552359295\n2.381201e-02\nroc_auc\nbinary\n0.8954230\n10\n0.008235443\nIter12\n12\n\n\n0.518642133\n2.172084e-02\nroc_auc\nbinary\n0.8957039\n10\n0.008241457\nIter13\n13\n\n\n0.699416391\n2.187984e-02\nroc_auc\nbinary\n0.8954909\n10\n0.008317203\nIter14\n14\n\n\n22.086197445\n3.848670e-02\nroc_auc\nbinary\n0.8647523\n10\n0.009452636\nIter15\n15\n\n\n0.415152612\n2.230321e-02\nroc_auc\nbinary\n0.8951613\n10\n0.008306736\nIter16\n16\n\n\n0.001157284\n2.435740e-03\nroc_auc\nbinary\n0.8656808\n10\n0.008630693\nIter17\n17\n\n\n0.924338505\n7.885258e-02\nroc_auc\nbinary\n0.8858168\n10\n0.008167638\nIter18\n18\n\n\n29.083907487\n1.192898e-03\nroc_auc\nbinary\n0.8979424\n10\n0.008064529\nIter19\n19\n\n\n31.924080961\n6.212242e-04\nroc_auc\nbinary\n0.8942442\n10\n0.008618062\nIter20\n20\n\n\n10.599233064\n1.275900e-03\nroc_auc\nbinary\n0.8952500\n10\n0.008543058\nIter21\n21\n\n\n30.740481343\n1.907822e-03\nroc_auc\nbinary\n0.8985736\n10\n0.007908688\nIter22\n22\n\n\n24.204296906\n1.712143e-03\nroc_auc\nbinary\n0.8983898\n10\n0.007782671\nIter23\n23\n\n\n31.939709359\n1.703217e-03\nroc_auc\nbinary\n0.8988150\n10\n0.007769977\nIter24\n24\n\n\n0.001100257\n1.000011e-07\nroc_auc\nbinary\n0.8639531\n10\n0.008702518\nIter25\n25\n\n\n\n\n\n\n\n\n\n\nKodshow_best(svm_bo) |&gt; \n  gt()\n\n\nTab. 12.4: Zestaw optymalnych parametrów na podstawie OB\n\n\n\n\n\n\ncost\nrbf_sigma\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n.iter\n\n\n\n31.9397094\n0.001703217\nroc_auc\nbinary\n0.8988150\n10\n0.007769977\nIter24\n24\n\n\n30.7404813\n0.001907822\nroc_auc\nbinary\n0.8985736\n10\n0.007908688\nIter22\n22\n\n\n24.2042969\n0.001712143\nroc_auc\nbinary\n0.8983898\n10\n0.007782671\nIter23\n23\n\n\n29.0839075\n0.001192898\nroc_auc\nbinary\n0.8979424\n10\n0.008064529\nIter19\n19\n\n\n0.5186421\n0.021720838\nroc_auc\nbinary\n0.8957039\n10\n0.008241457\nIter13\n13\n\n\n\n\n\n\n\n\n\n\nKodautoplot(svm_bo, type = \"performance\")\n\n\n\n\n\n\nRys. 12.6: Jakość dopasowania modelu w poszczególnych iteracjach OB\n\n\n\n\nPoniższa animacja wizualizuje wyniki wyszukiwania. Czarne “x” pokazują wartości początkowe zawarte w svm_initial. Niebieski panel u góry po lewej stronie pokazuje przewidywaną średnią wartość obszaru pod krzywą ROC. Czerwony panel na górze po prawej stronie pokazuje przewidywaną zmienność wartości ROC, podczas gdy dolny wykres wizualizuje oczekiwaną poprawę. W każdym panelu ciemniejsze kolory wskazują mniej atrakcyjne wartości (np. małe wartości średnie, duże zróżnicowanie i małe ulepszenia).\n\n\nVideo\n\n\nRys. 12.7: Przykład działania optymalizacji bayesowskiej\n\n\nPowierzchnia przewidywanej średniej jest bardzo niedokładna w pierwszych kilku iteracjach wyszukiwania. Pomimo tego, pomaga ona poprowadzić proces w rejon dobrej wydajności. W ciągu pierwszych dziesięciu iteracji, wyszukiwanie jest dokonywane w pobliżu optymalnego miejsca.\nPodczas gdy najlepsza kombinacja dostrajania parametrów znajduje się na granicy przestrzeni parametrów, optymalizacja bayesowska często wybierze nowe punkty poza granicami przestrzeni parametrów.\nPowyższy przykład startował z punktów startowych wybranych nieco arbitralnie ale nieco lepsze wyniki można osiągnąć stosując losowe wypełnienie przestrzeni parametrów.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Przeszukiwanie iteracyjne</span>"
    ]
  },
  {
    "objectID": "iterative.html#symulowane-wyżarzanie",
    "href": "iterative.html#symulowane-wyżarzanie",
    "title": "\n12  Przeszukiwanie iteracyjne\n",
    "section": "\n12.2 Symulowane wyżarzanie",
    "text": "12.2 Symulowane wyżarzanie\nSymulowane wyżarzanie (ang. simulated annealing) (Kirkpatrick, Gelatt, i Vecchi 1983; Laarhoven i Aarts 1987); jest nieliniową procedurą wyszukiwania zainspirowaną procesem stygnięcia metalu. Jest to metoda globalnego wyszukiwania, która może efektywnie poruszać się po wielu różnych obszarach poszukiwań, w tym po funkcjach nieciągłych. W przeciwieństwie do większości procedur optymalizacji opartych na gradiencie, symulowane wyżarzanie może ponownie ocenić poprzednie rozwiązania.\nProces użycia symulowanego wyżarzania rozpoczyna się od wartości początkowej i rozpoczyna kontrolowany losowy spacer przez przestrzeń parametrów. Każda nowa wartość parametru-kandydata jest niewielką perturbacją poprzedniej wartości, która utrzymuje nowy punkt w lokalnym sąsiedztwie.\nPunkt kandydujący jest oceniana przy zastosowaniu resamplingu, aby uzyskać odpowiadającą mu wartość wydajności. Jeśli osiąga ona lepsze wyniki niż poprzednie parametry, jest akceptowana jako nowa najlepsza i proces jest kontynuowany. Jeśli wyniki są gorsze niż poprzednia wartość, procedura wyszukiwania może nadal używać tego parametru do określenia dalszych kroków. Zależy to od dwóch czynników. Po pierwsze, prawdopodobieństwo zatrzymania złego kandydata maleje wraz z pogorszeniem się wyników. Innymi słowy, tylko nieco gorszy wynik od obecnie najlepszego ma większą szansę na akceptację niż ten z dużym spadkiem wydajności. Drugim czynnikiem jest liczba iteracji wyszukiwania. Symulowane wyżarzanie próbuje zaakceptować mniej suboptymalnych wartości w miarę postępu wyszukiwania. Z tych dwóch czynników prawdopodobieństwo akceptacji złego wyniku można sformalizować jako:\n\\[\n\\operatorname{Pr}[\\text{accept suboptimal parameters at iteration } i] = \\exp(c\\times D_i \\times i)\n\\tag{12.2}\\]\ngdzie \\(i\\) jest numerem iteracji, \\(c\\) jest stałą określoną przez użytkownika, \\(D_i\\) jest procentową różnicą pomiędzy starą i nową wartością (gdzie wartości ujemne oznaczają gorsze wyniki). Dla złego wyniku określamy prawdopodobieństwo akceptacji i porównujemy je z liczbą wylosowaną z rozkładu jednostajnego. Jeśli liczba ta jest większa od wartości prawdopodobieństwa, wyszukiwanie odrzuca bieżące parametry i następna iteracja tworzy swoją wartość kandydata w sąsiedztwie poprzedniej wartości. W przeciwnym razie następna iteracja tworzy kolejny zestaw parametrów na podstawie bieżących (suboptymalnych) wartości.\n\n\n\n\n\n\nZagrożenie\n\n\n\nPrawdopodobieństwa akceptacji symulowanego wyżarzania pozwalają na postępowanie w złym kierunku, przynajmniej na krótką metę, z potencjałem znalezienia znacznie lepszego regionu przestrzeni parametrów w dłuższej perspektywie.\n\n\nMapa ciepła na Rys. 12.8 pokazuje, jak prawdopodobieństwo akceptacji może się zmieniać w zależności od iteracji, wydajności i współczynnika określonego przez użytkownika.\n\n\n\n\n\nRys. 12.8: Mapa ciepła prawdopodobieństwa akceptacji symulowanego wyżarzania dla różnych wartości współczynnika\n\n\nUżytkownik może dostosować współczynniki \\(c\\), aby znaleźć profil prawdopodobieństwa, który odpowiada jego potrzebom. W finetune::control_sim_anneal(), domyślnym dla tego argumentu cooling_coef jest 0.02. Zmniejszenie tego współczynnika zachęci wyszukiwanie do bycia bardziej wyrozumiałym dla słabych wyników.\nProces ten trwa przez określoną ilość iteracji, ale może zostać zatrzymany, jeśli w ciągu określonej liczby iteracji nie pojawią się globalnie najlepsze wyniki. Bardzo pomocne może być ustawienie progu restartu. Jeśli wystąpi ciąg niepowodzeń, funkcja ta powraca do ostatnich globalnie najlepszych ustawień parametrów i zaczyna od nowa.\n\n\n\n\nNajważniejszym szczegółem jest określenie sposobu perturbacji parametrów dostrajania z iteracji na iterację. W literaturze można znaleźć wiele metod na to. My stosujemy metodę podaną przez Bohachevsky, Johnson, i Stein (1986) zwaną uogólnionym symulowanym wyżarzaniem. Dla ciągłych parametrów dostrajania definiujemy mały promień, aby określić lokalne “sąsiedztwo”. Na przykład załóżmy, że są dwa parametry dostrajania i każdy z nich jest ograniczony przez zero i jeden. Proces symulowanego wyżarzania generuje losowe wartości na otaczającym promieniu i losowo wybiera jedną z nich jako aktualną wartość kandydacką.\nWielkość promienia kontroluje, jak szybko wyszukiwanie bada przestrzeń parametrów. Im większy jest promień tym szybciej przestrzeń parametrów będzie przeszukiwana przez algorytm ale jednocześnie mniej dokładnie.\nDla zilustrowania użyjemy dwóch głównych parametrów dostrajania glmnet:\n\nWielkość kary (penalty). Domyślny zakres tego parametru to od \\(10^{-10}\\) do \\(10^0\\). Najczęściej jest podawany w skali logarytmicznej o podstawie 10.\nProporcja kary lasso (mixture) - wartość pomiędzy 0 i 1 określająca balans pomiędzy karą L1 i L2.\n\nProces rozpoczyna się od wartości początkowych penalty = 0,025 i mixture = 0,050. Używając promienia, który losowo waha się między 0,050 a 0,015, dane są odpowiednio skalowane, losowe wartości są generowane na promieniach wokół punktu początkowego, a następnie jedna jest losowo wybierana jako kandydat. Dla ilustracji przyjmiemy, że wszystkie wartości kandydujące faktycznie poprawiają wydajność modelu. Korzystając z nowej wartości, generowany jest zestaw nowych losowych sąsiadów, wybierany jest jeden, i tak dalej. Rys. 12.9 przedstawia sześć iteracji sukcesywnie podążających do lewego górnego rogu.\n\n\n\n\n\nRys. 12.9: Kilka iteracji procesu symulowanego wyżarzania\n\n\nZauważmy, że podczas niektórych iteracji zestawy kandydatów wzdłuż promienia wykluczają punkty poza granicami parametrów. Ponadto, nasza implementacja przesuwa wybór kolejnych konfiguracji dostrajania parametrów od nowych wartości, które są bardzo podobne do poprzednich konfiguracji.\nDla parametrów kategorycznych i całkowitych, każdy z nich jest generowany z określonym wcześniej prawdopodobieństwem. Argument flip w control_sim_anneal() może być użyty do określenia tego prawdopodobieństwa. Dla parametrów całkowitoliczbowych, używana jest najbliższa wartość całkowita.\nPrzeszukiwanie metodą symulowanego wyżarzania może nie być optymalna w przypadku, gdy wiele parametrów jest nienumerycznych lub całkowitoliczbowych o niewielu unikalnych wartościach. W tych przypadkach jest prawdopodobne, że ten sam zestaw kandydatów może być testowany więcej niż raz.\nAby zaimplementować wyszukiwanie iteracyjne poprzez symulowane wyżarzanie, użyj funkcji tune_sim_anneal(). Składnia tej funkcji jest niemal identyczna jak tune_bayes(). Nie ma żadnych opcji dotyczących funkcji akwizycji czy próbkowania niepewności. Funkcja control_sim_anneal() posiada pewne szczegóły, które definiują lokalne sąsiedztwo i harmonogram chłodzenia:\n\n\nno_improve jest liczbą całkowitą, która zatrzyma wyszukiwanie, jeśli w ciągu iteracji określonym przez no_improve nie zostaną odkryte globalnie najlepsze wyniki. Przyjęte suboptymalne lub odrzucone parametry liczą się jako “brak poprawy”.\n\nrestart to liczba iteracji, która musi minąć bez poprawy jakości modelu, po której następuje przejście do najlepszego poprzednio ustalonego zestawu parametrów.\n\nradius to wektor liczbowy w przedziale (0, 1), który określa minimalny i maksymalny promień lokalnego sąsiedztwa wokół punktu początkowego.\n\nflip to wartość prawdopodobieństwa określająca szanse zmiany wartości parametrów kategorycznych lub całkowitych.\n\ncooling_coef jest współczynnikiem \\(c\\) w \\(\\exp(c\\times D_i \\times i)\\), który moduluje jak szybko prawdopodobieństwo akceptacji maleje w trakcie iteracji. Większe wartości cooling_coef zmniejszają prawdopodobieństwo akceptacji suboptymalnego ustawienia parametrów.\n\nDla danych dotyczących segmentacji komórek składnia jest bardzo spójna z poprzednio stosowanymi funkcjami:\n\nKodctrl_sa &lt;- control_sim_anneal(verbose = TRUE, no_improve = 10L)\n\nset.seed(1404)\nsvm_sa &lt;-\n  svm_wflow %&gt;%\n  tune_sim_anneal(\n    resamples = cell_folds,\n    metrics = roc_res,\n    initial = svm_initial,\n    param_info = svm_param,\n    iter = 50,\n    control = ctrl_sa\n  )\n\n\nDane wyjściowe dla poszczególnych iteracji:\n\nKodcollect_metrics(svm_sa) |&gt; \n  gt()\n\n\nTab. 12.5: Podsumowanie symulowanego wyżarzania\n\n\n\n\n\n\ncost\nrbf_sigma\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n.iter\n\n\n\n0.0156250\n1.000000e-06\nroc_auc\nbinary\n0.8638724\n10\n0.008637894\ninitial_Preprocessor1_Model1\n0\n\n\n2.0000000\n1.000000e-06\nroc_auc\nbinary\n0.8625326\n10\n0.008672545\ninitial_Preprocessor1_Model2\n0\n\n\n0.0156250\n1.000000e-04\nroc_auc\nbinary\n0.8627495\n10\n0.008624554\ninitial_Preprocessor1_Model3\n0\n\n\n2.0000000\n1.000000e-04\nroc_auc\nbinary\n0.8659439\n10\n0.008545691\ninitial_Preprocessor1_Model4\n0\n\n\n2.7792215\n4.231877e-05\nroc_auc\nbinary\n0.8635104\n10\n0.008642153\nIter1\n1\n\n\n8.9751798\n1.317101e-04\nroc_auc\nbinary\n0.8733437\n10\n0.008401879\nIter2\n2\n\n\n8.0489544\n3.113457e-05\nroc_auc\nbinary\n0.8672404\n10\n0.008380301\nIter3\n3\n\n\n7.4631879\n6.401369e-05\nroc_auc\nbinary\n0.8704169\n10\n0.008147365\nIter4\n4\n\n\n25.4276256\n6.964316e-05\nroc_auc\nbinary\n0.8744436\n10\n0.008459084\nIter5\n5\n\n\n26.0442746\n1.620597e-05\nroc_auc\nbinary\n0.8696570\n10\n0.008110480\nIter6\n6\n\n\n27.8247939\n6.830484e-06\nroc_auc\nbinary\n0.8658910\n10\n0.008447841\nIter7\n7\n\n\n11.5246937\n1.459848e-06\nroc_auc\nbinary\n0.8623315\n10\n0.008657469\nIter8\n8\n\n\n11.5911446\n4.938754e-06\nroc_auc\nbinary\n0.8622997\n10\n0.008647646\nIter9\n9\n\n\n16.0739132\n2.533161e-06\nroc_auc\nbinary\n0.8623219\n10\n0.008658226\nIter10\n10\n\n\n22.9676231\n1.507608e-06\nroc_auc\nbinary\n0.8623751\n10\n0.008670690\nIter11\n11\n\n\n14.6174389\n3.625603e-07\nroc_auc\nbinary\n0.8624605\n10\n0.008645675\nIter12\n12\n\n\n30.2128582\n1.354001e-07\nroc_auc\nbinary\n0.8625366\n10\n0.008612533\nIter13\n13\n\n\n14.1812532\n7.415027e-05\nroc_auc\nbinary\n0.8726092\n10\n0.008351119\nIter14\n14\n\n\n5.1974958\n1.892985e-04\nroc_auc\nbinary\n0.8727956\n10\n0.008301061\nIter15\n15\n\n\n6.2799813\n1.101844e-03\nroc_auc\nbinary\n0.8897887\n10\n0.008859945\nIter16\n16\n\n\n1.7551679\n2.895039e-03\nroc_auc\nbinary\n0.8918849\n10\n0.008922869\nIter17\n17\n\n\n2.7450163\n7.152477e-03\nroc_auc\nbinary\n0.8958891\n9\n0.009100454\nIter18\n18\n\n\n0.6580640\n7.237199e-03\nroc_auc\nbinary\n0.8944414\n10\n0.008826226\nIter19\n19\n\n\n0.2894265\n2.800490e-03\nroc_auc\nbinary\n0.8781751\n10\n0.008519248\nIter20\n20\n\n\n0.8648804\n9.061603e-04\nroc_auc\nbinary\n0.8744488\n10\n0.008274188\nIter21\n21\n\n\n2.6900002\n4.370896e-04\nroc_auc\nbinary\n0.8748006\n10\n0.008467929\nIter22\n22\n\n\n1.9738868\n8.518383e-04\nroc_auc\nbinary\n0.8787410\n10\n0.008594452\nIter23\n23\n\n\n2.8390760\n1.343734e-04\nroc_auc\nbinary\n0.8694466\n10\n0.008255440\nIter24\n24\n\n\n0.6443050\n2.818297e-04\nroc_auc\nbinary\n0.8660129\n10\n0.008642925\nIter25\n25\n\n\n0.4332128\n8.494378e-04\nroc_auc\nbinary\n0.8702872\n10\n0.008407344\nIter26\n26\n\n\n1.5062749\n4.117203e-02\nroc_auc\nbinary\n0.8868902\n10\n0.008384564\nIter27\n27\n\n\n1.4313131\n1.549218e-02\nroc_auc\nbinary\n0.8963429\n10\n0.008367098\nIter28\n28\n\n\n2.7699626\n1.362418e-02\nroc_auc\nbinary\n0.8929540\n10\n0.008489741\nIter29\n29\n\n\n4.9266582\n4.583077e-02\nroc_auc\nbinary\n0.8698810\n10\n0.009097231\nIter30\n30\n\n\n7.7758900\n9.490760e-03\nroc_auc\nbinary\n0.8874485\n10\n0.008767151\nIter31\n31\n\n\n4.8976312\n1.580979e-02\nroc_auc\nbinary\n0.8825035\n10\n0.009000858\nIter32\n32\n\n\n12.4918485\n8.493479e-03\nroc_auc\nbinary\n0.8829449\n10\n0.008897035\nIter33\n33\n\n\n9.4138354\n2.056779e-03\nroc_auc\nbinary\n0.8971685\n10\n0.008342496\nIter34\n34\n\n\n16.7499203\n2.398614e-03\nroc_auc\nbinary\n0.8985179\n10\n0.007916078\nIter35\n35\n\n\n18.8139030\n4.403197e-04\nroc_auc\nbinary\n0.8839150\n9\n0.009020320\nIter36\n36\n\n\n21.9409340\n1.216970e-03\nroc_auc\nbinary\n0.8973481\n10\n0.008280488\nIter37\n37\n\n\n8.7296351\n5.154540e-03\nroc_auc\nbinary\n0.8961246\n10\n0.008263055\nIter38\n38\n\n\n13.5493211\n2.655684e-02\nroc_auc\nbinary\n0.8593977\n10\n0.009612964\nIter39\n39\n\n\n3.4729934\n1.312798e-03\nroc_auc\nbinary\n0.8847509\n9\n0.009126495\nIter40\n40\n\n\n3.1103793\n4.869128e-03\nroc_auc\nbinary\n0.8969694\n10\n0.008168916\nIter41\n41\n\n\n1.2292254\n6.540634e-03\nroc_auc\nbinary\n0.8965152\n10\n0.008550536\nIter42\n42\n\n\n0.5769315\n2.846348e-03\nroc_auc\nbinary\n0.8842725\n10\n0.008540335\nIter43\n43\n\n\n22.9584171\n1.241411e-02\nroc_auc\nbinary\n0.8618903\n10\n0.009740857\nIter44\n44\n\n\n10.6297370\n4.059190e-03\nroc_auc\nbinary\n0.8975137\n10\n0.008203966\nIter45\n45\n\n\n10.6164560\n2.519507e-02\nroc_auc\nbinary\n0.8617737\n10\n0.009517440\nIter46\n46\n\n\n24.5183146\n7.016539e-03\nroc_auc\nbinary\n0.8775665\n10\n0.008934404\nIter47\n47\n\n\n10.2189037\n1.330918e-02\nroc_auc\nbinary\n0.8733744\n10\n0.009258351\nIter48\n48\n\n\n7.1021781\n1.966405e-03\nroc_auc\nbinary\n0.8966379\n10\n0.008539778\nIter49\n49\n\n\n9.2002334\n5.308068e-04\nroc_auc\nbinary\n0.8841932\n10\n0.008839438\nIter50\n50\n\n\n\n\n\n\n\n\n\n\nKodshow_best(svm_sa) |&gt; \n  gt()\n\n\nTab. 12.6: Najlepsze kombinacje parametrów na podstawie SA\n\n\n\n\n\n\ncost\nrbf_sigma\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n.iter\n\n\n\n16.749920\n0.002398614\nroc_auc\nbinary\n0.8985179\n10\n0.007916078\nIter35\n35\n\n\n10.629737\n0.004059190\nroc_auc\nbinary\n0.8975137\n10\n0.008203966\nIter45\n45\n\n\n21.940934\n0.001216970\nroc_auc\nbinary\n0.8973481\n10\n0.008280488\nIter37\n37\n\n\n9.413835\n0.002056779\nroc_auc\nbinary\n0.8971685\n10\n0.008342496\nIter34\n34\n\n\n3.110379\n0.004869128\nroc_auc\nbinary\n0.8969694\n10\n0.008168916\nIter41\n41\n\n\n\n\n\n\n\n\n\nPodobnie jak w przypadku innych funkcji tune_*(), odpowiadająca im funkcja autoplot() tworzy wizualną ocenę wyników. Użycie autoplot(svm_sa, type = \"performance\") pokazuje wydajność w czasie iteracji (Rys. 12.10), podczas gdy autoplot(svm_sa, type = \"parameters\") przedstawia wydajność w zależności od konkretnych wartości dostrajania parametrów (Rys. 12.11).\n\nKodautoplot(svm_sa, type = \"performance\")\n\n\n\n\n\n\nRys. 12.10: Przebieg procesu symulowanego wyżarzania\n\n\n\n\n\nKodautoplot(svm_sa, type = \"parameters\")\n\n\n\n\n\n\nRys. 12.11: Wydajność modeli w kontekście hiperparametrów modelu\n\n\n\n\nWizualizacja ścieżki wyszukiwania pomaga zrozumieć, gdzie proces wyszukiwania poradził sobie dobrze, a gdzie pobłądził:\n\n\nVideo\n\n\nRys. 12.12: Wizualizacja przebiegu symulowanego wyżarzania\n\n\n\n\n\n\nBohachevsky, Ihor O., Mark E. Johnson, i Myron L. Stein. 1986. „Generalized Simulated Annealing for Function Optimization”. Technometrics 28 (3): 209–17. https://doi.org/10.1080/00401706.1986.10488128.\n\n\nFrazier, Peter I. 2018. „A Tutorial on Bayesian Optimization”. https://doi.org/10.48550/ARXIV.1807.02811.\n\n\nKirkpatrick, S., C. D. Gelatt, i M. P. Vecchi. 1983. „Optimization by Simulated Annealing”. Science 220 (4598): 671–80. https://doi.org/10.1126/science.220.4598.671.\n\n\nLaarhoven, Peter J. M. van, i Emile H. L. Aarts. 1987. „Simulated annealing”. W, 7–15. Springer Netherlands. https://doi.org/10.1007/978-94-015-7744-1_2.\n\n\nSchulz, Eric, Maarten Speekenbrink, i Andreas Krause. 2016. „A tutorial on Gaussian process regression: Modelling, exploring, and exploiting functions”. http://dx.doi.org/10.1101/095190.\n\n\nShahriari, Bobak, Kevin Swersky, Ziyu Wang, Ryan P. Adams, i Nando de Freitas. 2016. „Taking the Human Out of the Loop: A Review of Bayesian Optimization”. Proceedings of the IEEE 104 (1): 148–75. https://doi.org/10.1109/jproc.2015.2494218.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Przeszukiwanie iteracyjne</span>"
    ]
  },
  {
    "objectID": "example.html",
    "href": "example.html",
    "title": "\n13  Poszukiwanie optymalnego modelu\n",
    "section": "",
    "text": "13.1 Przygotowanie danych\nW przypadku projektów z nowymi zbiorami danych, które nie zostały jeszcze dobrze poznane, osoba zajmująca się danymi może być zmuszona do sprawdzenia wielu kombinacji modeli i różnych kombinacji metod przygotowania danych. Powszechne jest to, że nie posiadamy wiedzy a priori na temat tego, która metoda będzie działać najlepiej z nowym zestawem danych.\nDobrą strategią jest poświęcenie trochę uwagi na wypróbowanie różnych podejść do modelowania, określenie, co działa najlepiej, a następnie zainwestowanie dodatkowego czasu na dostosowanie / optymalizację małego zestawu modeli.\nAby zademonstrować, jak przesiewać zestaw wielu modeli, użyjemy jako przykładu danych mieszanki betonowej z książki Applied Predictive Modeling (Khun i Johnson 2013). W rozdziale 10 tej książki zademonstrowano modele do przewidywania wytrzymałości na ściskanie mieszanek betonowych z wykorzystaniem składników jako zmiennych niezależnych. Oceniono wiele różnych modeli z różnymi zestawami predyktorów i typami metod wstępnego przetwarzania.\nKodlibrary(tidymodels)\nlibrary(finetune)\ntidymodels_prefer()\ndata(concrete, package = \"modeldata\")\nglimpse(concrete)\n\nRows: 1,030\nColumns: 9\n$ cement               &lt;dbl&gt; 540.0, 540.0, 332.5, 332.5, 198.6, 266.0, 380.0, …\n$ blast_furnace_slag   &lt;dbl&gt; 0.0, 0.0, 142.5, 142.5, 132.4, 114.0, 95.0, 95.0,…\n$ fly_ash              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ water                &lt;dbl&gt; 162, 162, 228, 228, 192, 228, 228, 228, 228, 228,…\n$ superplasticizer     &lt;dbl&gt; 2.5, 2.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,…\n$ coarse_aggregate     &lt;dbl&gt; 1040.0, 1055.0, 932.0, 932.0, 978.4, 932.0, 932.0…\n$ fine_aggregate       &lt;dbl&gt; 676.0, 676.0, 594.0, 594.0, 825.5, 670.0, 594.0, …\n$ age                  &lt;int&gt; 28, 28, 270, 365, 360, 90, 365, 28, 28, 28, 90, 2…\n$ compressive_strength &lt;dbl&gt; 79.99, 61.89, 40.27, 41.05, 44.30, 47.03, 43.70, …\nZmienna compressive_strength jest zmienną zależną, przewidywaną w tym zadaniu. W kilku przypadkach w tym zestawie danych, ta sama formuła betonu była testowana wielokrotnie. Wolimy nie uwzględniać tych replikowanych mieszanek jako pojedynczych punktów danych, ponieważ mogą one być rozmieszczone zarówno w zbiorze treningowym, jak i testowym. Może to sztucznie zawyżyć nasze szacunki wydajności.\nKodconcrete &lt;- \n   concrete %&gt;% \n   group_by(across(-compressive_strength)) %&gt;% \n   summarize(compressive_strength = mean(compressive_strength),\n             .groups = \"drop\")\nnrow(concrete)\n\n[1] 992\nPodzielmy dane przy użyciu domyślnego stosunku 3:1 treningu do testu i ponownie wypróbujmy zestaw treningowy przy użyciu pięciu powtórzeń 10-krotnej walidacji krzyżowej:\nKodset.seed(1501)\nconcrete_split &lt;- initial_split(concrete, strata = compressive_strength)\nconcrete_train &lt;- training(concrete_split)\nconcrete_test  &lt;- testing(concrete_split)\n\nset.seed(1502)\nconcrete_folds &lt;- \n   vfold_cv(concrete_train, strata = compressive_strength, repeats = 5)\nNiektóre modele (zwłaszcza sieci neuronowe, KNN i SVM) wymagają predyktorów, które zostały wyśrodkowane i przeskalowane, więc niektóre przepływy pracy modelu będą wymagały przepisów z tymi krokami przetwarzania wstępnego. Dla innych modeli, tradycyjne rozwinięcie modelu powierzchni odpowiedzi (tj. interakcje kwadratowe i dwukierunkowe) jest dobrym pomysłem. Dla tych celów tworzymy dwie receptury:\nKodnormalized_rec &lt;- \n   recipe(compressive_strength ~ ., data = concrete_train) %&gt;% \n   step_normalize(all_predictors()) \n\npoly_recipe &lt;- \n   normalized_rec %&gt;% \n   step_poly(all_predictors()) %&gt;% \n   step_interact(~ all_predictors():all_predictors())",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Poszukiwanie optymalnego modelu</span>"
    ]
  },
  {
    "objectID": "example.html#określenie-modeli",
    "href": "example.html#określenie-modeli",
    "title": "\n13  Poszukiwanie optymalnego modelu\n",
    "section": "\n13.2 Określenie modeli",
    "text": "13.2 Określenie modeli\nTeraz zdefiniujmy model, które chcemy przetestować:\n\nKodlibrary(rules)\nlibrary(baguette)\n\nlinear_reg_spec &lt;- \n   linear_reg(penalty = tune(), mixture = tune()) %&gt;% \n   set_engine(\"glmnet\")\n\nnnet_spec &lt;- \n   mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %&gt;% \n   set_engine(\"nnet\", MaxNWts = 2600) %&gt;% \n   set_mode(\"regression\")\n\nmars_spec &lt;- \n   mars(prod_degree = tune()) %&gt;%  #&lt;- use GCV to choose terms\n   set_engine(\"earth\") %&gt;% \n   set_mode(\"regression\")\n\nsvm_r_spec &lt;- \n   svm_rbf(cost = tune(), rbf_sigma = tune()) %&gt;% \n   set_engine(\"kernlab\") %&gt;% \n   set_mode(\"regression\")\n\nsvm_p_spec &lt;- \n   svm_poly(cost = tune(), degree = tune()) %&gt;% \n   set_engine(\"kernlab\") %&gt;% \n   set_mode(\"regression\")\n\nknn_spec &lt;- \n   nearest_neighbor(neighbors = tune(), dist_power = tune(), weight_func = tune()) %&gt;% \n   set_engine(\"kknn\") %&gt;% \n   set_mode(\"regression\")\n\ncart_spec &lt;- \n   decision_tree(cost_complexity = tune(), min_n = tune()) %&gt;% \n   set_engine(\"rpart\") %&gt;% \n   set_mode(\"regression\")\n\nbag_cart_spec &lt;- \n   bag_tree() %&gt;% \n   set_engine(\"rpart\", times = 50L) %&gt;% \n   set_mode(\"regression\")\n\nrf_spec &lt;- \n   rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %&gt;% \n   set_engine(\"ranger\") %&gt;% \n   set_mode(\"regression\")\n\nxgb_spec &lt;- \n   boost_tree(tree_depth = tune(), learn_rate = tune(), loss_reduction = tune(), \n              min_n = tune(), sample_size = tune(), trees = tune()) %&gt;% \n   set_engine(\"xgboost\") %&gt;% \n   set_mode(\"regression\")\n\ncubist_spec &lt;- \n   cubist_rules(committees = tune(), neighbors = tune()) %&gt;% \n   set_engine(\"Cubist\") \n\n\nAutorzy w Khun i Johnson (2013) określa, że sieć neuronowa powinna mieć do 27 jednostek ukrytych w warstwie. Funkcja extract_parameter_set_dials() wyodrębnia zbiór parametrów, który modyfikujemy, aby miał prawidłowy zakres parametrów:\n\nKodnnet_param &lt;- \n   nnet_spec %&gt;% \n   extract_parameter_set_dials() %&gt;% \n   update(hidden_units = hidden_units(c(1, 27)))\n\n\nW następnym kroku stworzymy zestawy przepływu pracy:\n\nKodnormalized &lt;- \n   workflow_set(\n      preproc = list(normalized = normalized_rec), \n      models = list(SVM_radial = svm_r_spec, SVM_poly = svm_p_spec, \n                    KNN = knn_spec, neural_network = nnet_spec)\n   )\nnormalized\n\n# A workflow set/tibble: 4 × 4\n  wflow_id                  info             option    result    \n  &lt;chr&gt;                     &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 normalized_SVM_radial     &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 normalized_SVM_poly       &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 normalized_KNN            &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 normalized_neural_network &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n\nPonieważ zastosowaliśmy tylko jedna funkcję wstępnej obróbki danych (normalized_rec), to w podsumowaniu występują tylko kombinacje tego preprocesora i modeli. Kolumna wflow_id tworzona jest automatycznie, ale może być modyfikowana poprzez wywołanie mutate(). Kolumna info zawiera tibble z pewnymi identyfikatorami i obiektem przepływu pracy. Przepływ pracy może zostać wyodrębniony:\n\nKodnormalized %&gt;% extract_workflow(id = \"normalized_KNN\")\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-Nearest Neighbor Model Specification (regression)\n\nMain Arguments:\n  neighbors = tune()\n  weight_func = tune()\n  dist_power = tune()\n\nComputational engine: kknn \n\n\nKolumna option to miejsce na dowolne argumenty, których należy użyć, gdy oceniamy przepływ pracy. Na przykład, aby dodać obiekt parametrów sieci neuronowej:\n\n\n\n\n\nKodnormalized &lt;- \n   normalized %&gt;% \n   option_add(param_info = nnet_param, id = \"normalized_neural_network\")\nnormalized\n\n# A workflow set/tibble: 4 × 4\n  wflow_id                  info             option    result    \n  &lt;chr&gt;                     &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 normalized_SVM_radial     &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 normalized_SVM_poly       &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 normalized_KNN            &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 normalized_neural_network &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;\n\n\nKolumna result jest miejscem na wyjście funkcji dostrajania lub resamplingu. Dla innych modeli nieliniowych utwórzmy kolejny zestaw przepływów pracy:\n\nKodmodel_vars &lt;- \n   workflow_variables(outcomes = compressive_strength, \n                      predictors = everything())\n\nno_pre_proc &lt;- \n   workflow_set(\n      preproc = list(simple = model_vars), \n      models = list(MARS = mars_spec, CART = cart_spec, CART_bagged = bag_cart_spec,\n                    RF = rf_spec, boosting = xgb_spec, Cubist = cubist_spec)\n   )\nno_pre_proc\n\n# A workflow set/tibble: 6 × 4\n  wflow_id           info             option    result    \n  &lt;chr&gt;              &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 simple_MARS        &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 simple_CART        &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 simple_CART_bagged &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 simple_RF          &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n5 simple_boosting    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n6 simple_Cubist      &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n\nNa koniec składamy zestaw wykorzystujący warunki nieliniowe i interakcje z odpowiednimi modelami:\n\nKodwith_features &lt;- \n   workflow_set(\n      preproc = list(full_quad = poly_recipe), \n      models = list(linear_reg = linear_reg_spec, KNN = knn_spec)\n   )\n\n\nTe obiekty to tibble z dodatkową klasą workflow_set. Łączenie wierszy nie wpływa na stan zestawów, a wynik jest sam w sobie zestawem przepływów pracy:\n\nKodall_workflows &lt;- \n   bind_rows(no_pre_proc, normalized, with_features) %&gt;% \n   # Make the workflow ID's a little more simple: \n   mutate(wflow_id = gsub(\"(simple_)|(normalized_)\", \"\", wflow_id))\nall_workflows\n\n# A workflow set/tibble: 12 × 4\n   wflow_id             info             option    result    \n   &lt;chr&gt;                &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n 1 MARS                 &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 2 CART                 &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 3 CART_bagged          &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 4 RF                   &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 5 boosting             &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 6 Cubist               &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 7 SVM_radial           &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 8 SVM_poly             &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 9 KNN                  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n10 neural_network       &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;\n11 full_quad_linear_reg &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n12 full_quad_KNN        &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Poszukiwanie optymalnego modelu</span>"
    ]
  },
  {
    "objectID": "example.html#tuning-modeli",
    "href": "example.html#tuning-modeli",
    "title": "\n13  Poszukiwanie optymalnego modelu\n",
    "section": "\n13.3 Tuning modeli",
    "text": "13.3 Tuning modeli\nPrawie wszystkie modele ujęte w all_workflows zawierają parametry dostrajania. Aby ocenić ich wydajność, możemy użyć standardowych funkcji strojenia lub resamplingu (np. tune_grid() i tak dalej). Funkcja workflow_map() zastosuje tę samą funkcję do wszystkich przepływów w zestawie; domyślnie jest to tune_grid().\n\n\n\n\nDla tego przykładu, wyszukiwanie w oparciu o siatkę jest stosowane do każdego przepływu pracy, stosując jednocześnie 25 różnych kandydatów na parametry. Istnieje zestaw wspólnych opcji do wykorzystania przy każdym wykonaniu tune_grid(). Na przykład, w poniższym kodzie użyjemy tego samego próbkowania i obiektów kontrolnych dla każdego przepływu pracy, wraz z rozmiarem siatki równym 25. Funkcja workflow_map() posiada dodatkowy argument o nazwie seed, który służy do zapewnienia, że każde wykonanie tune_grid() zużywa tych samych liczb losowych.\n\nKodlibrary(doParallel)\ncl &lt;- makeCluster(4)\nregisterDoParallel(cl)\n\ngrid_ctrl &lt;-\n   control_grid(\n      save_pred = TRUE,\n      parallel_over = \"everything\",\n      save_workflow = TRUE\n   )\n\ngrid_results &lt;-\n   all_workflows %&gt;%\n   workflow_map(\n      seed = 1503,\n      resamples = concrete_folds,\n      grid = 25,\n      control = grid_ctrl\n   )\n\n\nW podsumowaniu widać, że kolumny option i result zostały zaktualizowane:\n\n\n# A workflow set/tibble: 12 × 4\n   wflow_id             info             option    result   \n   &lt;chr&gt;                &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n 1 MARS                 &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n 2 CART                 &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n 3 CART_bagged          &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;rsmp[+]&gt;\n 4 RF                   &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n 5 boosting             &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n 6 Cubist               &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n 7 SVM_radial           &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n 8 SVM_poly             &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n 9 KNN                  &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n10 neural_network       &lt;tibble [1 × 4]&gt; &lt;opts[4]&gt; &lt;tune[+]&gt;\n11 full_quad_linear_reg &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n12 full_quad_KNN        &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n\n\nKolumna option zawiera teraz wszystkie opcje, których użyliśmy w wywołaniu workflow_map(). W kolumnach result, notacje tune[+] i rsmp[+] oznaczają, że obiekt nie miał żadnych problemów w procesie optymalizacji. Wartość taka jak tune[x] pojawia się, gdy wszystkie modele z jakiegoś powodu zawiodły.\nIstnieje kilka wygodnych funkcji do badania wyników, takich jak grid_results. Funkcja rank_results() uporządkuje modele według wybranej metryki wydajności. Domyślnie używa ona pierwszej metryki w zestawie metryk (w tym przypadku RMSE). Przefiltrujmy wyniki, aby analizować tylko na RMSE:\n\nKodgrid_results %&gt;% \n   rank_results() %&gt;% \n   filter(.metric == \"rmse\") %&gt;% \n   select(model, .config, rmse = mean, rank)\n\n# A tibble: 252 × 4\n   model        .config                rmse  rank\n   &lt;chr&gt;        &lt;chr&gt;                 &lt;dbl&gt; &lt;int&gt;\n 1 boost_tree   Preprocessor1_Model04  4.25     1\n 2 boost_tree   Preprocessor1_Model06  4.29     2\n 3 boost_tree   Preprocessor1_Model13  4.31     3\n 4 boost_tree   Preprocessor1_Model14  4.39     4\n 5 boost_tree   Preprocessor1_Model16  4.46     5\n 6 boost_tree   Preprocessor1_Model03  4.47     6\n 7 boost_tree   Preprocessor1_Model15  4.48     7\n 8 boost_tree   Preprocessor1_Model05  4.55     8\n 9 boost_tree   Preprocessor1_Model20  4.71     9\n10 cubist_rules Preprocessor1_Model24  4.71    10\n# ℹ 242 more rows\n\n\nDomyślnie funkcja szereguje wszystkie zestawy kandydatów, dlatego ten sam model może pojawić się wielokrotnie na wyjściu. Opcja select_best może być użyta do uszeregowania modeli przy użyciu najlepszej kombinacji dostrajania parametrów. Metoda autoplot() tworzy wykresy rankingowy; posiada ona również argument select_best. Wykres na Rys. 13.1 wizualizuje najlepsze wyniki dla każdego modelu i jest generowany za pomocą:\n\nKodautoplot(\n   grid_results,\n   rank_metric = \"rmse\",  # &lt;- how to order models\n   metric = \"rmse\",       # &lt;- which metric to visualize\n   select_best = TRUE     # &lt;- one point per workflow\n) +\n   geom_text(aes(y = mean - 1/2, label = wflow_id), angle = 90, hjust = 1) +\n   lims(y = c(3, 9.5)) +\n   theme(legend.position = \"none\")\n\n\n\n\n\n\nRys. 13.1: Oszacowany RMSE (i przybliżone przedziały ufności) dla najlepszej konfiguracji modelu w każdym przepływie pracy\n\n\n\n\nW przypadku, gdy chcesz zobaczyć wyniki dostrajania parametrów dla konkretnego modelu, tak jak na Rys. 13.2, argument id może przyjąć pojedynczą wartość z kolumny wflow_id dla którego modelu ma być wykreślony:\n\nKodautoplot(grid_results, id = \"Cubist\", metric = \"rmse\")\n\n\n\n\n\n\nRys. 13.2: Wizualizacja RMSE w kontekście konfiguracji hiperparametrów modelu Cubist\n\n\n\n\nFunkcje collect_metrics() i collect_predictions() również pozwalają przejrzenie wyników optymalizacji.\nW powyższym procesie optymalizacji przeuczono 12600 modeli, co zajęło około 2 godzin przy wykorzystaniu 4 rdzeni procesora. Pokazuje to, że zagadnienie tuningu nawet kilku kandydackich modeli zajmuje sporo czasu.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Poszukiwanie optymalnego modelu</span>"
    ]
  },
  {
    "objectID": "example.html#efektywna-filtracja-modeli",
    "href": "example.html#efektywna-filtracja-modeli",
    "title": "\n13  Poszukiwanie optymalnego modelu\n",
    "section": "\n13.4 Efektywna filtracja modeli",
    "text": "13.4 Efektywna filtracja modeli\nJedną z metod efektywnego przesiewania dużego zbioru modeli jest zastosowanie podejścia wyścigowego opisanego wcześniej. Mając zestaw przepływów pracy, możemy użyć funkcji workflow_map() do podejścia wyścigowego.\n\n\n\n\n\nKodrace_ctrl &lt;-\n   control_race(\n      save_pred = TRUE,\n      parallel_over = \"everything\",\n      save_workflow = TRUE\n   )\n\nrace_results &lt;-\n   all_workflows %&gt;%\n   workflow_map(\n      \"tune_race_anova\",\n      seed = 1503,\n      resamples = concrete_folds,\n      grid = 25,\n      control = race_ctrl\n   )\n\n\nNowy obiekt wygląda bardzo podobnie, choć elementy kolumny wyników wykazują wartość race[+], co wskazuje na inny typ obiektu:\n\nKodrace_results &lt;- readRDS(\"models/race_results.rds\")\nrace_results\n\n# A workflow set/tibble: 12 × 4\n   wflow_id             info             option    result   \n   &lt;chr&gt;                &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n 1 MARS                 &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n 2 CART                 &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n 3 CART_bagged          &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;rsmp[+]&gt;\n 4 RF                   &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n 5 boosting             &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n 6 Cubist               &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n 7 SVM_radial           &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n 8 SVM_poly             &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n 9 KNN                  &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n10 neural_network       &lt;tibble [1 × 4]&gt; &lt;opts[4]&gt; &lt;race[+]&gt;\n11 full_quad_linear_reg &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n12 full_quad_KNN        &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n\n\n\nKodautoplot(\n   race_results,\n   rank_metric = \"rmse\",  \n   metric = \"rmse\",       \n   select_best = TRUE    \n) +\n   geom_text(aes(y = mean - 1/2, label = wflow_id), angle = 90, hjust = 1) +\n   lims(y = c(3.0, 9.5)) +\n   theme(legend.position = \"none\")\n\n\n\n\n\n\nRys. 13.3: Oszacowane RMSE (i przybliżone przedziały ufności) dla najlepszej konfiguracji modelu w poszukiwaniu za pomocą metody wyścigowej.\n\n\n\n\nPodejście wyścigowe oszacowało łącznie 1050 modeli, 8,33% z pełnego zestawu 12600 modeli w pełnej siatce. W rezultacie podejście wyścigowe trwało nieco ponad 17 min., więc było 7-krotnie szybsze1.\n1 Wartości te będą zależały od sprzętu na jakim wykonuje się obliczeniaNa ile zbliżone wyniki otrzymaliśmy stosując obie metody tuningu?\n\nKodmatched_results &lt;- \n   rank_results(race_results, select_best = TRUE) %&gt;% \n   select(wflow_id, .metric, race = mean, config_race = .config) %&gt;% \n   inner_join(\n      rank_results(grid_results, select_best = TRUE) %&gt;% \n         select(wflow_id, .metric, complete = mean, \n                config_complete = .config, model),\n      by = c(\"wflow_id\", \".metric\"),\n   ) %&gt;%  \n   filter(.metric == \"rmse\")\n\nlibrary(ggrepel)\n\nmatched_results %&gt;% \n   ggplot(aes(x = complete, y = race)) + \n   geom_abline(lty = 3) + \n   geom_point() + \n   geom_text_repel(aes(label = model)) +\n   coord_obs_pred() + \n   labs(x = \"Complete Grid RMSE\", y = \"Racing RMSE\") \n\n\n\n\n\n\n\nPodczas gdy podejście wyścigowe wybrało te same parametry kandydata co kompletna siatka tylko dla 41,67% modeli, metryki wydajności modeli wybranych przez wyścig były prawie równe. Korelacja wartości RMSE wyniosła 0,968, a korelacja rangowa 0,951. Wskazuje to, że w obrębie modelu istniało wiele kombinacji dostrajania parametrów, które dawały niemal identyczne wyniki.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Poszukiwanie optymalnego modelu</span>"
    ]
  },
  {
    "objectID": "example.html#finalizacja-modelu",
    "href": "example.html#finalizacja-modelu",
    "title": "\n13  Poszukiwanie optymalnego modelu\n",
    "section": "\n13.5 Finalizacja modelu",
    "text": "13.5 Finalizacja modelu\nPodobnie do tego, co pokazaliśmy w poprzednich rozdziałach, proces wyboru ostatecznego modelu i dopasowania go na zbiorze treningowym jest prosty. Pierwszym krokiem jest wybranie zbioru treningowego do sfinalizowania. Ponieważ model boosted tree działał dobrze, wyodrębnimy go ze zbioru, zaktualizujemy parametry o numerycznie najlepsze ustawienia i dopasujemy do zbioru treningowego. W przypadku gdy mamy wątpliwości dotyczące siatki hiperparametrów dobranych podczas filtrowania modeli, np. że pomija ona ważne kombinacje, możemy zastosować do wybranego modelu metody finetune przedstawione w poprzednim rozdziale.\n\nKodbest_results &lt;- \n   race_results %&gt;% \n   extract_workflow_set_result(\"boosting\") %&gt;% \n   select_best(metric = \"rmse\")\nbest_results\n\n# A tibble: 1 × 7\n  trees min_n tree_depth learn_rate loss_reduction sample_size .config          \n  &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1  1957     8          7     0.0756    0.000000145       0.679 Preprocessor1_Mo…\n\nKodboosting_test_results &lt;- \n   race_results %&gt;% \n   extract_workflow(\"boosting\") %&gt;% \n   finalize_workflow(best_results) %&gt;% \n   last_fit(split = concrete_split)\n\n\nWyniki metryki dla zbioru testowego oraz wizualizację predykcji możemy zobaczyć stosując.\n\nKodcollect_metrics(boosting_test_results)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3.41  Preprocessor1_Model1\n2 rsq     standard       0.954 Preprocessor1_Model1\n\n\n\nKodboosting_test_results %&gt;% \n   collect_predictions() %&gt;% \n   ggplot(aes(x = compressive_strength, y = .pred)) + \n   geom_abline(color = \"gray50\", lty = 2) + \n   geom_point(alpha = 0.5) + \n   coord_obs_pred() + \n   labs(x = \"observed\", y = \"predicted\")\n\n\n\n\n\n\nRys. 13.4: Porównanie wartości obserwowanych i przewidywanych z modelu\n\n\n\n\n\n\n\n\n\n\n\n\nKhun, M., i K. Johnson. 2013. Applied Predictive Modeling. New York: Springer.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Poszukiwanie optymalnego modelu</span>"
    ]
  },
  {
    "objectID": "dimensionality.html",
    "href": "dimensionality.html",
    "title": "\n14  Redukcja wymiarowości\n",
    "section": "",
    "text": "14.1 Opis danych\nRedukcja wymiarowości (ang. dimensionality reduction) jest przekształceniem zbioru danych z przestrzeni wielowymiarowej w przestrzeń niskowymiarową i może być dobrym wyborem, gdy podejrzewamy, że jest “za dużo” zmiennych. Nadmiar zmiennych, zwykle predyktorów, może być problemem, ponieważ trudno jest zrozumieć lub wizualizować dane w wyższych wymiarach.\nRedukcja wymiarowości może być stosowana zarówno w inżynierii cech, jak i w eksploracyjnej analizie danych. Na przykład, w wielowymiarowych eksperymentach biologicznych, jednym z pierwszych zadań, przed jakimkolwiek modelowaniem, jest określenie, czy istnieją jakiekolwiek niepożądane trendy w danych (np. efekty niezwiązane z interesującym nas zagadnieniem, takie jak różnice między laboratoriami). Eksploracja danych jest trudna, gdy istnieją setki tysięcy wymiarów, a redukcja wymiarowości może być pomocą w analizie danych.\nInną potencjalną konsekwencją posiadania wielu predyktorów jest model nadmiarową liczbą predyktorów. Najprostszym przykładem jest regresja liniowa, gdzie liczba predyktorów powinna być mniejsza niż liczba obserwacji użytych do dopasowania modelu. Innym problemem jest współliniowość, gdzie korelacje między predyktorami mogą negatywnie wpływać na operacje matematyczne używane do oszacowania modelu. Jeśli istnieje bardzo duża liczba predyktorów, jest mało prawdopodobne, że istnieje taka sama liczba rzeczywistych efektów leżących u podstaw. Predyktory mogą mierzyć ten sam ukryty efekt (efekty), a zatem takie predyktory będą wysoko skorelowane. Wiele technik redukcji wymiarowości sprawdza się w tej sytuacji. W rzeczywistości większość z nich może być skuteczna tylko wtedy, gdy istnieją takie relacje między predyktorami, które można wykorzystać.\nAnaliza składowych głównych (PCA) jest jedną z najprostszych metod redukcji liczby zmiennych w zbiorze danych, ponieważ opiera się na metodach liniowych i jest nienadzorowana. W przypadku wielowymiarowego problemu klasyfikacji, początkowy wykres głównych komponentów PCA może pokazać wyraźny podział między klasami. Jeśli tak jest, to można bezpiecznie założyć, że klasyfikator liniowy może się sprawdzić w takim zadaniu. Jednak nie jest prawdą, że brak separacji nie oznacza, że problem jest nie do pokonania.\nMetody redukcji wymiarowości omawiane w tym rozdziale nie są na ogół metodami selekcji cech. Metody takie jak PCA reprezentują oryginalne predyktory za pomocą mniejszego podzbioru nowych cech. Wszystkie oryginalne predyktory są wymagane do obliczenia tych nowych cech. Wyjątkiem są metody rzadkie, które mają zdolność do całkowitego usunięcia wpływu predyktorów podczas tworzenia nowych cech.\nDo celów ilustracji, jak używać redukcji wymiarowości z przepisami wykorzystamy przykładowy zestaw danych. Koklu i Ozkan (2020) opublikowali zestaw danych dotyczących wizualnych cech suszonej fasoli i opisali metody określania odmian suszonej fasoli na obrazie. Chociaż wymiarowość tych danych nie jest bardzo duża w porównaniu z wieloma problemami modelowania w świecie rzeczywistym, zapewnia ładny przykład roboczy, aby zademonstrować, jak zmniejszyć liczbę cech. W swojej pracy napisali:\nKażdy obraz zawiera wiele ziaren. Proces określania, które piksele odpowiadają konkretnej fasoli, nazywany jest segmentacją obrazu. Te piksele mogą być analizowane w celu uzyskania cech dla każdej fasoli, takich jak kolor i morfologia (tj. kształt). Cechy te są następnie wykorzystywane do modelowania wyniku (odmiany fasoli), ponieważ różne odmiany fasoli wyglądają inaczej. Dane treningowe pochodzą z zestawu ręcznie oznakowanych obrazów, a ten zestaw danych jest używany do tworzenia modelu predykcyjnego, który może rozróżnić siedem odmian fasoli: Cali, Horoz, Dermason, Seker, Bombay, Barbunya i Sira. Stworzenie skutecznego modelu może pomóc producentom w ilościowym określeniu jednorodności partii fasoli.\nIstnieje wiele metod kwantyfikacji kształtów obiektów (Mingqiang, Kidiyo, i Joseph 2008). Wiele z nich dotyczy granic lub regionów interesującego nas obiektu. Przykładowe cechy obejmują:\nW danych dotyczących fasoli obliczono 16 cech morfologicznych: powierzchnię, obwód, długość osi głównej, długość osi małej, współczynnik kształtu, ekscentryczność, powierzchnię wypukłą, średnicę równoważną, rozległość, zwartość, krągłość, zwięzłość, współczynnik kształtu 1, współczynnik kształtu 2, współczynnik kształtu 3 i współczynnik kształtu 4.\nKodlibrary(tidymodels)\ntidymodels_prefer()\nlibrary(tidyverse)\nlibrary(beans)\nDla naszych analiz zaczynamy od podziału zbioru za pomocą initial_split(). Pozostałe dane są dzielone na zbiory treningowe i walidacyjne:\nKodset.seed(1601)\nbean_split &lt;- initial_split(beans, strata = class, prop = 3/4)\n\nbean_train &lt;- training(bean_split)\nbean_test  &lt;- testing(bean_split)\n\nset.seed(1602)\nbean_val &lt;- validation_split(bean_train, strata = class, prop = 4/5)\nbean_val$splits[[1]]\n\n&lt;Training/Validation/Total&gt;\n&lt;8163/2043/10206&gt;\nAby wizualnie ocenić, jak dobrze działają różne metody, możemy oszacować metody na zbiorze treningowym (n = 8163 ziaren) i wyświetlić wyniki przy użyciu zbioru walidacyjnego (n = 2043).\nPrzed rozpoczęciem jakiejkolwiek redukcji wymiarowości możemy poświęcić trochę czasu na zbadanie naszych danych. Ponieważ wiemy, że wiele z tych cech kształtu prawdopodobnie mierzy podobne koncepcje, przyjrzyjmy się strukturze korelacyjnej danych na Rys. 14.1.\nKodlibrary(corrplot)\ntmwr_cols &lt;- colorRampPalette(c(\"#91CBD765\", \"#CA225E\"))\nbean_train %&gt;% \n  select(-class) %&gt;% \n  cor() %&gt;% \n  corrplot(col = tmwr_cols(200), tl.col = \"black\", method = \"ellipse\")\n\n\n\n\n\n\nRys. 14.1: Macierz korelacji\nWiele z tych predyktorów jest silnie skorelowanych, jak np. powierzchnia i obwód lub współczynniki kształtu 2 i 3. Chociaż nie poświęcamy temu czasu tutaj, ważne jest również, aby zobaczyć, czy ta struktura korelacji znacząco zmienia się w różnych kategoriach wyników. Może to pomóc w stworzeniu lepszych modeli.\nZacznijmy od podstawowego przepisu wstępnego przetwarzania danych, który często stosujemy przed jakimikolwiek krokami redukcji wymiarowości. Kilka predyktorów to współczynniki, a więc prawdopodobnie będą miały skośne rozkłady. Takie rozkłady mogą siać spustoszenie w obliczeniach wariancji (takich jak te używane w PCA). Pakiet bestNormalize posiada krok, który może wymusić symetryczny rozkład predyktorów. Użyjemy tego, aby złagodzić problem skośnych rozkładów:\nKodlibrary(bestNormalize)\nbean_rec &lt;-\n  # Use the training data from the bean_val split object\n  recipe(class ~ ., data = analysis(bean_val$splits[[1]])) %&gt;%\n  step_zv(all_numeric_predictors()) %&gt;%\n  step_orderNorm(all_numeric_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors())\nPrzepis ten zostanie rozszerzony o dodatkowe kroki dla analiz redukcji wymiarowości. Zanim to zrobimy, przejdźmy do tego, jak receptura może być używana poza przepływem pracy.\nPrzepływ pracy zawierający recepturę wykorzystuje fit() do estymacji receptury i modelu, a następnie predict() do przetwarzania danych i tworzenia przewidywań modelu. W pakiecie recipes znajdują się analogiczne funkcje, które mogą być użyte do tego samego celu:\nRysunek 16.3 podsumowuje to.\nKodbean_rec_trained &lt;- prep(bean_rec)\nbean_rec_trained\nZauważ, że kroki zostały wytrenowane i że selektory nie są już ogólne (tj. all_numeric_predictors()); teraz pokazują rzeczywiste kolumny, które zostały wybrane. Również, prep(bean_rec) nie wymaga argumentu training. Możesz przekazać dowolne dane do tego argumentu, ale pominięcie go oznacza, że użyte zostaną oryginalne dane z wywołania recipe(). W naszym przypadku były to dane ze zbioru treningowego.\nJednym z ważnych argumentów funkcji prep() jest retain. Kiedy retain = TRUE (domyślnie), szacunkowa wersja zbioru treningowego jest przechowywana wewnątrz receptury. Ten zestaw danych został wstępnie przetworzony przy użyciu wszystkich kroków wymienionych w recepturze. Ponieważ funkcja prep() musi wykonywać recepturę w trakcie jej wykonywania, korzystne może być zachowanie tej wersji zbioru treningowego, tak że jeśli ten zbiór danych ma być użyty później, można uniknąć zbędnych obliczeń. Jednakże, jeśli zestaw treningowy jest duży, może być problematyczne przechowywanie tak dużej ilości danych w pamięci. Użyj wówczas retain = FALSE, aby tego uniknąć.\nPo dodaniu nowych kroków do oszacowanej receptury, ponowne zastosowanie prep() oszacuje tylko niewykształcone kroki. Przyda się to, gdy będziemy próbować różnych metod ekstrakcji cech. Inną opcją, która może pomóc zrozumieć, co dzieje się w analizie, jest log_changes:\nKodshow_variables &lt;- \n  bean_rec %&gt;% \n  prep(log_changes = TRUE)\n\nstep_zv (zv_6JtxV): same number of columns\n\nstep_orderNorm (orderNorm_4r8al): same number of columns\n\nstep_normalize (normalize_x6oqH): same number of columns\nNa przykład, próbki zbioru walidacyjnego mogą być przetwarzane następująco:\nKodbean_validation &lt;- bean_val$splits %&gt;% pluck(1) %&gt;% assessment()\nbean_val_processed &lt;- bake(bean_rec_trained, new_data = bean_validation)\nRys. 14.3 przedstawia histogramy predyktora powierzchni przed i po przygotowaniu receptury.\nKodlibrary(patchwork)\np1 &lt;- \n  bean_validation %&gt;% \n  ggplot(aes(x = area)) + \n  geom_histogram(bins = 30, color = \"white\", fill = \"blue\", alpha = 1/3) + \n  ggtitle(\"Original validation set data\")\n\np2 &lt;- \n  bean_val_processed %&gt;% \n  ggplot(aes(x = area)) + \n  geom_histogram(bins = 30, color = \"white\", fill = \"red\", alpha = 1/3) + \n  ggtitle(\"Processed validation set data\")\n\np1 + p2\n\n\n\n\n\n\nRys. 14.3: Rozkłady przed i po transformacji\nWarto tutaj zwrócić uwagę na dwa ważne aspekty bake(). Po pierwsze, jak wcześniej wspomniano, użycie prep(recipe, retain = TRUE) zachowuje istniejącą przetworzoną wersję zbioru treningowego w recepturze. Dzięki temu użytkownik może użyć bake(recipe, new_data = NULL), która zwraca ten zestaw danych bez dalszych obliczeń. Na przykład:\nKodbake(bean_rec_trained, new_data = NULL) %&gt;% nrow()\n\n[1] 8163\n\nKodbean_val$splits %&gt;% pluck(1) %&gt;% analysis() %&gt;% nrow()\n\n[1] 8163\nJeśli zestaw treningowy nie jest patologicznie duży, użycie retain może zaoszczędzić dużo czasu obliczeniowego. Po drugie, w wywołaniu można użyć dodatkowych selektorów, aby określić, które kolumny mają zostać zwrócone. Domyślnym selektorem jest everything(), ale można dokonać bardziej szczegółowych wyborów.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Redukcja wymiarowości</span>"
    ]
  },
  {
    "objectID": "dimensionality.html#opis-danych",
    "href": "dimensionality.html#opis-danych",
    "title": "\n14  Redukcja wymiarowości\n",
    "section": "",
    "text": "Podstawowym celem niniejszej pracy jest dostarczenie metody uzyskiwania jednolitych odmian nasion z produkcji roślinnej, która ma postać populacji, więc nasiona nie są certyfikowane jako jedyna odmiana. W związku z tym opracowano system wizji komputerowej do rozróżniania siedmiu różnych zarejestrowanych odmian suchej fasoli o podobnych cechach w celu uzyskania jednolitej klasyfikacji nasion. Dla modelu klasyfikacji wykonano obrazy 13 611 ziaren 7 różnych zarejestrowanych suchych odmian fasoli za pomocą kamery o wysokiej rozdzielczości.\n\n\n\n\n\n\n\n\nObszar (lub rozmiar) może być oszacowany przy użyciu liczby pikseli w obiekcie lub rozmiaru wypukłego kadłuba wokół obiektu.\nObwód możemy zmierzyć używając liczby pikseli w granicy, jak również prostokąta ograniczającego (najmniejszy prostokąt zamykający obiekt).\nOś główna określa ilościowo najdłuższą linię łączącą najbardziej skrajne części obiektu. Mała oś jest prostopadła do osi głównej.\nZwartość obiektu możemy mierzyć za pomocą stosunku pola powierzchni obiektu do pola powierzchni koła o tym samym obwodzie. Na przykład symbole \\(\\bullet\\) i \\(\\times\\) mają bardzo różną zwartość.\nIstnieją również różne miary tego, jak bardzo obiekt jest podłużny. Na przykład statystyka ekscentryczności to stosunek osi głównej i małej. Istnieją również powiązane szacunki dla okrągłości i wypukłości.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOstrzeżenie\n\n\n\nPamiętaj, że podczas wywoływania funkcji recipe() kroki nie są w żaden sposób szacowane ani wykonywane.\n\n\n\n\n\n\n\n\n\n\nprep(recipe, training) dopasowuje przepis do zbioru treningowego.\n\nbake(recipe, new_data) stosuje operacje receptury do new_data.\n\n\n\n\n\n\n\nRys. 14.2: Zasada działania poszczególnych czasowników\n\n\n\n\n\n\n\n\n\n\n\n\n\nWskazówka\n\n\n\nUżywanie bake() z recepturą jest bardzo podobne do używania predict() z modelem; operacje oszacowane na zbiorze treningowym są stosowane do dowolnych danych, jak dane testowe lub nowe dane w czasie predykcji.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Redukcja wymiarowości</span>"
    ]
  },
  {
    "objectID": "dimensionality.html#ekstrakcja-cech",
    "href": "dimensionality.html#ekstrakcja-cech",
    "title": "\n14  Redukcja wymiarowości\n",
    "section": "\n14.2 Ekstrakcja cech",
    "text": "14.2 Ekstrakcja cech\nPonieważ receptury są podstawową opcją w tidymodels do redukcji wymiarowości, napiszmy funkcję, która oszacuje transformację i wykreśli wynikowe dane w postaci macierzy wykresów za pośrednictwem pakietu ggforce:\n\n\n\n\n\nKodlibrary(ggforce)\nplot_validation_results &lt;- function(recipe, dat = assessment(bean_val$splits[[1]])) {\n  recipe %&gt;%\n    # Estimate any additional steps\n    prep() %&gt;%\n    # Process the data (the validation set by default)\n    bake(new_data = dat) %&gt;%\n    # Create the scatterplot matrix\n    ggplot(aes(x = .panel_x, y = .panel_y, color = class, fill = class)) +\n    geom_point(alpha = 0.4, size = 0.5) +\n    geom_autodensity(alpha = .3) +\n    facet_matrix(vars(-class), layer.diag = 2) + \n    scale_color_brewer(palette = \"Dark2\") + \n    scale_fill_brewer(palette = \"Dark2\")\n}\n\n\nFunkcji tej użyjemy wielokrotnie do wizualizacji wyników po zastosowaniu różnych metod ekstrakcji cech.\n\n14.2.1 PCA\nPCA jest metodą bez nadzoru, która wykorzystuje liniowe kombinacje predyktorów do określenia nowych cech. Cechy te starają się uwzględnić jak najwięcej zmienności w oryginalnych danych. Dodajemy step_pca() do oryginalnego przepisu i wykorzystujemy naszą funkcję do wizualizacji wyników na zbiorze walidacyjnym na Rys. 14.4 za pomocą:\n\nKodbean_rec_trained %&gt;%\n  step_pca(all_numeric_predictors(), num_comp = 4) %&gt;%\n  plot_validation_results() + \n  ggtitle(\"Principal Component Analysis\")\n\n\n\n\n\n\nRys. 14.4: Wynik działania PCA dla pierwszych czterech składowych głównych\n\n\n\n\nWidzimy, że pierwsze dwie składowe PC1 i PC2, zwłaszcza gdy są używane razem, skutecznie rozróżniają lub oddzielają klasy. To może nas skłonić do oczekiwania, że ogólny problem klasyfikacji tych ziaren nie będzie szczególnie trudny.\nPrzypomnijmy, że PCA jest metodą bez nadzoru. Dla tych danych okazuje się, że składowe PCA, które wyjaśniają największe różnice w predyktorach, są również predyktorami klas. Jakie cechy wpływają na wydajność? Pakiet learntidymodels posiada funkcje, które mogą pomóc w wizualizacji najważniejszych cech dla każdego komponentu. Będziemy potrzebowali przygotowanego przepisu; krok PCA jest dodany w poniższym kodzie wraz z wywołaniem prep():\n\nKodlibrary(learntidymodels)\nbean_rec_trained %&gt;%\n  step_pca(all_numeric_predictors(), num_comp = 4) %&gt;% \n  prep() %&gt;% \n  plot_top_loadings(component_number &lt;= 4, n = 5) + \n  scale_fill_brewer(palette = \"Paired\") +\n  ggtitle(\"Principal Component Analysis\")\n\n\n\n\n\n\nRys. 14.5: Ładunki predyktorów w transformacji PCA\n\n\n\n\nNajwyższe ładunki są w większości związane z grupą skorelowanych predyktorów pokazanych w lewej górnej części poprzedniego wykresu korelacji: obwód, powierzchnia, długość osi głównej i powierzchnia wypukła. Wszystkie one są związane z wielkością fasoli. Miary wydłużenia wydają się dominować w drugim komponencie PCA.\n\n14.2.2 PLS\nPartial Least Squares jest nadzorowaną wersją PCA. Próbuje znaleźć składowe, które jednocześnie maksymalizują zmienność predyktorów, a jednocześnie maksymalizują związek między tymi składowymi a wynikiem. Rys. 14.6 przedstawia wyniki tej nieco zmodyfikowanej wersji kodu PCA:\n\nKodbean_rec_trained %&gt;%\n  step_pls(all_numeric_predictors(), outcome = \"class\", num_comp = 4) %&gt;%\n  plot_validation_results() + \n  ggtitle(\"Partial Least Squares\")\n\n\n\n\n\n\nRys. 14.6: Oceny składowych PLS dla zbioru walidacyjnego fasoli, pokolorowane według klas\n\n\n\n\nPierwsze dwie składowe PLS wykreślone na Rys. 14.6 są niemal identyczne z pierwszymi dwiema składowymi PCA! Pozostałe składowe różnią się nieco od PCA. Rys. 14.7 wizualizuje ładunki, czyli najwyższe cechy dla każdej składowej.\n\nKodbean_rec_trained %&gt;%\n  step_pls(all_numeric_predictors(), outcome = \"class\", num_comp = 4) %&gt;%\n  prep() %&gt;% \n  plot_top_loadings(component_number &lt;= 4, n = 5, type = \"pls\") + \n  scale_fill_brewer(palette = \"Paired\") +\n  ggtitle(\"Partial Least Squares\")\n\n\n\n\n\n\nRys. 14.7: Ładunki predyktorów w transformacji PLS\n\n\n\n\n\n14.2.3 ICA\nMetoda składowych niezależnych różni się nieco od PCA tym, że znajduje składowe, które są tak statystycznie niezależne od siebie, jak to tylko możliwe (w przeciwieństwie do bycia nieskorelowanym). Można o tym myśleć jako o rozdzielaniu informacji zamiast kompresji informacji jak w przypadku PCA. Użyjmy funkcji step_ica() do wykonania dekompozycji (patrz Rys. 14.8):\n\nKodbean_rec_trained %&gt;%\n  step_ica(all_numeric_predictors(), num_comp = 4) %&gt;%\n  plot_validation_results() + \n  ggtitle(\"Independent Component Analysis\")\n\n\n\n\n\n\nRys. 14.8: Oceny komponentów ICA dla zbioru walidacyjnego fasoli, pokolorowane według klas\n\n\n\n\nAnaliza wyników nie wskazuje na wyraźne różnice między klasami w pierwszych kilku komponentach ICA. Te niezależne (lub tak niezależne, jak to możliwe) komponenty nie separują typów fasoli.\n\n14.2.4 UMAP\nUniform manifold approximation and projection jest podobna do innej znanej metody t-SNE służącej do nieliniowej redukcji wymiarów. W oryginalnej przestrzeni wielowymiarowej UMAP wykorzystuje opartą na odległości metodę najbliższych sąsiadów, aby znaleźć lokalne obszary danych, w których punkty z dużym prawdopodobieństwem są powiązane. Relacje między punktami danych są zapisywane jako model grafu skierowanego, gdzie większość punktów nie jest połączona. Stąd UMAP tłumaczy punkty w grafie na przestrzeń o zmniejszonym wymiarze. Aby to zrobić, algorytm posiada proces optymalizacji, który wykorzystuje entropię krzyżową do mapowania punktów danych do mniejszego zestawu cech, tak aby graf był dobrze przybliżony.\nPakiet embed zawiera funkcję krokową dla tej metody, zwizualizowaną na Rys. 14.9.\n\nKodlibrary(embed)\n\n\n\nKodbean_rec_trained %&gt;%\n  step_umap(all_numeric_predictors(), num_comp = 4) %&gt;%\n  plot_validation_results() +\n  ggtitle(\"UMAP\")\n\n\n\n\n\n\nRys. 14.9: Oceny komponentów UMAP dla zestawu walidacyjnego fasoli, pokolorowane według klas\n\n\n\n\nWidać wyraźne przestrzenie pomiędzy skupieniami, choć te dalej mogą być heterogeniczne.\nIstnieje też odmiana UMAP nadzorowana, która zamiast używać podobieństwa do określenia skupień w wielowymiarowej przestrzeni, używa etykiet.\n\nKodbean_rec_trained %&gt;%\n  step_umap(all_numeric_predictors(), outcome = \"class\", num_comp = 4) %&gt;%\n  plot_validation_results() +\n  ggtitle(\"UMAP (supervised)\")\n\n\n\n\n\n\nRys. 14.10: Oceny komponentów UMAP (werjsa nadzorowana) dla zestawu walidacyjnego fasoli, pokolorowane według klas\n\n\n\n\nUMAP jest wydajną metodą redukcji przestrzeni cech. Jednak może być bardzo wrażliwa na dostrajanie parametrów (np. liczba sąsiadów). Z tego powodu pomocne byłoby eksperymentowanie z kilkoma parametrami, aby ocenić, jak wiarygodne są wyniki dla tych danych.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Redukcja wymiarowości</span>"
    ]
  },
  {
    "objectID": "dimensionality.html#modelowanie-ze-wstępną-ekstrakcją-cech",
    "href": "dimensionality.html#modelowanie-ze-wstępną-ekstrakcją-cech",
    "title": "\n14  Redukcja wymiarowości\n",
    "section": "\n14.3 Modelowanie ze wstępną ekstrakcją cech",
    "text": "14.3 Modelowanie ze wstępną ekstrakcją cech\nZarówno metoda PLS jak i UMAP są warte zbadania w połączeniu z różnymi modelami. Zbadajmy wiele różnych modeli z tymi technikami redukcji wymiarowości (wraz z brakiem jakiejkolwiek transformacji): jednowarstwowa sieć neuronowa, bagged trees, elastyczna analiza dyskryminacyjna (FDA), naiwny Bayes i regularyzowana analiza dyskryminacyjna (RDA).\n\n\n\n\nStworzymy serię specyfikacji modeli, a następnie użyjemy zestawu przepływów pracy do dostrojenia modeli w poniższym kodzie. Zauważ, że parametry modelu są dostrajane w połączeniu z parametrami receptury (np. rozmiar zredukowanego wymiaru, parametry UMAP).\n\nKodlibrary(baguette)\nlibrary(discrim)\n\nmlp_spec &lt;-\n  mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %&gt;%\n  set_engine('nnet') %&gt;%\n  set_mode('classification')\n\nbagging_spec &lt;-\n  bag_tree() %&gt;%\n  set_engine('rpart') %&gt;%\n  set_mode('classification')\n\nfda_spec &lt;-\n  discrim_flexible(\n    prod_degree = tune()\n  ) %&gt;%\n  set_engine('earth')\n\nrda_spec &lt;-\n  discrim_regularized(frac_common_cov = tune(), frac_identity = tune()) %&gt;%\n  set_engine('klaR')\n\nbayes_spec &lt;-\n  naive_Bayes() %&gt;%\n  set_engine('klaR')\n\n\nPotrzebujemy również receptur dla metod redukcji wymiarowości, które będziemy testować. Zacznijmy od bazowego przepisu bean_rec, a następnie rozszerzmy go o różne kroki redukcji wymiarowości:\n\nKodbean_rec &lt;-\n  recipe(class ~ ., data = bean_train) %&gt;%\n  step_zv(all_numeric_predictors()) %&gt;%\n  step_orderNorm(all_numeric_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\npls_rec &lt;- \n  bean_rec %&gt;% \n  step_pls(all_numeric_predictors(), outcome = \"class\", num_comp = tune())\n\numap_rec &lt;-\n  bean_rec %&gt;%\n  step_umap(\n    all_numeric_predictors(),\n    outcome = \"class\",\n    num_comp = tune(),\n    neighbors = tune(),\n    min_dist = tune()\n  )\n\n\nPo raz kolejny pakiet workflowsets bierze reguły wstępnego przetwarzani oraz modele i krzyżuje je. Opcja kontrolna parallel_over jest ustawiona tak, aby przetwarzanie równoległe mogło działać jednocześnie w różnych kombinacjach dostrajania parametrów. Funkcja workflow_map() stosuje przeszukiwanie siatki w celu optymalizacji parametrów modelu/przetwarzania wstępnego (jeśli istnieją) w 10 kombinacjach parametrów. Obszar pod krzywą ROC jest szacowany na zbiorze walidacyjnym.\n\nKodctrl &lt;- control_grid(parallel_over = \"resamples\")\n\nbean_res &lt;- \n  workflow_set(\n    preproc = list(basic = class ~., pls = pls_rec, umap = umap_rec), \n    models = list(bayes = bayes_spec, fda = fda_spec,\n                  rda = rda_spec, bag = bagging_spec,\n                  mlp = mlp_spec)\n  ) %&gt;% \n  workflow_map(\n    verbose = TRUE,\n    seed = 1603,\n    resamples = bean_val,\n    grid = 10,\n    metrics = metric_set(roc_auc),\n    control = ctrl\n  )\n\n\n\nKodbean_res &lt;- readRDS(\"models/bean_res.rds\")\nautoplot(\nbean_res,\nrank_metric = \"roc_auc\",  # &lt;- how to order models\nmetric = \"roc_auc\",       # &lt;- which metric to visualize\nselect_best = TRUE     # &lt;- one point per workflow\n) +\ngeom_text(aes(y = mean - 0.02, label = wflow_id), angle = 90, hjust = 0.1) +\nlims(y = c(0.9, 1))\n\n\n\n\n\n\nRys. 14.11: Obszar pod krzywą ROC ze zbioru walidacyjnego\n\n\n\n\nMożemy uszeregować modele według ich oszacowań obszaru pod krzywą ROC w zbiorze walidacyjnym:\n\nKodrankings &lt;- \n  rank_results(bean_res, select_best = TRUE) %&gt;% \n  mutate(method = map_chr(wflow_id, ~ str_split(.x, \"_\", simplify = TRUE)[1])) \n\ntidymodels_prefer()\nfilter(rankings, rank &lt;= 5) %&gt;% dplyr::select(rank, mean, model, method)\n\n# A tibble: 5 × 4\n   rank  mean model               method\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt; \n1     1 0.995 mlp                 pls   \n2     2 0.995 discrim_regularized pls   \n3     3 0.994 naive_Bayes         pls   \n4     4 0.994 mlp                 basic \n5     5 0.994 discrim_flexible    basic \n\n\nZ tych wyników jasno wynika, że większość modeli daje bardzo dobre wyniki; niewiele jest tu złych wyborów. Dla demonstracji użyjemy modelu RDA z cechami PLS jako modelu końcowego.\n\nKodrda_res &lt;- \n  bean_res %&gt;% \n  extract_workflow(\"pls_rda\") %&gt;% \n  finalize_workflow(\n    bean_res %&gt;% \n      extract_workflow_set_result(\"pls_rda\") %&gt;% \n      select_best(metric = \"roc_auc\")\n  ) %&gt;% \n  last_fit(split = bean_split, metrics = metric_set(roc_auc))\n\nrda_wflow_fit &lt;- extract_workflow(rda_res)\n\ncollect_metrics(rda_res)\n\n# A tibble: 1 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 roc_auc hand_till      0.995 Preprocessor1_Model1\n\n\n\n\n\n\nKoklu, Murat, i Ilker Ali Ozkan. 2020. „Multiclass Classification of Dry Beans Using Computer Vision and Machine Learning Techniques”. Computers and Electronics in Agriculture 174 (lipiec): 105507. https://doi.org/10.1016/j.compag.2020.105507.\n\n\nMingqiang, Yang, Kpalma Kidiyo, i Ronsin Joseph. 2008. „A Survey of Shape Feature Extraction Techniques”. W. InTech. https://doi.org/10.5772/6237.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Redukcja wymiarowości</span>"
    ]
  },
  {
    "objectID": "imbalance.html",
    "href": "imbalance.html",
    "title": "\n15  Nierównowaga klas w zadaniu klasyfikacyjnym\n",
    "section": "",
    "text": "15.1 Próbkowanie danych\nPróbkowanie (ang. subsampling) zbioru treningowego, zarówno zaniżanie (ang. undersampling), jak i zawyżanie (ang. oversampling) próbkowania odpowiedniej klasy lub klas, może być pomocne w radzeniu sobie z danymi klasyfikacyjnymi, w których jedna lub więcej klas występuje bardzo rzadko. W takiej sytuacji (bez kompensacji), większość modeli będzie nadmiernie dopasowana do klasy większościowej i wytworzy bardzo dobre statystyki dopasowania dla klasy zawierającej często występujące klasy, podczas gdy klasy mniejszościowe będą miały słabe wyniki.\nTen rozdział opisuje podpróbkowanie stosowane w kontekście radzenia sobie z nierównowagą klas.\nRozważmy problem dwuklasowy, w którym pierwsza klasa ma bardzo niską częstość występowania. Dane zostały zasymulowane i można je zaimportować do R za pomocą poniższego kodu:\nJeśli “klasa1” jest zdarzeniem będącym przedmiotem zainteresowania, jest bardzo prawdopodobne, że model klasyfikacyjny byłby w stanie osiągnąć bardzo dobrą specyficzność, ponieważ prawie wszystkie dane należą do drugiej klasy. Czułość jednak będzie prawdopodobnie słaba, ponieważ modele będą optymalizować dokładność (lub inne funkcje straty) poprzez przewidywanie, że wszystko jest klasą większościową.\nJednym z rezultatów braku równowagi klasowej, gdy istnieją dwie klasy, jest to, że domyślne odcięcie prawdopodobieństwa na poziomie 50% jest nieodpowiednie; inne odcięcie, które jest bardziej ekstremalne, może być w stanie osiągnąć lepszą wydajność.\nJednym ze sposobów na złagodzenie tego problemu jest podpróbkowanie danych. Istnieje wiele sposobów, aby to zrobić, ale najprostszym jest próbkowanie w dół (undersample) danych klasy większościowej, aż wystąpi ona z taką samą częstotliwością jak klasa mniejszościowa. Choć może się to wydawać sprzeczne z intuicją, wyrzucenie dużego procentu danych może być skuteczne w tworzeniu użytecznego modelu, który potrafi rozpoznać zarówno klasy większościowe, jak i mniejszościowe. W niektórych przypadkach oznacza to nawet, że ogólna wydajność modelu jest lepsza (np. poprawiony obszar pod krzywą ROC). Podpróbkowanie prawie zawsze daje modele, które są lepiej skalibrowane, co oznacza, że rozkłady prawdopodobieństwa klas są lepiej zachowane. W rezultacie, domyślne odcięcie 50% daje znacznie większe prawdopodobieństwo uzyskania lepszych wartości czułości i specyficzności niż w innym przypadku.\nIstnieją również techniki oversampling, które sprowadzają klasy mniejszościowe do liczebności takiej samej jak klasa większościowa (lub jej części) poprzez odpowiednie próbkowanie istniejących obserwacji lub też (jak to jest w przypadku metody SMOTE) tworzy się syntetyczne obserwacje podobne do już istniejących w klasie mniejszościowej. W pakiecie themis można znaleźć różne techniki próbkowania w górę: step_upsample(), step_smote(), step_bsmote(method = 1), step_bsmote(method = 2), step_adasyn(), step_rose() oraz kilka technik próbkowania w dół: step_downsample(), step_nearmiss() i step_tomek().\nZbadajmy działanie próbkowania używając themis::step_rose() w przepisie dla symulowanych danych. Wykorzystuje ona metodę ROSE (ang. Random Over Sampling Examples) z Menardi i Torelli (2012). Jest to przykład strategii oversampling.\nW zakresie przepływu pracy:\nOto prosta recepta implementująca oversampling:\nKodlibrary(themis)\nimbal_rec &lt;- \n  recipe(Class ~ ., data = imbal_data) %&gt;%\n  step_rose(Class, seed = 1234)\nJako modelu użyjmy modelu kwadratowej analizy dyskryminacyjnej (QDA). Z poziomu pakietu discrim, model ten można określić za pomocą:\nKodlibrary(discrim)\nqda_mod &lt;- \n  discrim_regularized(frac_common_cov = 0, frac_identity = 0) %&gt;% \n  set_engine(\"klaR\")\nAby utrzymać te obiekty związane ze sobą, połączymy je w ramach przepływu pracy:\nKodqda_rose_wflw &lt;- \n  workflow() %&gt;% \n  add_model(qda_mod) %&gt;% \n  add_recipe(imbal_rec)\n\nqda_rose_wflw\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: discrim_regularized()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_rose()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRegularized Discriminant Model Specification (classification)\n\nMain Arguments:\n  frac_common_cov = 0\n  frac_identity = 0\n\nComputational engine: klaR\nDo oceny jakości dopasowania modelu zastosujemy 10-krotny sprawdzian krzyżowy z powtórzeniami:\nKodset.seed(5732)\ncv_folds &lt;- vfold_cv(imbal_data, strata = \"Class\", repeats = 5)\nAby zmierzyć wydajność modelu, użyjmy dwóch metryk:\nJeśli model jest źle skalibrowany, wartość krzywej ROC może nie wykazywać zmniejszonej wydajności. Jednak wskaźnik J byłby niższy dla modeli z patologicznymi rozkładami prawdopodobieństw klas. Do obliczenia tych metryk zostanie użyty pakiet yardstick.\nKodcls_metrics &lt;- metric_set(roc_auc, j_index)\nKodset.seed(2180)\nqda_rose_res &lt;- fit_resamples(\n  qda_rose_wflw, \n  resamples = cv_folds, \n  metrics = cls_metrics\n)\n\ncollect_metrics(qda_rose_res)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 j_index binary     0.768    50 0.0214  Preprocessor1_Model1\n2 roc_auc binary     0.951    50 0.00509 Preprocessor1_Model1\nJak wyglądają wyniki bez użycia ROSE? Możemy stworzyć kolejny przepływ pracy i dopasować model QDA dla tych samych foldów:\nKodqda_wflw &lt;- \n  workflow() %&gt;% \n  add_model(qda_mod) %&gt;% \n  add_formula(Class ~ .)\n\nset.seed(2180)\nqda_only_res &lt;- fit_resamples(qda_wflw, resamples = cv_folds, metrics = cls_metrics)\n\ncollect_metrics(qda_only_res)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 j_index binary     0.250    50 0.0288  Preprocessor1_Model1\n2 roc_auc binary     0.953    50 0.00479 Preprocessor1_Model1\nWygląda na to, że próbkowanie metodą ROSE bardzo pomogło, zwłaszcza w przypadku indeksu J. Metody próbkowania nierównowagi klasowej mają tendencję do znacznej poprawy metryk opartych na twardych przewidywaniach klasowych (tj. przewidywaniach kategorycznych), ponieważ domyślne odcięcie ma tendencję do lepszej równowagi pomiędzy czułością i specyficznością.\nWykreślmy metryki dla każdej próbki, aby zobaczyć, jak zmieniły się poszczególne wyniki.\nKodno_sampling &lt;- \n  qda_only_res %&gt;% \n  collect_metrics(summarize = FALSE) %&gt;% \n  dplyr::select(-.estimator) %&gt;% \n  mutate(sampling = \"no_sampling\")\n\nwith_sampling &lt;- \n  qda_rose_res %&gt;% \n  collect_metrics(summarize = FALSE) %&gt;% \n  dplyr::select(-.estimator) %&gt;% \n  mutate(sampling = \"rose\")\n\nbind_rows(no_sampling, with_sampling) %&gt;% \n  mutate(label = paste(id2, id)) %&gt;%  \n  ggplot(aes(x = sampling, y = .estimate, group = label)) + \n  geom_line(alpha = .4) + \n  facet_wrap(~ .metric, scales = \"free_y\")\n\n\n\n\n\n\nRys. 15.1: Porównanie dopasowania przed i po ROSE dla obu metryk\nJak widać na podstawie Rys. 15.1 szczególnie w kontekście miar, które wykorzystują twardy podział (czyli zdefiniowany przez parametr odcięcia) nastąpiła znaczna poprawa.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Nierównowaga klas w zadaniu klasyfikacyjnym</span>"
    ]
  },
  {
    "objectID": "imbalance.html#próbkowanie-danych",
    "href": "imbalance.html#próbkowanie-danych",
    "title": "\n15  Nierównowaga klas w zadaniu klasyfikacyjnym\n",
    "section": "",
    "text": "Niezwykle ważne jest, aby subsampling występował wewnątrz resamplingu. W przeciwnym razie proces resamplingu może dać słabe oszacowania wydajności modelu.\nProces próbkowania powinien być stosowany tylko do zbioru analiz. Zestaw analiz powinien odzwierciedlać częstość zdarzeń widzianych “w naturze” i z tego powodu argument skip w step_downsample() i innych krokach receptury próbkowania ma domyślnie wartość TRUE.\n\n\n\n\n\n\n\n\n\n\n\nObszar pod krzywą ROC;\nWskaźnik J (statystyka Youdena J) określony jako czułość + specyficzność - 1. Wartości bliskie jeden są najlepsze.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Nierównowaga klas w zadaniu klasyfikacyjnym</span>"
    ]
  },
  {
    "objectID": "imbalance.html#zagrożenia",
    "href": "imbalance.html#zagrożenia",
    "title": "\n15  Nierównowaga klas w zadaniu klasyfikacyjnym\n",
    "section": "\n15.2 Zagrożenia",
    "text": "15.2 Zagrożenia\nPierwszą komplikacją związaną z próbkowaniem jest połączenie jej z przetwarzaniem wstępnym. Czy próbkowanie powinno mieć miejsce przed czy po przetwarzaniu wstępnym? Na przykład, jeśli zmniejszamy próbkę danych i używamy PCA do ekstrakcji cech, czy ładunki powinny być oszacowane z całego zbioru treningowego? Estymacja ta byłaby potencjalnie lepsza, ponieważ wykorzystywany byłby cały zbiór treningowy, ale może się zdarzyć, że podpróbka uchwyci niewielką część przestrzeni PCA. Nie ma żadnej oczywistej odpowiedzi ale zaleca się stosować próbkowanie przed procedurą wstępnego przetwarzania.\nInne zagrożenia to:\n\nSłabo reprezentowane kategorie w zmiennych czynnikowych (predyktorach) mogą przekształcić się w predyktory o zerowej wariancji lub mogą być całkowicie wyrywane z modelu.\nJeśli używasz grid_search() do określenia siatki wyszukiwania, może się zdarzyć, że dane, które są używane do określenia siatki po próbkowaniu, nie wypełniają pełnych zakresów zmienności hiperparametrów. W większości przypadków nie ma to znaczenia, ale czasami może doprowadzić do uzyskania nieoptymalnej siatki.\nW przypadku niektórych modeli, które wymagają więcej próbek niż parametrów, zmniejszenie rozmiaru próbki może uniemożliwić dopasowanie modelu.\n\n\n\n\n\nMenardi, Giovanna, i Nicola Torelli. 2012. „Training and Assessing Classification Rules with Imbalanced Data”. Data Mining and Knowledge Discovery 28 (1): 92–122. https://doi.org/10.1007/s10618-012-0295-5.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Nierównowaga klas w zadaniu klasyfikacyjnym</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "3.4 Evaluating Forecast Accuracy |\nForecasting: Principles and\nPractice (2nd Ed). n.d.\n\n\nBellon-Maurel, Véronique, Elvira Fernandez-Ahumada, Bernard Palagos,\nJean-Michel Roger, and Alex McBratney. 2010. “Critical Review of\nChemometric Indicators Commonly Used for Assessing the Quality of the\nPrediction of Soil Attributes by NIR Spectroscopy.”\nTrAC Trends in Analytical Chemistry 29 (9): 1073–81. https://doi.org/10.1016/j.trac.2010.05.006.\n\n\nBohachevsky, Ihor O., Mark E. Johnson, and Myron L. Stein. 1986.\n“Generalized Simulated Annealing for Function\nOptimization.” Technometrics 28 (3): 209–17. https://doi.org/10.1080/00401706.1986.10488128.\n\n\nBolstad, Benjamin Milo. 2004. Low-Level Analysis of\nHigh-density Oligonucleotide Array Data:\nBackground, Normalization and\nSummarization. University of California,\nBerkeley.\n\n\nBooth, Gordon D., George E. P. Box, William G. Hunter, and J. Stuart\nHunter. 1979. “Statistics for Experimenters: An Introduction to\nDesign, Data Analysis, and Model Building.” Journal of the\nAmerican Statistical Association 74 (367): 731. https://doi.org/10.2307/2287009.\n\n\nCouch, Simon P., Andrew P. Bray, Chester Ismay, Evgeni Chasnovski,\nBenjamin S. Baumer, and Mine Çetinkaya-Rundel. 2021.\n“Infer: An r\nPackage for Tidyverse-Friendly Statistical Inference” 6: 3661. https://doi.org/10.21105/joss.03661.\n\n\nCraig-Schapiro, Rebecca, Max Kuhn, Chengjie Xiong, Eve H. Pickering,\nJingxia Liu, Thomas P. Misko, Richard J. Perrin, et al. 2011.\n“Multiplexed Immunoassay Panel Identifies Novel CSF Biomarkers for\nAlzheimer’s Disease Diagnosis and Prognosis.” Edited by Ashley I.\nBush. PLoS ONE 6 (4): e18850. https://doi.org/10.1371/journal.pone.0018850.\n\n\nDavison, A. C., and D. V. Hinkley. 1997. “Bootstrap Methods and\nTheir Application,” October. https://doi.org/10.1017/cbo9780511802843.\n\n\nFranses, Philip Hans. 2016. “A Note on the Mean Absolute\nScaled Error.” International Journal of\nForecasting 32 (1): 20–22. https://doi.org/10.1016/j.ijforecast.2015.03.008.\n\n\nFrazier, Peter I. 2018. “A Tutorial on Bayesian\nOptimization.” https://doi.org/10.48550/ARXIV.1807.02811.\n\n\nFriedman, Jerome H. 2001. “Greedy Function Approximation: A\nGradient Boosting Machine.” The Annals of Statistics 29\n(5). https://doi.org/10.1214/aos/1013203451.\n\n\nGentleman, Robert, Vincent Carey, Wolfgang Huber, Sandrine Dudoit, and\nRafael Irizarry. 2005. Bioinformatics and Computational\nBiology Solutions Using R and Bioconductor.\nSpringer.\n\n\nHill, Andrew A, Peter LaPan, Yizheng Li, and Steve Haney. 2007.\n“Impact of Image Segmentation on High-Content Screening Data\nQuality for SK-BR-3 Cells.” BMC Bioinformatics 8 (1). https://doi.org/10.1186/1471-2105-8-340.\n\n\nHosmer, David W., and Stanley Lemeshow. 2000. Applied Logistic\nRegression. John Wiley & Sons, Inc. https://doi.org/10.1002/0471722146.\n\n\nHyndman, Rob J., and Anne B. Koehler. 2006. “Another Look at\nMeasures of Forecast Accuracy.” International Journal of\nForecasting 22 (4): 679–88. https://doi.org/10.1016/j.ijforecast.2006.03.001.\n\n\nHyndman, Robin John, and George Athanasopoulos. 2018. Forecasting:\nPrinciples and Practice. 2nd ed. Australia: OTexts.\n\n\nJohnson, Dan, Phoebe Eckart, Noah Alsamadisi, Hilary Noble, Celia\nMartin, and Rachel Spicer. 2018. “Polar Auxin Transport Is\nImplicated in Vessel Differentiation and Spatial Patterning During\nSecondary Growth in Populus.” American Journal\nof Botany 105 (2): 186–96. https://doi.org/10.1002/ajb2.1035.\n\n\nJoseph, V. Roshan. 2022. “Optimal Ratio for Data\nSplitting.” Statistical Analysis and Data Mining: The ASA\nData Science Journal 15 (4): 531–38. https://doi.org/10.1002/sam.11583.\n\n\nJoseph, V. Roshan, Evren Gul, and Shan Ba. 2015. “Maximum\nProjection Designs for Computer Experiments.” Biometrika\n102 (2): 371–80. https://doi.org/10.1093/biomet/asv002.\n\n\nKhun, M., and K. Johnson. 2013. Applied Predictive\nModeling. New York: Springer.\n\n\nKim, Jungsu, Jacob M. Basak, and David M. Holtzman. 2009. “The\nRole of Apolipoprotein E in Alzheimer’s Disease.” Neuron\n63 (3): 287–303. https://doi.org/10.1016/j.neuron.2009.06.026.\n\n\nKirkpatrick, S., C. D. Gelatt, and M. P. Vecchi. 1983.\n“Optimization by Simulated Annealing.” Science 220\n(4598): 671–80. https://doi.org/10.1126/science.220.4598.671.\n\n\nKoklu, Murat, and Ilker Ali Ozkan. 2020. “Multiclass\nClassification of Dry Beans Using Computer Vision and Machine Learning\nTechniques.” Computers and Electronics in Agriculture\n174 (July): 105507. https://doi.org/10.1016/j.compag.2020.105507.\n\n\nKuhn, Max, and Kjell Johnson. 2019. Feature Engineering and\nSelection. Chapman; Hall/CRC. https://doi.org/10.1201/9781315108230.\n\n\n———. 2021. Feature Engineering and\nSelection: A Practical Approach for\nPredictive Models. Taylor & Francis\nGroup.\n\n\nKuhn, Max, and Hadley Wickham. 2020. “Tidymodels: A Collection of\nPackages for Modeling and Machine Learning Using Tidyverse\nPrinciples.” https://www.tidymodels.org.\n\n\nKvalseth, Tarald O. 1985. “Cautionary Note about\nR2.” The American Statistician 39 (4):\n279–85. https://doi.org/10.2307/2683704.\n\n\nLaarhoven, Peter J. M. van, and Emile H. L. Aarts. 1987.\n“Simulated Annealing.” In, 7–15. Springer Netherlands. https://doi.org/10.1007/978-94-015-7744-1_2.\n\n\nMaron, Oded, and Andrew Moore. 1993. “Hoeffding Races:\nAccelerating Model Selection Search for Classification and\nFunction Approximation.” In Advances in Neural Information\nProcessing Systems, edited by J. Cowan, G. Tesauro, and J.\nAlspector. Vol. 6. Morgan-Kaufmann.\n\n\nMcKay, M. D., R. J. Beckman, and W. J. Conover. 1979. “A\nComparison of Three Methods for Selecting Values of Input Variables in\nthe Analysis of Output from a Computer Code.”\nTechnometrics 21 (2): 239. https://doi.org/10.2307/1268522.\n\n\nMenardi, Giovanna, and Nicola Torelli. 2012. “Training and\nAssessing Classification Rules with Imbalanced Data.” Data\nMining and Knowledge Discovery 28 (1): 92–122. https://doi.org/10.1007/s10618-012-0295-5.\n\n\nMingqiang, Yang, Kpalma Kidiyo, and Ronsin Joseph. 2008. “A Survey\nof Shape Feature Extraction Techniques.” In. InTech. https://doi.org/10.5772/6237.\n\n\nSchulz, Eric, Maarten Speekenbrink, and Andreas Krause. 2016. “A\nTutorial on Gaussian Process Regression: Modelling, Exploring, and\nExploiting Functions.” http://dx.doi.org/10.1101/095190.\n\n\nShahriari, Bobak, Kevin Swersky, Ziyu Wang, Ryan P. Adams, and Nando de\nFreitas. 2016. “Taking the Human Out of the Loop: A Review of\nBayesian Optimization.” Proceedings of the IEEE 104 (1):\n148–75. https://doi.org/10.1109/jproc.2015.2494218.\n\n\nShewry, M. C., and H. P. Wynn. 1987. “Maximum Entropy\nSampling.” Journal of Applied Statistics 14 (2): 165–70.\nhttps://doi.org/10.1080/02664768700000020.\n\n\nThomas, Rachel, and David Uminsky. 2020. “The Problem with Metrics\nIs a Fundamental Problem for AI.” https://doi.org/10.48550/ARXIV.2002.08512.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the Tidyverse.” Journal of Open Source\nSoftware 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nXu, Qing-Song, and Yi-Zeng Liang. 2001. “Monte Carlo Cross\nValidation.” Chemometrics and Intelligent Laboratory\nSystems 56 (1): 1–11. https://doi.org/10.1016/s0169-7439(00)00122-2.\n\n\nYeh, I.-Cheng. 2006. “Analysis of Strength of\nConcrete Using Design of Experiments and\nNeural Networks.” Journal of Materials in Civil\nEngineering 18 (4): 597–604. https://doi.org/10.1061/(ASCE)0899-1561(2006)18:4(597).",
    "crumbs": [
      "References"
    ]
  }
]